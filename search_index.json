[["index.html", "Data Analysis and Numerical Methods for Biological Systems Engineers About this book", " Data Analysis and Numerical Methods for Biological Systems Engineers Durelle Scott R. Clay Wright 2025-03-24 About this book This book was developed as a companion text for the Virginia Tech Biological Systems Engineering course BSE3144: Engineering Analysis for Biological Systems using Numerical Methods. This course focuses on the process of solving engineering problems related to biological systems using numerical analysis including root finding, numerical integration, differentiation, interpolation and numerical solution of ordinary differential equations, error analysis and programming with engineering software. The course will cover topics of numerical analysis including: root finding, numerical integration, differentiation, interpolation and numerical solution to ordinary differential equations to biological systems engineering related problems. R is the programming language we will use in this course. We will spend the first portion of this course devoted to learning the basics of structured and modular programming. Although we focus on R, the described techniques are also applicable in any other computer languages. Furthermore, the techniques provide a logical approach to addressing complex problems, breaking them down into manageable (and solvable) pieces. The numerical computation techniques included in this course are utilized by many engineers. For biological systems engineers, the applications of the techniques include consideration of characteristics of biological systems, such as natural variability, growth/decay cycles, nonhomogeneity, anisotropy, and process uncertainty. Learning Objectives: Upon successful completion of this course, the student will be able to: Apply the following numerical techniques to solve problems in biological systems engineering: regression analysis root finding solving systems of linear equations interpolation differentiation and integration solving ordinary differential equations Define and quantify sources of error in numerical techniques Write programs, using engineering software, that involve loops, logical block constructs, function, plotting, and input/output "],["introduction-to-posit-rstudio-markdown.html", "1 Introduction to Posit, Rstudio, &amp; Markdown 1.1 Pre-work 1.2 Posit 1.3 Rstudio", " 1 Introduction to Posit, Rstudio, &amp; Markdown This week our goals are to be able to: Navigate the Posit and RStudio interface with proficiency. Articulate the process of using RStudio to develop markdown documents. Practice using markdown to format reproducible documents 1.1 Pre-work Read https://posit.cloud/learn/guide to learn a little bit about the Posit cloud space To get a quick intro to the Rstudio user interface watch this YouTube Video, or this one which is a little longer and provides some more motivation, or check out the Posit/Rstudio User Guide (if you prefer reading to videos). Check out the Posit/RStudio Introduction to Rmarkdown and Quarto Peruse the material below and complete week 1 concept check (in Canvas) Open the 01-2_Problem-Set.Rmd file from the files pane in the lower right and be ready to work through these materials on Monday. 1.2 Posit Posit, the company, used to be called RStudio, but as they expanded beyond offering services just for R they decided to change their name. I thought that RStudio, the software, was also going to change names, because it is not just for working with R, but it hasn’t yet changed names. Posit now makes RStudio (which is still open source!) and also hosts RStudio servers via Posit.cloud that we pay for to help you all learn R, basic programming, data science, and engineering data analytics. Within the Posit Cloud you will see “Spaces” to the left including the BSE 3144 workspace for this class. Within this workspace there are projects for each week. I can make “assignment” projects, which means that when you open the project it will make a copy of the project that you own and can edit and complete. Think about each of these new projects you make as your personal notebook for the class. Add your notes as you work through the material. Within the project is the RStudio interface. This is just like if you installed R and RStudio on your computer only it is in a website. You can download R and RStudio on your computer if you like, following the directions at https://posit.co/download/rstudio-desktop/, but especially as we get deeper into the material installing packages can take a good chunk of time. By working in the environment we have set up for you in the Posit Cloud you can hopefully spend time learning in class or at home as opposed to setting up your computing environment. We will cover installing packages next week, and setting up your environment is a useful skill, but it is largely an exercise in patience, with the occasional internet troubleshooting/debugging (which we will cover in detail by week 03). 1.3 Rstudio Rstudio (or Posit if they ever change it) is an integrated development environment, or IDE. An IDE is a program in which you can write code, view the outputs it creates, keep your code and outputs organized, and many other useful things. You can write R code in RStudio as well as many other programming languages. RStudio has lots of nice features. You can check out many of them by scanning through the menu bar. Tools&gt;Global Options… also has many nice customization options including the the coveted hacker dark mode themes in the “Appearance” tab by changing the Editor Theme. A good IDE can make learning a language much easier, and even if you are an experienced coder a good IDE can make you much more efficient. Let’s take a look around the RStudio window. You will see several panes within the RStudio window. You will see several panes within the RStudio window. 1.3.1 The Source pane If a file is open, in the top left of the screen perhaps this one, you will see the Source pane. (If you don’t see this pane, create a new file by pressing shift + command(or ctrl) + N, or click the universal ‘New Document’ icon and select ‘R script’). Source shows the files with which you are currently working. This is where you will spend most of your time building and testing lines of code and then combining them to make scripts for data analysis. In general you should be doing most all of your typing into the source pane to build a document that you can use to reproduce your analysis. In this class we will be mostly working in markdown documents in the source pane. 1.3.2 Markdown Markdown is a document formatting computer language. The name markdown comes from html, or HyperText Markup Language, what websites are written in (along with a lot of other languages these days). Markdown is meant to be a much more chill version of html, but can also be used to make just about any kind of document from websites to PDFs, PowerPoint, and Word docxs. Markdown uses very simple text-based symbols to format text into potentially very attractive and useful documents. Markdown is used in many different places these days, but we’ll use it in this class to help guide our data analyses. Remember: “If you are thinking without writing, you only think you are thinking!” -Leslie Lamport (emphasis my own). We will use markdown in the form of Rmarkdown or Quarto files. Rmarkdown is a product of Rstudio the company, and holding with the trend of changing names, the newest extended version of Rmarkdown is called Quarto, which really goes beyond R to include any programming language (or many at least). We will mostly use Rmarkdown in this class, for now, as most of the materials were developed before Quarto. But feel free to experiment with Quarto and I will do the same as I develop new materials. When you make a new R Markdown document the following useful template begins the file. 1.3.2.1 R markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 1.3.2.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. This provides a concise introduction to markdown and its power. Through simple text symbols your can format text to be headings (with #’s), make text bold, denote what is code, and offset code chunks with three back-ticks (the key next to 1 on your keyboard) and curly braces indicating the programming language. Code chunks (or blocks) also allow you to name them and changing some settings with how the code should be run and output. Note how the above and all of this document (in its markdown format) has empty lines between text elements. This makes sure that particularly headings and sections, like numbered/bullet lists, are formatted correctly. 1.3.3 More Markdown formatting There are lots of cool things you can do with markdown. Check out the resources available from links in the menu bar by going to Help&gt;Markdown Quick Reference as well as Help&gt;Cheat Sheets. There are cheat sheets for the RStudio IDE and a couple for R Markdown. We will practice with those a bit in Examples and your Problem Set. You can also click “Visual” next to “Source” up at the top of this file to use a more traditional rich text format editor using mouse clicks to format your text. I have found this function is still a little glitchy and you especially need to be careful pasting text into the Visual editor. There are some things the visual editor does make easier like inserting images and citations and formatting tables. As you’ll see below with the code chunk settings in Code chunk/block settings and global settings below. Links and images are particularly useful for documents. You can link to any website you simply sandwich the text you want to be linked in square brackets, followed by parentheses containing the link/web-address/URL (uniform resource locator). Links can also be to sections (defined by headings in your document) as you see above (figuring out how the names change can be tricky though, I had to look in the knitted html file for the section above). Images are similar but the link is the alt-text or caption text, and the link is a link or path to the image. The image can be somewhere in your project folder or anywhere on the internet. So for example we could use the code![This is a screenshot highlighting the path to this screenshot](images/Screenshot 2024-01-24 at 9.54.34 PM.png) to insert this screenshot which is in this projects images folder: This is a screenshot highlighting the path to this screenshot For all file paths that you use in a markdown document the path is relative to the markdown document. So this Rmd is in the 01_Posit-Rstudio-Markdown folder and we only need to specify that, relative to this file, the screenshot above is in the images folder/directory and then we also need to specify the file name of the image. 1.3.4 Running code Any line of code in a code chunk the source pane can be run in the Console pane by placing your cursor on that line and pressing control+enter (command+return on macs). Practice running the lines below in the code chunk. 2+2 ## [1] 4 9/3 ## [1] 3 3^3 ## [1] 27 2&gt;3 ## [1] FALSE 2&lt;3 ## [1] TRUE The result of the code is printed right below the chunk and is also shown in the Console pane below. You can also use the green play button (right-facing triangle) to run all of the lines in the chunk, or try to run either of the back-tick-containing top and bottom lines. 1.3.5 The Console pane The Console pane is in the bottom-left corner labeled Console at the top left. (If the Source pane is closed, the Console pane may take up the whole left side of the window). The Console is essentially an R command line window. You can type in anything following the &gt; then press return (or Enter) and whatever you typed will be interpreted by R and the appropriate output will be returned. R understands all basic algebra as well as logical expressions (aka Boolean expressions, such as 5&lt;7 or True &amp; False = False). Most of the time you want to type your code into the source pane, but occasionally you will want to test something out or do a quick calculation that might not be needed in the file you are working on. Then you can work directly in the console. In addition to being a basic calculator, R interprets functions or variables that can be assigned to and represented by words. Variables (any piece of data of any variety) will be single words possibly followed by a $ or square brackets [] if the variable is a matrix (a two dimensional array of data, where matrix$y would return column y and matrix[x,y] would return the piece of data in row x and column y). Functions (blocks of code that perform a specific function) are followed by parentheses containing the arguments/parameters on which that function will operate. For example, print(\"Hello World\"), uses the print function to print the text argument (an argument is anything we pass to a function) “Hello World” if you run that line in the console. Variables and Functions R is an object oriented language. This means we can use R to create abstracted objects that contain data (of any type, shape or size) called variables or procedures/methods (individual blocks of code) called functions. There are numerous functions and datasets included in the base R installation. Also, as an open source language countless programmers in the R community have written useful functions and created useful datasets that are freely available in the form of R-packages (more on these later). You can also write your own! But more on this in the The big difference between using RStudio and running R from the command line is that this pane has an auto-complete feature. Try typing pri into the console below (or in a code chunk or Source pane and pressing tab. RStudio automatically provides you with a list of all the available functions and variables beginning with ‘pri’! You can navigate this list using the arrow keys or your mouse. When you select a particular object, RStudio also gives you some information about that object. Navigate down to print (if there are multiple, select the one that has {base} at the far right) and press tab. You will see that RStudio has completed print in the console and added a set of parentheses because print is a function, print(). Now we can add arguments that this function will operate on, within the parentheses. But what does this function do? To figure out type ?print in the Console and press return. This opens the documentation for this function in the Help pane. A ? before any function name, or passing a function name to the help() function will do the same. 1.3.6 The Help pane This pane is essentially a browser window for R documentation. You can also search for functions or variables in R and all of the installed packages on your computer using the search box at the top. You can search within a documentation page using the Find in Topic box. Using this pane you should be able to answer almost any question you have about any R function. All R documentation follows standard formatting. Description is pretty self explanatory. Usage demonstrates how you use the function, sometimes with specifics for different variable types. For print this shows us that print takes the input argument x (an argument is just variable that is used in a function). If x is a ‘factor’ or a ‘table’, print will also take some additional arguments. In the Usage section the default value of each argument is listed (e.g. FALSE is the default value for argument quote). A description of each argument is listed below in the Arguments section. Value is the type of data returned by the function. There are a few other self-explanatory sections and finally Examples. This is often one of the most useful sections as it shows you how to use the function. The code in Examples can be copied and pasted into the console and run. 1.3.7 The Files/Plots/Packages/Help/Viewer pane The Help pane contains additional tabs that can also be quite helpful. Files allows you to navigate through folders on your computer and open files. Plots shows you the most recent plot your code has produced and allows you to save it. Packages allows you to install and load packages into memory. Packages are bundles of code that other people have written and shared with the community (more about packages later). Additionally, there is a search engine specific to R resources including the documentation, blogs, books and questions users have asked on discussion boards. This invaluable resource is at Rseek.org. This is especially helpful if you want to find a function to perform a specific task. 1.3.8 OK, back to the Source pane… You should have print() in there now. Put your cursor in the middle of the parentheses and press tab. RStudio will feed you all of the arguments of this function using auto-complete! Press tab again and x = will appear in the parentheses. Type \"hello world\" and press command/ctrl + return/enter. This will copy your line of code into the console and execute it. Amazing! Your first line of R code worked, hopefully… print(x = &quot;hello world&quot;) # prints hello world in the console ## [1] &quot;hello world&quot; Take note that if you are missing the quotes around hello world R will look for a variable named hello and return an error. If your code didn’t work, try to fix it and run it again. command + return (ctrl + enter on Windows machines) can be used to execute a whole line or any selected code. Now just highlight x = \"hello world\" within the print(x = \"hello world\") line and press command + return (or ctrl + enter). This will just execute the highlighted text. If everything worked properly, you have just created your first variable object in R! 1.3.9 The Environment pane See, over on the top right next to x (the variable object name) is “hello world” (the value assigned to that variable). You can now execute just print(x), and you will get [1] \"hello world\"! The Environment pane shows all of the objects you have created or stored in memory. You can view data sets or functions by clicking on them, but at the moment we only have the simple variable x. Don’t worry, we’ll practice this later. 1.3.10 The History/Connections/Git/Build/Tutorial pane The Environment pane also has many tabs that can do lots of cool things. For now all that I want to cover is the History pane/tab. This keeps track of all of the code you have run. So if you forgot what you typed into the console, or you want to transfer something you ran in the Console into the Source pane, the History pane will do that for you. 1.3.11 Terminal/Render/Background Jobs Also the Console pane has other tabs. Terminal can run Unix code. Render is for tracking markdown documents as they are rendering into whatever their output format is. We are going to expand on using these panes and practice writing some more R code next week but for now we are going to focus a little more on R Markdown. 1.3.12 YAML headers Each R Markdown or Quarto document starts with a YAML header like: --- title: &quot;Diamond sizes&quot; date: 2023-10-26 format: html draft: true --- (Note if you are reading in markdown that I used a code chunk to make sure this header text didn’t mess up our current document). YAML stands for Yet Another Markup Language and this header provides some data and variables or settings for how the document should be rendered, or knit. We could change the YAML header of this document to make an HTML webpage version of this document for example by just changing --- title: &quot;01-0 Introduction to Posit, Rstudio, &amp; Markdown&quot; author: &quot;Clay Wright&quot; date: &quot;2025-03-24&quot; output: pdf_document --- to --- title: &quot;01-0 Introduction to Posit, Rstudio, &amp; Markdown&quot; author: &quot;Clay Wright&quot; date: &quot;2025-03-24&quot; output: html_document --- We could also make this into a docx or slides, but slides work a little differently. I made the slides on the first day of class in Quarto, you can check them out in that project. In general this works quite well. But the YAML header is often a source of hard to track down errors. These errors can often be solved by replacing the header with the header from an RStudio template document. 1.3.13 Code chunk/block settings and global settings Code chunk or block settings, which set how a code block will be run or formatted is one area in which R Markdown and Quarto differ. (from https://www.stephaniehicks.com/jhustatprogramming2023/posts/2023-10-26-build-website/) 1.3.13.1 R Markdown vs Quarto Some high-level differences include Standardized YAML across formats Decoupled from RStudio More consistent presentation across formats Tab Panels Code Highlighting Another noticeable difference are settings/options for code blocks. Rather than being in the header of the code block, options are moved to within the code block using the #| (hash-pipe) for each line. This is a code block for R Markdown: ```{r setup, include=FALSE} library(tidyverse) library(tidytext) ``` (Note if you are reading in markdown that the text formatting is broken from here on. This is a known issue in RStudio https://community.rstudio.com/t/continued-issues-with-new-verbatim-in-rstudio/139737, and is possibly a reason to switch to Quarto). This is a code block for Quarto: ```{{r}} #| label: &quot;setup&quot; #| include: false library(tidyverse) library(tidytext) ``` 1.3.14 Output Options There are a wide variety of output options available for customizing output from executed code. All of these options can be specified either globally (in the document front-matter) or per code-block For example, here’s a modification of a Python-containing markdown example to specify that we don’t want to “echo” the code into the output document: --- title: &quot;My Document&quot; execute: echo: false jupyter: python3 --- Note that we can override this option on a per code-block basis. For example: ```{python} #| echo: true import matplotlib.pyplot as plt plt.plot([1,2,3,4]) plt.show() ``` Code block options available for customizing output include: Option Description eval Evaluate the code chunk (if false, just echos the code into the output). echo Include the source code in output output Include the results of executing the code in the output (true, false, or asis to indicate that the output is raw markdown and should not have any of Quarto’s standard enclosing markdown). warning Include warnings in the output. error Include errors in the output (note that this implies that errors executing code will not halt processing of the document). include Catch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block). Here’s a example with r code blocks and some of these additional options included: --- title: &quot;Knitr Document&quot; execute: echo: false --- ```{{r}} #| warning: false library(ggplot2) ggplot(airquality, aes(Temp, Ozone)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) ``` ```{{r}} summary(airquality) ``` When using the Knitr engine, you can also use any of the available native options (e.g. collapse, tidy, comment, etc.) which help format your code chunks nicely in the final document. See the Knitr options documentation for additional details. You can include these native options in option comment blocks as shown above, or on the same line as the {r} as shown in the Knitr documentation. 1.3.15 Troubleshooting Knit/Render problems Knitting or Rendering markdown documents can lead to lots of tough to identify error messages. It is many of the questions we recieve. Especially right before assignments are due. Here is a great write-up of common markdown problems: https://rmd4sci.njtierney.com/common-problems-with-rmarkdown-and-some-solutions. Probably the most common issue we see is something (like your name in the author: line) in the YAML header that is not in quotes to indicate it is a string, or that there is some variable in your environment pane that is not actually created in your markdown document. To avoid this second problem in the first place, I try and do the following: Develop code in chunks and execute the chunks until they work, then move on. knit the document regularly to check for errors. Then, if there is an error: recreate the error in an interactive session: restart R run all chunks below find the chunk that did not work, fix until it does run all chunks below explore working directory issues remember that the RMarkdown directory is wherever the .Rmd file is in your Files pane "],["rstudio-and-markdown-examples.html", "Rstudio and Markdown examples 1.4 Navigate the Posit and RStudio interface with proficiency. 1.5 Articulate the process of using RStudio to develop markdown documents. 1.6 Practice using markdown to format reproducible documents", " Rstudio and Markdown examples We will look at a few examples and do some live demos here in class. Remember our goals are to: Navigate the Posit and RStudio interface with proficiency. Articulate the process of using RStudio to develop markdown documents. Practice using markdown to format reproducible documents 1.4 Navigate the Posit and RStudio interface with proficiency. 1.4.1 Making new documents 1.4.2 Navigating the file pane and file paths 1.4.3 Console pane 1.4.4 Environment pane 1.4.5 History pane 1.5 Articulate the process of using RStudio to develop markdown documents. 1.5.1 Cheat Sheets! Yay! 1.5.2 Inserting code chunks 1.5.3 Changing code chunk settings 1.6 Practice using markdown to format reproducible documents The below from https://github.com/njtierney/rmd-errors/tree/master is an Rmd that contains many errors we will troubleshoot. 1.6.1 Tasks Get this rmarkdown document to compile hint: knit the document and look at the line for the error if there is an error: recreate the session in an interactive session: restart R run all chunks below (top section Run&gt;arrow&gt;run all chunks below) find the chunk that did not work, fix that chunk run all chunks below (top section Run&gt;arrow&gt;run all chunks below) explore working directory issues where is the packages.bib file? remember that the rmarkdown directory is where the .Rmd file lives ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE, dev = &quot;png&quot;}) ``` ```{r library, echo = } library(tidyverses) ``` # Introduction This is a simple analysis of the New York Air Quality Measurements using the R statistical programming language [@Rcore]. As stated in the helpfile `?airquality`, this dataset contains: &gt; Daily air quality measurements in New York, May to September 1973. And the dataset is sourced from: &gt; ... the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data). It contains the following variables - Ozone: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island. - Solar.R: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park. - Wind: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport. - Temp: Maximum daily temperature in degrees Fahrenheit at La Guardia Airport. - Month: Month (1--12) - Day: Day of month (1--31) We are going to explore the relationship between solar radiation and other selected variables, solar radiation, wind, and temperature. # Method First, we tidy the names of the dataset, to provide information about the units of measurement for Ozone, Solar Radiation, Wind, and Temperature. We do this by renaming the variables and adding a suffix at the end to describe the units. To do this we use the `rename` function from the `dplyr` package[@dplyr]. ```{r tidy-data} tidy_aq &lt;-rename(.data = airquality, ozone_ppb = Ozone, solar_rad_lang = Solar.R, wind_mph = Wind, temp_fah = Temp, month = Month, day = Day) ``` We can see that there is an interesting relationship between ozone and solar radiation in figure 1 below, plotted using ggplot2 [@ggplot2] ```{r figure-1, fig.height = 4, fig.width=4} ggplot(tidy_aq, aes(x = ozone_ppb, y = solar_rad_lang)) + geom_point() ``` We can also see that there is an interesting relationship between Ozone and temperature. ```{r figure-2} ggplot(tidy_aq, aes(x = ozone_ppb, y = temp_fah)) + geom_point() ``` To explore the relationships between Ozone and all of the variables in the dataset, we can fit a basic linear model, with Ozone as the outcome, and all other variables as the predictors. We can express this as: $$ Ozone \\sim \\beta_0 + \\beta_1Solar.R + \\beta_2 Wind + \\beta_3Temp + \\epsilon $$ And we can fit this model using the code below. ```{r data-model} lm_aq &lt;- lm(ozone_ppb ~ solar_rad_lang + wind_mph + temp_fah, data = tidy_aq) ``` # Results The key results are given below, using the `tidy` function from the `broom` package [@broom] to clean up the data. ```{r broom-tidy} library(broom) tidy_lm_aq &lt;- tidy(lm_aq) tidy_lm_aq ``` We can present this result in a nice table using the `kable` function from the knitr package [@knitr] ```{r lm-kable} knitr::kable(tidy_lm_aq_broom, digits = 3, caption = &quot;Table of results from the linear model&quot;) ``` We can also refer to individual results of the model inside the text. For example, we can say that the estimated coefficient of Wind miles per hour is `r tidy_lm_aq$estimate[3]`, and the P value of this is `r tidy_lm_aq$p.value[3]`. # Conclusion We have explored the relationship of Ozone with other variables in the airquality dataset # References "],["the-r-language-and-tidy-data.html", "2 The R Language and Tidy Data 2.1 Reading and Pre-class Materials 2.2 Tidy data 2.3 R Basics 2.4 Functions 2.5 Objects - Variables and Functions 2.6 Tidyverse tables", " 2 The R Language and Tidy Data This week is devoted to basic tutorials on R programming and using Tidy Data, building on our intro to Rstudio/Posit and Markdown last week. Just like learning a foreign language, to learn programming and get comfortable, you’ll need to practice and immerse yourself! If you attempt to plow through in one sitting, unless you’re a genius, you’re unlikely to retain much. My suggestion is to start working early - start with either watching the assigned video or the reading. I’ve noted that you can choose one or the other; for extra reinforcement, read and watch (the video is only 15 minutes)! Then go ahead and get started with the in-class activities. There are so many excellent resources to augment what we’re working together on learning. If you want to dive deeper, check out this freely available online resource - R for Data Science. Lastly, I hope you all take time to reflect on this week, make adjustments, think about your time management, and lastly make some time to relax and unwind. With the skills you learned last week it would only take a little more learning to make your own personal github-hosted webpage. You can see mine here: &lt;wrightrc.github.io&gt; 2.0.1 Data Organization and manipulation in Spreadsheets Spreadsheets are commonly employed software applications for entering, storing, analyzing, and visualizing data. Concentrating on the data entry and storage components, this article provides practical suggestions on structuring spreadsheet data to minimize errors and facilitate subsequent analyses. 2.1 Reading and Pre-class Materials Reading - choose 1 based on your learning style: Read section 12.2 Tidy data of R for Data Science: https://r4ds.had.co.nz/tidy-data.html#tidy-data-1 Watch a video on Tidy data - The one from Posit, is pretty good, but in the quick check I did all of the first page for my search above were OK. Whichever reading or video you choose, I’d recommend opening up Posit/RStudio and following along with the reading material/video. For most of the content, you can duplicate the exercises in Rstudio. For example, once you create a student account, you’ll be able to create your own project (RUwithMe covers this aspect) Work through the Tidy Data and R Basics intro materials in this document below which are based in part on: Posit Cloud R Basics Recipes. Have a great weekend! Post any questions in Teams. 2.2 Tidy data Tidy data is pretty important for coding reproducible data analyses, because when data is tidy your code can operate on it in a consistent way. 2.2.1 Top 12 Tips for Data Organization (from Broman and Woo, 2018). Organize data as a series of rectangles in separate sheets. 1 row for column IDs., remaining rows the observations. One piece of data per cell. Imagine you are running an instrument with the racks to hold your samples. Each rack (A,B,C) has 10 sample slots. For the label, you could use A-1, A-2,… or have 2 columns: column 1 containing the rack ID, column 2 containing the sample slot ID. Using the latter (2-columns) is preferred. Why? In subsequent data analysis, this allows you to easily sort based on rackID (imagine there was a problem when the instrument got to rack B). Add notes in separate columns Add units in column name or separate column/data dictionary/annotation. Be consistent Use consistent “codes” for categorical variables. For example, imagine if you have a list of chemicals with a label for flammable or corrosive. As you enter data, sometimes you write out flammable, but other times just flam. What should you do? Choose one and stick with it! (Once we get into R there are some special jargon: “codes” are “levels” and “categorical variables” are “factors”). Use consistent code for missing values. Fill in every cell even if the value is unknown. While some datasets use a default number (e.g. -9999) to signify no data, it’s preferred to use NA. Never put a note in the data column; rather have a separate column just for notes. Use consistent variable names. When you have multiple spreadsheets representing the same type of data (e.g. flow record from different sites; experimental data from different days), keep your variable names the same and avoid spaces (e.g. “nitrate_mgperml” is preferred over “nitrate mg per ml) Keep layout consistent between sheets. One approach here is to set up a template, and then work from there. Use consistent file names. For example, Q_James_2015.csv and Potamac_Q_2015.csv are not consistent making it more difficult to sort, read into software, and view. Use consistent date formats. This is a big one! I prefer using yyyymmdd, or yyyy-mm-dd. If you sometimes show 01/22/2021 and other times 20210122, it’ll make it difficult to perform data analysis! Use consistent notes in your note column. Choose good names No spaces, choose with an underscore or hyphen, avoid special characters. Using CamelCase (e.g. ExampleFileName). Include units if possible, avoid abbreviations (but not at the expense of too long) See the table below from datacarpentry.org that illustrates this concept: Write dates as YYYY-MM-DD. This is a global standard (ISO 8601), and using this consistently is good practice. Unfortunately, Excel is not consistent with how dates are treated across platforms (e.g. mac vs PC). Furthermore, Excel will almost always try to convert the date into a number based on an arbitrary initial date (e.g. 1904). Here are approaches to get around this: Use plain text format for columns containing text: select column → in the menu bar, select format cells → choose “text” Add dates as YYYYMMDD (e.g. 20210122) Add dates in between apostrophes (‘2021-01-22’). This approach saves as text and not numeric. No empty cells - use “NA” when you do not have data for a given cell in a spreadsheet (for a column or row that is otherwise full of data) Are the following spreadsheets following good practice? Why or why not? Figures A &amp; B - you’d need to rearrange and figure out what is going on Figure C - data is already processed to some degree, and you don’t have access to raw data Figure D - Incomplete table, either fill out completely or break into 2 tables. See figure 6 in reading. Use a data dictionary. Keep data in a data file, use a complimentary README file (or a metadata/annotation sheet) that contains ‘metadata’. This should include explanation of the variable, units, expected values (minimum and maximum). It may also include information about how the data was collected and how the experiment was performed. There is an example README from the Vermont Covid dashboard in the images folder, “images/VT_COVID-19_Cases_by_County_Time_Series_modify_README.rtf”. This could be a txt file, rtf, html, csv or any other file type. Do not perform calculations in raw data files. Use a separate file for visualizations, calculations, analysis, etc. Do not use color or highlighting in data files. Rather, use a column that provides a note instead of highlighting cells. (This is because the color or highlighting is data! Every datum gets its own cell - #2.) Backup data. Use version control—if anything is added, updated, etc save a new version of the file. Validate data. Excel, Data → Validation (see icon at right), or Data → Data Validation in Google Sheets. Then you can set up logical rules to check that your columns of data contain valid values (for example a column of percent values should be between 0 and 100). Any values that don’t pass your logical test will be highlighted. To see the rules you have created follow these instructions. In R you can write code to validate data. More on R code below! Save data in plain text files. Save as -&gt; Format -&gt; csv. Why? Simplified archive, preservation of data for future scientists and engineers. When data is saved in a proprietary format, sharing across versions/platforms and the future may be hampered. Also check out the useful list of shortcut keys in Excel, or shortcut keys in Google Sheets. 2.3 R Basics While Excel certainly has its uses, it can often do more harm than good. And after all this course is, in part, about R. So here we are going to jump in to using R to analyze data. 2.3.1 Basic Math to Linear Algebra 2.3.1.1 Arithmetic with single numbers You want to use R to do simple arithmetic, as if R were a calculator. Step 1 - Write the mathematical expression you wish to evaluate. R recognizes the following operators: Operator Operation Example + Addition 1 + 2 - Subtraction 2 - 1 * Multiplication 2 * 3 / Division 4 / 2 ^ Exponentiation 4 ^ 2, i.e. \\(4^2\\) %% Modulo 5 %% 3, i.e what is the remainder when you divide five by three? %/% Integer division 5 %/% 3, i.e how many times does three go into five? Step 2 - Run the expression. R will return the result. 2.3.1.1.1 Example Suppose we want to divide the difference between 111 and 75 by 3. Using the operators above, we can write 111 - 75 / 3, but R will evaluate this as 111 minus 75 / 3 111 - 75 / 3 ## [1] 86 R isn’t doing anything funny, R is just following the standard order of mathematical operations. To correct the order of operations, we add parentheses. (111 - 75) / 3 ## [1] 12 Remember PEMDAS PEMDAS is an acronym that describes the order of operations in arithmetic. We first evaluate expressions that are grouped together by Parentheses. Next, we evaluate Exponentiation, followed by Multiplication or Division. Finally, we evaluate Addition and Subtraction. Use ( and ) when you need to control the order of mathematical operations in R. 2.4 Functions 2.4.1 What is a function? a way to reuse a chunk of code easily without having to copy and paste performs simple to complex operations often requires inputs \\[known as the arguments\\], some of which are optional used when a task is performed over and over again An example function is abs(x) where abs is the function name and x is the input argument. This function computes the absolute value of the input argument. You can learn more about this function by typing ?abs into the console. abs(-19) ## [1] 19 2.4.1.1 Call a function on a number R provides thousands of functions to use. Functions are prepackaged pieces of code that perform useful tasks. To use a function in R: Step 1 - Write the name of the function. Do not surround it in quotes. The name tells R which function you would like to run. Step 2 - Place a pair of parentheses after the name. Parentheses are the trigger that runs the function. Step 3 - If the function needs a piece of input to do its job, place the input inside of the parentheses. Your function call will now look something like this, factorial(4). Step 4 - Click Run. Or press Enter if you are using R from a command line. R will run your function and return the result. 2.4.1.2 Example Suppose we’d like to find the square root of 1764. We can do this with a function named sqrt, which is short for square root. To call sqrt, we first write its name. Notice that if we run just the name of sqrt, R shows us the code associated with sqrt. That’s not what we want. sqrt ## function (x) .Primitive(&quot;sqrt&quot;) Next, we place a pair of parentheses after sqrt. When we click Run, the parentheses will cause R to execute our function. Our function call is not yet complete because sqrt() requires a piece of input to do its job. Which number should square root take the square of? If we run our command in this unfinished state, R will return an error message. sqrt() ## Error in sqrt(): 0 arguments passed to &#39;sqrt&#39; which requires 1 We finish our command by giving sqrt() a number to take the square of, in this case, 1764. sqrt(1764) ## [1] 42 In markdown we will use a special convention whenever we mention a function: we will write the function’s name in code font and follow it with a pair of parentheses, like this sqrt(). R users have several names for the code that runs a function. You may see it referred to as: A command A function call An expression Or some variation of the above. They all mean the same thing: a bit of code to run. Watch our for this common mistake. it is easy to create an error by omitting the closing parenthesis of a function call: sqrt(1764 ## Error in parse(text = input): &lt;text&gt;:2:0: unexpected end of input ## 1: sqrt(1764 ## ^ 2.5 Objects - Variables and Functions R is an object-oriented programming language. This means we can use R to create abstracted objects that contain data (of any type, shape or size) called variables or procedures/methods (individual blocks of code) called functions. There are numerous functions and datasets included in the base R installation. Also, as an open source language countless programmers in the R community have written useful functions and created useful datasets that are freely available in the form of R-packages (more on these later). You can also write your own! 2.5.1 Obey R’s naming rules You want to give a valid name to an object in R. Step 1 - Choose a name that includes valid characters. Names in R may be made up of three types of characters: Capital and lowercase letters Numbers The symbols . and _ Other characters are forbidden because we use them with names to perform actions. R wouldn’t be able to distinguish the name x+y from the command x + y written without spaces. Step 2 - Double-check that the first character is a letter or .. Names in R may not start with a number or _, even though these symbols can appear anywhere else in the name. Step 3 - Assign an object to the name. Use the assignment arrow (&lt;-) with the name on the left and the object on the right. 2.5.1.1 Example Suppose we’d like to create an object storing the number of trials in an experiment. In our first attempt, we give the object the name #_of_trials. #_of_trials &lt;- 15 #_of_trials This doesn’t give us any output, because # is not a valid character for an object name. Let’s try again, using only valid characters: number_of_trials &lt;- 15 number_of_trials ## [1] 15 That worked! Now let’s record the observed value of our first trial: 1st_trial &lt;- 476.2 1st_trial ## Error in parse(text = input): &lt;text&gt;:1:2: unexpected symbol ## 1: 1st_trial ## ^ That didn’t work either. Even though all the characters were valid, object names canonly start with a letter or ., not a number or _. Let’s try again. first_trial &lt;- 476.2 first_trial ## [1] 476.2 Success! Object names can include capital and lowercase letters in any order, but names in R are case sensitive. Look what happens if you try to get the value of an object with the wrong capitalization: my_value &lt;- 100 My_Value ## Error: object &#39;My_Value&#39; not found Some names follow all of R’s rules, but can’t be used because they are reserved for a special purpose. Look what happens if you try to assign an object to the name TRUE. TRUE &lt;- 100 ## Error in TRUE &lt;- 100: invalid (do_set) left-hand side to assignment Names like TRUE, FALSE, NA, and function can’t be assigned to objects, because they already have important uses in R. 2.5.2 Variables and data types You can create objects (variables~values, large data structures~think spreadsheets and databases, and functions) using the =, &lt;- or -&gt; operators. You can see what type of data (or data type) a variable is using the class function. Go ahead, try class(x). Data in R can be of several different, basic types: Data Type aka Example Logical Boolean TRUE, FALSE Numeric float 42, 3.14, Character string ‘a’ , “good”, “TRUE”, ‘23.4’ Integer 2L, 34L, 0L Complex 3 + 2i Raw hexadecimal “Hello” is stored as 48 65 6c 6c 6f 2.5.3 Vectors Vectors in R are simply ordered lists of values. These values can be of any type (strings, numerics, Boolean, etc), but they must all be of the same type, or R will force them to be the same. We can construct vectors using the c() function. Let’s run through a quick example: col_names &lt;- c(&#39;plant&#39;, &#39;genotype&#39;) col_names ## [1] &quot;plant&quot; &quot;genotype&quot; #####???Question??? What is c abbreviating? (i.e. what is the title of the c() function?) answer here What are the arguments that you can pass to c()? answer here Now we have a vector of strings. We can access the individual elements (the values we put in our vector) using the square bracket operator. col_names[1] ## [1] &quot;plant&quot; col_names[2] ## [1] &quot;genotype&quot; #Note that the indices begin at 1 in R!!! col_names[0] ## character(0) We can also change elements or add elements to the vector using the bracket operator. col_names[2] &lt;- &#39;phenotype&#39; col_names[3] &lt;- &#39;root_length&#39; col_names ## [1] &quot;plant&quot; &quot;phenotype&quot; &quot;root_length&quot; col_names[4] &lt;- FALSE col_names ## [1] &quot;plant&quot; &quot;phenotype&quot; &quot;root_length&quot; &quot;FALSE&quot; #####???Question??? What happened to FALSE (is it a Boolean)? answer here Write a block of code to test what would happen if we instead added a character string to a vector of logical values (i.e. make a new variable containing a few Boolean values, then add a string to that vector)! What happens? answer here We can also do mathematical or logical operations on entire vectors. col_names == FALSE ## [1] FALSE FALSE FALSE TRUE vector &lt;- c(1, 2, 3, 4, 5) 6*vector ## [1] 6 12 18 24 30 vector^2 ## [1] 1 4 9 16 25 vector &gt; 2 ## [1] FALSE FALSE TRUE TRUE TRUE 2.5.4 Matrices, Arrays and Lists Matrices are two dimensional data sets and Arrays are N-dimensional data sets. Like vectors these must be made of a single data type. For more info ?matrix and ?array. Lists are more complex data structures that are similar to vectors but allow multiple data types. Lists can contain vectors as elements and even other lists! This makes them potentially N-dimensional but clunky to work with. You might encounter them if you use R in the future. For more info ?list. 2.5.5 Data frames Variables in R are not limited to just strings or integers or even matrices. You can store and operate on entire spreadsheets with columns of defined data types, using what R calls ‘data frames’. Data frames have columns that are made of vectors. The data frame is one of the most fundamental data structures used in R. ?data.frame provides a wealth of knowledge about data frames, but let’s just go ahead and make one! Run the following code. L3 &lt;- LETTERS[1:3] fac &lt;- sample(L3, 10, replace = TRUE) d &lt;- data.frame(x = 1, y = 1:10, fac = fac) #notice how the columns of the data frame can be named using &#39;=&#39;, just as if we were creating individual vectors d ## x y fac ## 1 1 1 A ## 2 1 2 C ## 3 1 3 B ## 4 1 4 C ## 5 1 5 C ## 6 1 6 C ## 7 1 7 B ## 8 1 8 C ## 9 1 9 C ## 10 1 10 C class(d) ## [1] &quot;data.frame&quot; #####???Questions??? What is LETTERS? What is L3? answer here What does sample do? answer here What does setting the replace argument of sample to TRUE do? Try sample(L3, 10, replace = FALSE) Now we have a data frame d with 10 rows and 3 columns. You can retrieve individual columns using the $ operator. Try it, d$fac!. Wait a minute, why is this no longer a column? The columns of a data frame are actually just vectors. #####???Question??? What class of data is d$fac? answer here You can also create new columns using the $ operator. For example we could make a column called new_column that contains \"new_column\" by executing d$new_column &lt;- \"new_column\". 2.5.5.1 Factors Factors used to be an efficient way of storing large vectors of repetitive discrete or categorical data. Factors do this by translating the potentially long individual pieces of data into integers, using a table called levels. Try levels(d$fac). This gives us a list of all the unique possible values in d$fac. R creates a key (1 = A, 2 = B, 3 = C) to read and write this factor. In this way long level values, like sentences, or large datasets, like thousands of lines, are compressed. To see the compressed version of d$fac we can use as.integer(d$fac). R now stores large data structures by indexing values like this regardless of whether it’s a factor or not. Despite this fact there are still some useful features of factors. For example, if we are adding a dataset from a new replicate of an experiment to an existing dataset, columns that are factors will only allow us to add values that match our existing levels. This will often help you find typos in your dataset. Additionally, some functions require factor variables, like the ANOVA functions we will use later. &gt; &gt;Giving ?factor a look, you will see that we can also assign a particular order to the levels of a factor. This can be handy for ordering variables when plotting. We can also assign labels to the levels, just in case your level names are too abstracted to be understandable. &gt; &gt;However when manipulating data frames containing factors you must be careful because some functions may interpret factors as their integer values! We could also avoid creating a factor in our data frame and just keep this column as characters by including stringsAsFactors = F in our call to data.frame(). Going back to our data frame d, similar to vectors we can access rows, columns and elements of the data frame using the square bracket operator. I’ll suppress the output below and let you run these examples yourself. #get the first row of d d[1,] #get the first column of d d[,1] #get the column named &#39;fac&#39; d[,&#39;fac&#39;] #or d[[&#39;fac&#39;]] #or (most efficient and readable) d$fac #get the element in the 5th row and 3rd column d[5,3] We can also perform calculations or other operations on the elements of a data frame. d[,2] + 1 d[[2]] + 1 d[,2] * 2 #similarly for logical operations, note that logical &#39;is equivalent to&#39; is &#39;==&#39; d[,3] == &#39;B&#39; d$y &lt;= 5 #we can also use functions to perform complex calculations mean(d$y) median(d$y) sum(d$y) Just like with vectors we can change elements or add elements to a data frame. #####???Question??? How would you add a column to d with the integer values representing d$fac? answer here What is the mean of your new column of d? Copy the code you used. answer here What is the median or your new column of d? Copy the code you used. answer here What is the sum of your new column? Copy the code you used. answer here What fraction of the sum of your new column is each row’s value? Make a new column for d showing this fraction. Copy the code you used. answer here 2.5.6 Run a function You want to run a function on a single value, for instance, to perform a calculation. Step 1 - Write the name of the function. Each function in R is saved as an object. To use an object, we call its name. Step 2 - Type a pair of parentheses ( ) after the name. Parentheses tell R that you would like to run the function that is stored in the object. Step 3 - Place any input inside of the parentheses. Most functions need a value to do their job, like abs(-4) or round(pi). Step 4 - Click Run. Or press Enter if you are using R from a command line. R will run your function and return the result. 2.5.6.1 Example Suppose we want to apply a trigonometric function, like sine, cosine, or tangent, to an angle (in radians). In R, the functions are sin(), cos(), and tan(). To call cosine, we first write its name cos. Next, we place a pair of parentheses after the name. Our call to the cos() function is not yet complete, since cosine needs an angle to do its computation. If we run our command in this unfinished state, R will return an error message. cos() ## Error in cos(): 0 arguments passed to &#39;cos&#39; which requires 1 We can complete the command in one of two ways. We can give cos() a specific angle to act on, in this case, 0. cos(0) ## [1] 1 Or we could give cos() an object that contains an angle to act on, such as pi. cos(pi) ## [1] -1 Some of the most common mathematical functions in R are: Function Operation Example sin() Sine sin(pi/2) cos() Cosine cos(3*pi/2) tan() Tangent tan(pi/4) sign() Sign sign(-2), i.e. is -2 positive or negative? log() Natural logarithm log(2.718282) exp() Exponential exp(1) floor() Integer floor floor(pi), i.e. what is the largest integer less than pi? ceiling() Integer ceiling ceiling(pi), i.e. what is the smallest integer greater than pi? 2.6 Tidyverse tables The programmer who wrote the R for Data Science book, along with his research group and now company Posit, have written and maintain several packages of R code functions to deal with data from reading and writing to tidying and wrangling. These packages are called the Tidyverse. You can install all of these packages at once using install.packages(&quot;tidyverse&quot;) We will use some of the functions from these packages below to deal with tables of data. 2.6.1 Create a Table Manually You want to create a data frame by typing in each value by hand. This is an alternative to reading a file that contains the data. Step 1 - Call tibble::tibble(). tibble() constructs a tibble, a type of data frame. Step 2 - Choose a column name for your tibble. Pass the name as an argument name to tibble(), e.g. tibble(col_1) Step 3 - Provide a vector of values for the column. Assign them to the column name, e.g. tibble(col_1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) Step 4 - Repeat for every column in your tibble. Every column should have the same number of values. If you pass a column a single value, tibble() will repeat that value for each row of the data frame. Remember to separate each new argument/column name with a comma. Step 5 - Save the tibble to an object, so you can access it later. data &lt;- tibble( col_1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), col_2 = 1:3 ) 2.6.1.1 Example We want to create a data frame to keep track of: The teachers at Grove Middle School The number of students in their classrooms The grade levels of the students We begin by loading the tibble package which contains tibble(). Next we input our data into tibble(). Our tibble will have the column names teacher, class_size, and grade with the values provided below. library(tibble) tibble(teacher = c(&quot;Gaines&quot;, &quot;Johnson&quot;, &quot;Hernandez&quot;), class_size = c(30, 26, 28), grade = c(6, 7, 8)) ## # A tibble: 3 × 3 ## teacher class_size grade ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gaines 30 6 ## 2 Johnson 26 7 ## 3 Hernandez 28 8 After checking out the created tibble, we realize it would be a good idea to include the school name in the data frame in case we decide to include other schools in the future. If we pass only a single value to this column, R will use its recycling rules to reuse that value for each row in the tibble. tibble( teacher = c(&quot;Gaines&quot;, &quot;Johnson&quot;, &quot;Hernandez&quot;), class_size = c(30, 26, 28), grade = c(6, 7, 8), school = &quot;Grove MS&quot;) ## # A tibble: 3 × 4 ## teacher class_size grade school ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Gaines 30 6 Grove MS ## 2 Johnson 26 7 Grove MS ## 3 Hernandez 28 8 Grove MS Lastly, we assign the created tibble to the object, teachers, so we can access is later. teachers &lt;- tibble( teacher = c(&quot;Gaines&quot;, &quot;Johnson&quot;, &quot;Hernandez&quot;), class_size = c(30, 26, 28), grade = c(6, 7, 8), school = &quot;Grove MS&quot;) 2.6.2 tribble() You can also define your tibble row by row with tibble:tribble(). Place a ~ before each value in the first row to indicate that the values are column names. If you watch your spaces, tribble() will provide a low-fi preview of your table as you write it. tribble( ~teacher, ~class_size, ~grade, ~school, &quot;Gaines&quot;, 30, 6, &quot;Grove MS&quot;, &quot;Johnson&quot;, 26, 7, &quot;Grove MS&quot;, &quot;Hernandez&quot;, 28, 8, &quot;Grove MS&quot;) ## # A tibble: 3 × 4 ## teacher class_size grade school ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Gaines 30 6 Grove MS ## 2 Johnson 26 7 Grove MS ## 3 Hernandez 28 8 Grove MS Other ways to read in raw data Any of the readr::read_* functions, such as readr::read_csv() or readr::read_delim(), can be used to create a table manually as a character string wrapped in I(). readr::read_csv(I(&quot;col_1,col_2\\na,1\\nb,2\\nc,3&quot;)) ## Rows: 3 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): col_1 ## dbl (1): col_2 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3 × 2 ## col_1 col_2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 2 ## 3 c 3 2.6.3 Read a CSV file (.csv) You want to read a CSV file into your R session, where you can manipulate its contents. The file has the extension .csv. A CSV file is a text file that contains a table whose values are separated by commas, i.e. a Comma Separated Values file. Step 1 - Call readr::read_csv(). read_csv() is designed to read in .csv files with , as the field separator. Step 2 - Give read_csv() the filepath to your file as a character string. R will read the filepath as if it begins at your working directory. For example: read_csv(&quot;my/file.csv&quot;) Step 3 - Save the output to an object, so you can access it later. csv_table &lt;- read_csv(&quot;my/file.csv&quot;) 2.6.3.1 Example We want to read in the drought records dataset which is a CSV file. We have this file saved on cloud at /data/Drought_paneldata.csv. We begin by loading the readr package which contains read_csv(). Next, we pass read_csv() the filepath for our CSV file in order to read in the dataset. The file is in the data folder. The output looks like this: library(readr) read_csv(&quot;data/Drought_paneldata.csv&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## date (3): Time_fornow_enddate, StartDate, EndDate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 137 × 12 ## CountyFP Time_fornow_enddate StartDate EndDate DurationDays Hazard ## &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10003 2016-09-10 2016-09-10 2016-09-10 1 Heat ## 2 10003 2016-08-20 2016-08-20 2016-08-20 1 Heat ## 3 10005 2014-06-19 2014-06-18 2014-06-19 2 Heat ## 4 10003 2013-07-20 2013-07-15 2013-07-20 6 Heat ## 5 10003 2011-06-09 2011-06-08 2011-06-09 2 Heat ## 6 10001 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 7 24011 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 8 24015 2010-06-24 2010-06-23 2010-06-24 2 Heat ## 9 10001 2006-08-03 2006-08-01 2006-08-03 3 Heat ## 10 10003 2006-08-03 2006-08-01 2006-08-03 3 Heat ## # ℹ 127 more rows ## # ℹ 6 more variables: CropDmg &lt;dbl&gt;, avg_tmax &lt;dbl&gt;, avg_tmin &lt;dbl&gt;, ## # avg_ppt &lt;dbl&gt;, unique_id &lt;chr&gt;, duplicate &lt;lgl&gt; Notice that read_csv() automatically chose intelligent data types for each of the columns. Lastly, we assign the dataset read in by read_csv() to an object, Drought_rec, so we can access it later. Duoght_rec &lt;- read_csv(&quot;data/Drought_paneldata.csv&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## date (3): Time_fornow_enddate, StartDate, EndDate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Duoght_rec ## # A tibble: 137 × 12 ## CountyFP Time_fornow_enddate StartDate EndDate DurationDays Hazard ## &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10003 2016-09-10 2016-09-10 2016-09-10 1 Heat ## 2 10003 2016-08-20 2016-08-20 2016-08-20 1 Heat ## 3 10005 2014-06-19 2014-06-18 2014-06-19 2 Heat ## 4 10003 2013-07-20 2013-07-15 2013-07-20 6 Heat ## 5 10003 2011-06-09 2011-06-08 2011-06-09 2 Heat ## 6 10001 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 7 24011 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 8 24015 2010-06-24 2010-06-23 2010-06-24 2 Heat ## 9 10001 2006-08-03 2006-08-01 2006-08-03 3 Heat ## 10 10003 2006-08-03 2006-08-01 2006-08-03 3 Heat ## # ℹ 127 more rows ## # ℹ 6 more variables: CropDmg &lt;dbl&gt;, avg_tmax &lt;dbl&gt;, avg_tmin &lt;dbl&gt;, ## # avg_ppt &lt;dbl&gt;, unique_id &lt;chr&gt;, duplicate &lt;lgl&gt; read_csv() comes with many arguments that you can use to customize which parts of the file will be read in and how. Here are a few of the most useful: Argument Description col_names Should the first row be read in as column names? Defaults to TRUE. Can also be a character vector of column names. col_types Explicitly set the data type for each column. skip Number of rows to skip before reading any data. guess_max Maximum number of lines to read for guessing column types. Read the help page at ?read_csv to learn more. 2.6.4 Read a character-delimited file (.txt) You want to read a delimited data file into your R session, where you can manipulate its contents. The file typically has the file extension .txt. Within each row of a delimited file, the column fields are separated by a character delimiter. Some common choices for the delimiter are |, :, ;, ,, or ~. Step 1 - Call readr::read_delim(). Step 2 - Give read_delim() the filepath to your dataset as a character string. For example: read_delim(&quot;my/file.txt&quot;) Step 3 - Specify the delim argument to tell read_delim() the delimiter character. read_delim(&quot;my/file.txt&quot;, delim = &quot;,&quot;) Step 4 - Save the output to an object, so you can access it later. delim_table &lt;- read_delim(&quot;my/file.txt&quot;, delim = &quot;,&quot;) 2.6.4.1 Example We want to read in the drought records dataset in text format which is a delimited dataset .txt file and uses the delimiting character ,. This dataset contains drought/heat hazard records and associated weather characteristics for different dates and counties in Delmarva Peninsula. We have this file saved on cloud as data/Drought_paneldatatx.txt. We begin by loading the readr package which contains read_delim(). Then, we pass read_delim() the filepath for our .txt file in order to read in the dataset. The file is in the data folder in our working directory. The output looks like this: library(readr) read_delim(&quot;data/Drought_paneldatatx.txt&quot;, delim = &quot;,&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Time_fornow_enddate, StartDate, EndDate, Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 137 × 12 ## CountyFP Time_fornow_enddate StartDate EndDate DurationDays Hazard CropDmg ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 10003 9/10/2016 9/10/2016 9/10/2016 1 Heat 0 ## 2 10003 8/20/2016 8/20/2016 8/20/2016 1 Heat 0 ## 3 10005 6/19/2014 6/18/2014 6/19/2014 2 Heat 0 ## 4 10003 7/20/2013 7/15/2013 7/20/2013 6 Heat 0 ## 5 10003 6/9/2011 6/8/2011 6/9/2011 2 Heat 0 ## 6 10001 6/9/2011 6/9/2011 6/9/2011 1 Heat 0 ## 7 24011 6/9/2011 6/9/2011 6/9/2011 1 Heat 0 ## 8 24015 6/24/2010 6/23/2010 6/24/2010 2 Heat 0 ## 9 10001 8/3/2006 8/1/2006 8/3/2006 3 Heat 0 ## 10 10003 8/3/2006 8/1/2006 8/3/2006 3 Heat 0 ## # ℹ 127 more rows ## # ℹ 5 more variables: avg_tmax &lt;dbl&gt;, avg_tmin &lt;dbl&gt;, avg_ppt &lt;dbl&gt;, ## # unique_id &lt;chr&gt;, duplicate &lt;lgl&gt; Notice that read_delim() automatically chose intelligent data types for each of the columns. Lastly, we assign the solar dataset read in by read_delim() to an object, solar, so we can access it later. data &lt;- read_delim(&quot;data/Drought_paneldatatx.txt&quot;, delim = &quot;,&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Time_fornow_enddate, StartDate, EndDate, Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. read_delim() is a more general case of readr::read_csv() and readr::read_tsv() read_csv() is the equivalent of calling read_delim() with delim = \",\". read_tsv() is the equivalent of read_delim() with delim - \"\\t\". 2.6.5 Read an Excel file (.xls, .xlsx) You want to read a Microsoft Excel file into your R session, where you can manipulate its contents. The file has the extension .xls or .xlsx. Step 1 - Call readxl::read_excel(). read_excel() is designed to read in .xls or .xlsx files. Step 2 - Give read_excel() the filepath to your file as a character string. For example: read_excel(&quot;my/file.xlsx&quot;) Step 3 - Specify the sheet you want to read in with the sheet argument. By default, read_excel() reads in the first sheet of an Excel file. You can set sheet to the name of a different sheet (as a character string) or the location of a different sheet (as a number). read_excel(&quot;my/file.xlsx&quot;, sheet = &quot;Sheet_B&quot;) Step 4 - Save the output to an object, so you can access it later. xl_table &lt;- read_excel(&quot;my/file.xlsx&quot;, sheet = &quot;Sheet_B&quot;) 2.6.5.1 Example We want to read in an hazards dataset which is a Excel file containing financial agricultural loss records and associated weather conditions for three types of climatic hazards. Hazard column defines the hazard type (drought, storm, heat). We have saved the dataset on cloud at /data/hazards.xlsx. We begin by loading the readxl package which contains read_excel(). Next, we pass read_excel() the filepath for our Excel file in order to read in the dataset. The file is in our working directory, therefore we do not need the entire filepath. We only need the portion that goes from our working directory to the file. The output looks like this: library(readxl) read_excel(&quot;data/hazards.xlsx&quot;) ## # A tibble: 1,321 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1989-07-01 00:00:00 24045 7.93 3.75 3.29 ## 2 1990-06-01 00:00:00 24045 1.34 0.823 3.83 ## 3 1990-07-01 00:00:00 10003 4.29 1.31 1.90 ## 4 1990-07-01 00:00:00 24011 4.96 0.902 1.90 ## 5 1990-07-01 00:00:00 24019 4.24 0.919 1.65 ## 6 1990-07-01 00:00:00 24045 4.15 0.969 1.52 ## 7 1990-08-01 00:00:00 24015 7.53 1.49 1.65 ## 8 1990-08-01 00:00:00 10001 5.71 0.811 1.10 ## 9 1990-09-01 00:00:00 24045 1.82 0.705 3.03 ## 10 1990-10-01 00:00:00 24045 2.30 0.927 3.71 ## # ℹ 1,311 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; Looking at the “Hazard” column of the data frame, we realize read_excel() automatically read in the first sheet of the file. We are looking for data on storm hazard events, not drought. However, we cannot remember what the sheet was named. We can call readxl::excel_sheets() on the filepath to see the names of the sheets in the “hazards” file. excel_sheets(path = &quot;data/hazards.xlsx&quot;) ## [1] &quot;drought&quot; &quot;storm&quot; &quot;heat&quot; Now we can see that we wanted to read in Sheet 2 from the file. We can do this by specifying the sheet argument in read_excel(). read_excel(path = &quot;data/hazards.xlsx&quot;, sheet = 2) ## # A tibble: 1,609 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981-06-01 00:00:00 24015 4.21 0.826 1.23 ## 2 1982-05-01 00:00:00 24015 3.67 2.02 4.06 ## 3 1983-04-01 00:00:00 24019 7.03 2.21 2.33 ## 4 1983-07-01 00:00:00 24047 1.94 0.915 5.32 ## 5 1983-08-01 00:00:00 24015 2.00 0.992 6 ## 6 1984-03-01 00:00:00 51001 6.85 1.77 1.87 ## 7 1984-05-01 00:00:00 10005 7.64 2.60 3.81 ## 8 1985-09-01 00:00:00 51001 5.73 4.65 4.53 ## 9 1988-05-01 00:00:00 10003 6.03 2.12 1.68 ## 10 1989-03-01 00:00:00 10003 4.57 1.06 3.23 ## # ℹ 1,599 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; Lastly, we assign the earthquake data frame read in by read_excel() to an object so we can access it later. storm &lt;- read_excel(path = &quot;data/hazards.xlsx&quot;, sheet = 2) storm ## # A tibble: 1,609 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981-06-01 00:00:00 24015 4.21 0.826 1.23 ## 2 1982-05-01 00:00:00 24015 3.67 2.02 4.06 ## 3 1983-04-01 00:00:00 24019 7.03 2.21 2.33 ## 4 1983-07-01 00:00:00 24047 1.94 0.915 5.32 ## 5 1983-08-01 00:00:00 24015 2.00 0.992 6 ## 6 1984-03-01 00:00:00 51001 6.85 1.77 1.87 ## 7 1984-05-01 00:00:00 10005 7.64 2.60 3.81 ## 8 1985-09-01 00:00:00 51001 5.73 4.65 4.53 ## 9 1988-05-01 00:00:00 10003 6.03 2.12 1.68 ## 10 1989-03-01 00:00:00 10003 4.57 1.06 3.23 ## # ℹ 1,599 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; read_excel() comes with many arguments that you can use to customize which parts of the spreadsheet will be read in and how. Here are a few of the most useful: Argument Description col_names Should the first row be read in as column names? Defaults to TRUE. Can also be a character vector of column names. col_types Explicitly set the data type for each column. skip Number of rows to skip before reading any data. range Specify a subset of cells to read in. Read the help page at ?read_excel to learn more. "],["the-r-language-and-tidy-data-examples.html", "The R Language and Tidy Data Examples 2.7 Tidy Data 2.8 R Basics 2.9 Data, tibbles, dataframes", " The R Language and Tidy Data Examples 2.7 Tidy Data Each of the following datasets shows TB cases and some other variables per country organized in different ways. table1 ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 table3 ## # A tibble: 6 × 3 ## country year rate ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 # Spread across two tibbles table4a # cases ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4b # population ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 There are three interrelated rules which make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Which table above is Tidy? table1 ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 table3 ## # A tibble: 6 × 3 ## country year rate ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 # Spread across two tibbles table4a # cases ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4b # population ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 Why ensure that your data is tidy? There are two main advantages: There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. As you learned in mutate and summary functions, most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural. 2.7.1 Let’s make a data dictionary for this dataset country = The country in which TB case data was reported. year = Calendar year cases = Test-positive cases with culture-based and ELISA-based tests in these countries with sample dates within the year above. population = The self-reported population of each country according to their census data. 2.8 R Basics 2.8.1 Data types You can create objects (variables~values, large data structures~think spreadsheets and databases, and functions) using the =, &lt;- or -&gt; operators. You can see what type of data (or data type) a variable is using the class function. Go ahead, try class(x). Data in R can be of several different, basic types: Data Type aka Example Logical Boolean TRUE, FALSE Numeric float 42, 3.14, Character string ‘a’ , “good”, “TRUE”, ‘23.4’ Integer 2L, 34L, 0L Complex 3 + 2i Raw hexadecimal “Hello” is stored as 48 65 6c 6c 6f 2.8.2 Functions What is a function? function_name(argument_name = argument_value) Using Tab-complete to make function calls will prevent errors! 2.8.3 Objects An object is essentially anything that shows up in the Environment pane! functions variables data objects 2.8.4 Vectors To demonstrate what a vector is let’s load some data! 2.9 Data, tibbles, dataframes 2.9.1 Reading in data library(readxl) storm &lt;- read_excel(path = &quot;data/hazards.xlsx&quot;, sheet = 2) storm ## # A tibble: 1,609 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981-06-01 00:00:00 24015 4.21 0.826 1.23 ## 2 1982-05-01 00:00:00 24015 3.67 2.02 4.06 ## 3 1983-04-01 00:00:00 24019 7.03 2.21 2.33 ## 4 1983-07-01 00:00:00 24047 1.94 0.915 5.32 ## 5 1983-08-01 00:00:00 24015 2.00 0.992 6 ## 6 1984-03-01 00:00:00 51001 6.85 1.77 1.87 ## 7 1984-05-01 00:00:00 10005 7.64 2.60 3.81 ## 8 1985-09-01 00:00:00 51001 5.73 4.65 4.53 ## 9 1988-05-01 00:00:00 10003 6.03 2.12 1.68 ## 10 1989-03-01 00:00:00 10003 4.57 1.06 3.23 ## # ℹ 1,599 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; Each column is a vector! # check whether something is a vector with is.vector() # but some vectors are special like dates with formatting # use head(), summary(), or view() to look at data 2.9.2 Factors Factors are categorical variables. # look at a factor variable # can you add new values to factors? How would we check that a variable only contains certain values? storm$monthly.ppt &gt; 7 ## [1] FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [13] TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE TRUE ## [25] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE ## [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## [157] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE ## [169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [181] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [253] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [277] FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE ## [289] FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE ## [385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [433] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [445] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [481] FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [493] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [517] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [553] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE ## [565] TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## [577] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [589] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [601] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [661] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [685] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [697] TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [721] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [733] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [745] TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [793] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [829] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [877] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [889] FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE FALSE ## [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [913] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE ## [925] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE TRUE ## [937] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [961] FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [973] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [985] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1009] FALSE FALSE FALSE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE ## [1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1045] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1057] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1069] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1081] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE ## [1093] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [1105] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1117] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1129] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1141] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1153] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE ## [1165] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1177] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1189] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1201] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE ## [1213] TRUE FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE ## [1225] TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1237] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1249] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1261] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1273] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1285] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1297] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1309] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1321] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1333] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1345] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1357] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1369] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1381] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1393] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1405] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1417] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1429] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## [1441] TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [1453] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1465] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1477] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1489] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1501] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1513] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1525] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE ## [1537] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1549] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [1561] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1573] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## [1585] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [1597] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1609] FALSE "],["functions-best-coding-practices-and-debugging-intro.html", "3 Functions, Best-coding Practices, and Debugging Intro 3.1 Functions 3.2 Packages 3.3 Best coding practices 3.4 Debugging", " 3 Functions, Best-coding Practices, and Debugging Intro Now that we have introduced Rstudio, markdown, R basics, and Tidy data, it is our goal this week to improve our programming skills such that as we go forward you will be able to practice your problem solving efficiently by staying organized, avoiding common errors, and effectively debugging errors when they occur. This week our goals are to be able to: Understand the value of abstraction and “reusability” provided by functional programming Create basic functions Use Packages to access functions from the R community Organize code into logical blocks and use comments to help understand code Use RStudio’s formatting tools to avoid common errors Identify common errors and strategies for fixing/debugging them Reading: Skim https://r4ds.hadley.nz/program Skim https://rstudio-education.github.io/hopr/basics.html#functions down through 2.7 Summary Skim https://rstudio-education.github.io/hopr/packages.html Skim https://r4ds.hadley.nz/workflow-style Skim one of https://statsandr.com/blog/top-10-errors-in-r/ https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html Ooof, videos on these topics were pretty rough. Let me know if you find any that you liked! 3.1 Functions Functions in programming are just like functions in math. In R they are even written just like they are in math \\(f(x)\\) in math is the same as F(x) in R Structured/Modular/Functional programming: a general programming concept where developers separate program functions into independent, modular, organized pieces, primarily functions. 3.1.1 Functions… why and how Write functions to encapsulate sections of code Allows you to avoid global variables that can be accessed and changed anywhere (functions only know about arguments you specifically pass to them) To avoid duplicating code: if you have multiple copies of almost identical code, put it into a function Create a function when R does not have a built-in function for your needs When running an analysis over and over again, can just call the function (one line) instead of running many lines of code 3.1.2 A simple function: CV() There is no R function for CV, or coefficient of variation (standard deviation divided by mean) # create the CV function CV &lt;- function(xvec) { CV.value &lt;- sd(xvec)/mean(xvec) return(CV.value) } Our CV function takes a single argument xvec which has no default value. (We could set a default value of 1 for example by using xvec = 1 inside the function(...) function). It calculates the standard deviation divided by the mean and returns the result. The return(CV.value) indicates what is returned by the function. If there is no return function the result of the last line is returned. 3.1.3 Now we need to apply our function. # pick 1000 random values from a normal distribution with mean 5 and sd 2 values &lt;- rnorm(n=1000, mean=5, sd=2) # now lets test our function CV(values) ## [1] 0.3995964 3.1.4 What objects are available? Check out the environment now. Only the new function CV and the object values are available in the Global Environment (they are global objects) Object CV.value (defined inside the function) is encapsulated within the function CV and not available outside it 3.1.5 Optional arguments Some functions have optional arguments. Optional arguments often have a default value, that is a value that is used if one isn’t provided by the user. For example, our CV function doesn’t work if xvec includes missing values. # Set the 100th value in values to NA values[100] &lt;- NA CV(values) ## [1] NA We could add an optional argument to allow it to remove NA values, as sd and mean both have na.rm optional arguments. When set to na.rm = TRUE, NA values are removed before the respective calculation. CVnew &lt;- function(xvec, na.rm=F) { CV.value &lt;- sd(xvec, na.rm=na.rm)/mean(xvec, na.rm=na.rm) return(CV.value) } ## this code could be tidied up, with Reformat Code ## try highlighting the code and using Code&gt;Reformat Code in the menu bar # Test out the new function CVnew(values) ## [1] NA CVnew(values, na.rm = T) ## [1] 0.3998223 3.2 Packages Packages, as we have discussed, are collections of functions, code, and data collected and curated by the R community. Packages are available through 3 primary sources CRAN https://cran.r-project.org/ see “Packages” on the left Bioconductor https://www.bioconductor.org/ GitHub CRAN and Bioconductor packages have undergone some peer review GitHub packages may or may not have been reviewed 3.2.1 Installing and using packages To install CRAN packages you can use the install.packages function with the argument being a character string of the package name use the Packages pane (which runs install.packages for you) Bioconductor and Github packages are a bit more tricky requiring packages to install their packages Bioconductor install instructions https://www.bioconductor.org/install/ To install Github packages you can use the remotes package https://cran.r-project.org/web/packages/remotes/index.html 3.2.2 Installing and using packages Clicking on the link to a package in the package pane takes you to the documentation of the package Documentation for most all CRAN and Bioconductor include description files help pages for their functions examples for their functions Many also have “vignettes” which are worked examples of how to use the package You can find all of this in the Help pane for that package. 3.3 Best coding practices 3.3.1 Reproducibility I (almost) never save my R workspace It contains global variables that could mess up your future code I regularly clear the workspace or run rm(list=ls()) Much better practice to rerun the entire script in an empty workspace Exception Results from analyses that take hours or days to run 3.3.2 Style guides Big companies have strict guidelines on how to organize your code, e.g. Google Style Guide https://google.github.io/styleguide/Rguide.html http://adv-r.had.co.nz/Style.html (Hadley Wickham’s guide) Indent your code inside a function, inside an if-statement, inside a for-loop, and if a statement goes over two lines Use meaningful object names Too long and you will get tired of typing Too short and you won’t know what the object contains Objects in R are global and available everywhere (more on this later) 3.3.3 Automated code styling/formatting Fortunately people have also written code to properly style your code according to common guidelines. In R-studio you can go to “Code” in the menu bar and “Reformat Code”. There is also a package called “styler” described a bit in https://r4ds.hadley.nz/workflow-style 3.4 Debugging The origin of debugging comes from literally removing a moth that shorted out one of the early computers at NASA. Admiral Grace Hopper said in a famous report they were “debugging” the computer and the term has stuck for all computers and code. 3.4.1 Verifying code Test your code, test your code, test your code! Write the smallest possible amount of code (a portion of one line if possible) Then try simple examples you know the answer to (zero, negative number, positive number) Show the results: is this what you expected? Pay special attention if you are copying sections of code and changing a variable name (common to forget to change all occurrences) 3.4.2 Commenting your code R ignores everything on a line that follows a # Comment at the top of your script/markdown What the code does, your name, email, date started Comment before each function or section of code What is the purpose of that section of code, what does it do Comment throughout Whenever an unusual function is used Whenever the code is hard to understand Whenever an algorithm is particularly useful 3.4.3 “Commenting out” code Instead of deleting code you might not need, or When you make modifications to your code, or During debugging Copy the code that works then comment it out by prefixing it with # Change the new copy of the code If you need to revert to the old code, just remove the # before each line (“uncomment”) Delete the old commented out code only once you have thoroughly tested the new code ctrl+shift+C is a shortcut in Rstudio to comment/uncomment large blocks of code #plot(iris$Sepal.Length) plot(iris$Petal.Length) 3.4.4 Common errors and How to Fix Them Semantic errors = mistyping errors Missing parentheses/brackets Missing quotes Misplaced commas Misspelled object names R is case-sensitive Resolving semantic errors Semantic errors generally have useful error messages, except missing parentheses brackets Always make sure there is a &gt; in the console! A + means the last line wasn’t completed. Use ESC to get from + back to &gt; 3.4.4.1 Missing Parentheses or Brackets If you forget a closing parenthesis ), bracket ], or curly brace }, R will show a continuation prompt in the console (+), meaning it’s waiting for you to finish the expression. Example (Missing Parenthesis) mean(c(1, 2, 3) # Missing closing parenthesis ## Error in parse(text = input): &lt;text&gt;:2:0: unexpected end of input ## 1: mean(c(1, 2, 3) # Missing closing parenthesis ## ^ Error Message: None, but look in the console + # Cursor is stuck at the continuation prompt Fix: Press ESC to return to the &gt; prompt, then add the missing ). mean(c(1, 2, 3)) # Corrected ## [1] 2 3.4.4.1.1 Missing or Mismatched Quotes If you forget to close a string with a quotation mark (\" or '), R doesn’t know where the string ends. Example (Missing Quote) x &lt;- &quot;Hello # Missing closing quote ## Error in parse(text = input): &lt;text&gt;:1:6: unexpected INCOMPLETE_STRING ## 1: x &lt;- &quot;Hello # Missing closing quote ## ^ Error Message: None, but again look in the console + # Cursor is stuck at the continuation prompt Fix: Press ESC to return to the &gt; prompt, then add the missing ). x &lt;- &quot;Hello&quot; # Corrected 3.4.4.2 Unexpected Symbols (e.g., Typos, Missing Commas) If you forget a comma between function arguments or mistype a variable name, R will return an “unexpected symbol” error. Example (Missing Comma) x &lt;- c(1 3, 5, 7) # Missing comma between 1 and 3 ## Error in parse(text = input): &lt;text&gt;:1:10: unexpected numeric constant ## 1: x &lt;- c(1 3 ## ^ Error Message: Error: unexpected numeric constant in \"x &lt;- c(1 3\" Fix: Add the missing comma. x &lt;- c(1, 3, 5, 7) # Corrected 3.4.4.3 Object Not Found (Using Undefined Variables and Functions) If you try to use a variable or function that hasn’t been defined, R will return an object not found error. Example (Misspelled Function) meann(c(1, 2, 3)) # Function name is incorrect ## Error in meann(c(1, 2, 3)): could not find function &quot;meann&quot; Error Message: Error in meann(c(1, 2, 3)) : could not find function \"meann\" Fix: Check for typos and use tab-completion. mean(c(1, 2, 3)) # Corrected ## [1] 2 Example (Undefined Variable) y &lt;- x + 10 # x has not been defined yet Error Message: Error: object 'x' not found Fix: Make sure x exists before using it. x &lt;- 5 y &lt;- x + 10 # Now it works 3.4.4.4 Type Mismatches (Coercion Issues) R automatically converts mixed types in vectors. If you try to perform numeric operations on character strings, you’ll get an error or warning. Example (Mixing Numbers and Text) x &lt;- c(1, 3, &quot;Emma&quot;) # Mixed data types as.numeric(x) # Convert to numeric ## Warning: NAs introduced by coercion ## [1] 1 3 NA Warning message: Warning: NAs introduced by coercion Fix: Ensure all elements are numeric or remove text values. x &lt;- c(1, 3, 5) # All numeric as.numeric(x) # Works fine ## [1] 1 3 5 If you must handle mixed data, filter out text: x &lt;- c(1, 3, &quot;Emma&quot;) x_numeric &lt;- as.numeric(x[!is.na(as.numeric(x))]) # Removes text ## Warning: NAs introduced by coercion 3.4.5 Warning or Error? Warnings are OK, just meant to inform you about something you might not intend to do. x &lt;- c(1, 3, 7, &quot;Emma&quot;) as.numeric(x) ## Warning: NAs introduced by coercion ## [1] 1 3 7 NA Errors mean the code didn’t run properly x &lt;- c(1 3, 7, &quot;Emma&quot;) as.numeric(x) ## Error in parse(text = input): &lt;text&gt;:1:10: unexpected numeric constant ## 1: x &lt;- c(1 3 ## ^ 3.4.6 Debugging is scientific Hypothesis: If I execute this line of code, then variable A will change from value x to value y Method: create an observation to report the value of A, then run the line of code Results: is A == y? Discussion: If A == y, then we move to our next investigation. A != y then you have found the bug (or your hypothesis is wrong) "],["functions-best-coding-practices-and-debugging-examples.html", "Functions, Best-coding Practices, and Debugging Examples 3.5 Functions 3.6 Best coding practices 3.7 Debugging", " Functions, Best-coding Practices, and Debugging Examples 3.5 Functions 3.5.1 Example 1 Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? mean(is.na(x)) mean(is.na(y)) mean(is.na(z)) x / sum(x, na.rm = TRUE) y / sum(y, na.rm = TRUE) z / sum(z, na.rm = TRUE) round(x / sum(x, na.rm = TRUE) * 100, 1) round(y / sum(y, na.rm = TRUE) * 100, 1) round(z / sum(z, na.rm = TRUE) * 100, 1) percent_total &lt;- function(xvec) { mean(is.na(x)) x / sum(x, na.rm = TRUE) round(x / sum(x, na.rm = TRUE) * 100, 1) } percent_total(xvec = 1) ## Error in percent_total(xvec = 1): object &#39;x&#39; not found Note that clicking show traceback will allow you to see where the error occured. percent_total &lt;- function(xvec) { mean(is.na(xvec)) # % of values that are NA xvec / sum(xvec, na.rm = TRUE) return(round(xvec / sum(xvec, na.rm = TRUE) * 100, 1)) } percent_total(xvec = c(1, NA, 1)) ## [1] 50 NA 50 mean(is.na(c(1, NA, 1))) ## [1] 0.3333333 library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors percent_total &lt;- function(xvec) { tibble(# % of values that are NA mean(is.na(xvec)), # fraction of total for each value xvec / sum(xvec, na.rm = TRUE), # percent of total to 1 decimal round(xvec / sum(xvec, na.rm = TRUE) * 100, 1)) } percent_total(xvec = c(1, NA, 1)) ## # A tibble: 3 × 3 ## `mean(is.na(xvec))` `xvec/sum(xvec, na.rm = TRUE)` round(xvec/sum(xvec, na.r…¹ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.333 0.5 50 ## 2 0.333 NA NA ## 3 0.333 0.5 50 ## # ℹ abbreviated name: ¹​`round(xvec/sum(xvec, na.rm = TRUE) * 100, 1)` 3.5.2 Example 2 Create a function LW() with three arguments: a vector lengths, and values a and b It returns the weights of fishes using the length-weight equation: \\(W = aL^b\\) Use the function to calculate the weight (in g) of fish of length 100, 200, 300 cm for: Mola mola, a = 0.0454, b = 3.05 Regalecus glesne, a = 0.0039, b = 2.90 #&#39; Title #&#39; #&#39; @param lengths in centimeters #&#39; @param a constant #&#39; @param b exponent #&#39; #&#39; @return weight in grams #&#39; @export #&#39; #&#39; @examples #&#39; LW &lt;- function(lengths, a, b){ return(a * lengths ^ b) } LW(lengths = c(100, 200, 300), a = 0.0454, b = 3.05) ## [1] 57155.21 473366.30 1630330.60 3.6 Best coding practices 3.6.1 Code from inside out, running the smallest bits of code possible 3.6.2 Use RStudio to your advantage Tab-complete Reformat code Reindent lines Rainbow parentheses 3.7 Debugging Missing parentheses/brackets mean(x ## Error in parse(text = input): &lt;text&gt;:2:0: unexpected end of input ## 1: mean(x ## ^ Missing quotes print(&quot;Hello) ## Error in parse(text = input): &lt;text&gt;:1:7: unexpected INCOMPLETE_STRING ## 1: print(&quot;Hello) ## ^ Misplaced, missing commas x &lt;- c(&quot;1&quot; &quot;3&quot;, &quot;7&quot;, &quot;Emma&quot;) is.numeric(x) ## Error in parse(text = input): &lt;text&gt;:1:12: unexpected string constant ## 1: x &lt;- c(&quot;1&quot; &quot;3&quot; ## ^ Misspelled object names 3.7.1 Also use RStudio Debug menu is helpful for writing more advanced functions "],["data-wrangling.html", "4 Data Wrangling 4.1 Connection to previous work on Data Organization 4.2 Source 4.3 3.1 Introduction - Example dataset nycflights13 4.4 3.1.2 4.5 3.1.3 dplyr basics 4.6 3.2 Row-wise functions 4.7 3.3 Columns 4.8 3.4 The pipe 4.9 3.5 Groups", " 4 Data Wrangling This week our goals are to be able to: Use the dplyr package to perform basic data transformation and analysis Filter and arrange rows of datasets Create new columns with mutate Select columns Use pipes to make our code more readable Summarize data by groups using summarize 4.1 Connection to previous work on Data Organization This week, we will finally see why organized data is worth the effort. We’ll follow an exercise using a data source with over 300,000 rows! The work this week will show us (1) why R is awesome and fast for analysis, and (2) reinforce the purpose of organized data (following the 12 best practices we learned in Week 1). Because we are dealing with large datasets now, make sure that your Problem Set does not include pages and pages of data by just showing the top of the final result using head(dataset) 4.2 Source This exercise follows along with the reading for this week R for Data Science Chapter 3 https://r4ds.hadley.nz/data-transform (this was chapter 5 in the old version https://r4ds.had.co.nz/transform.html, hopefully I’ve updated everything but incase I haven’t there’s the link). The template below is for you to be able to follow along in the reading and complete the exercises. 4.3 3.1 Introduction - Example dataset nycflights13 I’ve gone ahead and installed the 2 packages, but you need to load them into the environment using: library(nycflights13) library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors Why is it important to do this? When you are creating code, explicitly turning on packages that are required is considered good practice. This goes along with the importance of being intentional and making your code reproducible by anyone, anywhere. Tell the computer what to do…explicitly! Tell everyone explicitly what you have done to get to your results. This also keeps your R sessions memory low and prevents duplicate functions from being loaded from different packages. Notice above when we load tidyverse we get the message that ✖ dplyr::filter() masks stats::filter() and ✖ dplyr::lag() masks stats::lag(), that is because the stats package also has filter and lag functions as well as the dplyr package which is part of the tidyverse package. The tidyverse is actually a package of packages including ggplot2, purrr, tibble, dplyr, tidyr, stringr, readr, and forcats (and maybe more since writing this). We will learn more about all of these in coming weeks. In our case, because we more recently loaded tidyverse if we call filter(some_argument...) this will run the tidyverse/dplyr version of the function. As it says in the reading, if you want to use the base, or stats, version of these functions after loading dplyr, you’ll need to specify the package that the function comes from using two colons :: as in stats::filter() and stats::lag(). 4.4 3.1.2 Run flights in the code chunk below. The output should match the reading. Note that you can find a nice README/data dictionary/documentation of this dataset by viewing its help documentation ?flights. flights ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; As described, flights is a data frame called a tibble. What does int mean on the third line of the table? Or dbl? These are types of variables. Be sure to familiarize yourself with the various types as you move forward, so focus on this section in the reading. You can also check out Vectors and data types in Data Carpentry. 4.5 3.1.3 dplyr basics https://r4ds.hadley.nz/data-transform#dplyr-basics Most of the tidyverse aims to make programming make “grammatical” sense in that it is easy to read, understand, and talk about using typical language. One of the really tricky parts of many programming languages including R that you have already experienced is how nested accessors (like [] and $) combined with functions and logical statements are used to do operations on parts of datasets (like finding the mean of certain columns from certain rows). This can make reading a line of code really difficult. You have to read the code from the inside out. For example, from last week, we can run from inside to out. mean(is.na(c(1, NA, 1))) ## [1] 0.3333333 # Inner parentheses c(1, NA, 1) ## [1] 1 NA 1 # next set is.na(c(1, NA, 1)) ## [1] FALSE TRUE FALSE #full line mean(is.na(c(1, NA, 1))) ## [1] 0.3333333 From the dplyr homepage: “dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges” where verbs are functions that operate on nouns, which are your dataset and elements within it. For all dplyr “verbs”: The first argument is always a data frame. The subsequent arguments typically describe which columns to operate on, using the variable names (without quotes). The output is always a new data frame. Direct from R4DS: “Because each verb does one thing well, solving complex problems will usually require combining multiple verbs, and we’ll do so with the pipe, |&gt;. We’ll discuss the pipe more in Section 3.4, but in brief, the pipe takes the thing on its left and passes it along to the function on its right so that x |&gt; f(y) is equivalent to f(x, y), and x |&gt; f(y) |&gt; g(z) is equivalent to g(f(x, y), z). The easiest way to pronounce the pipe is “then”. That makes it possible to get a sense of the following code even though you haven’t yet learned the details: flights |&gt; filter(dest == &quot;IAH&quot;) |&gt; group_by(year, month, day) |&gt; summarize( arr_delay = mean(arr_delay, na.rm = TRUE) ) ## `summarise()` has grouped output by &#39;year&#39;, &#39;month&#39;. You can override using the ## `.groups` argument. ## # A tibble: 365 × 4 ## # Groups: year, month [12] ## year month day arr_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 17.8 ## 2 2013 1 2 7 ## 3 2013 1 3 18.3 ## 4 2013 1 4 -3.2 ## 5 2013 1 5 20.2 ## 6 2013 1 6 9.28 ## 7 2013 1 7 -7.74 ## 8 2013 1 8 7.79 ## 9 2013 1 9 18.1 ## 10 2013 1 10 6.68 ## # ℹ 355 more rows dplyr’s verbs are organized into four groups based on what they operate on: rows, columns, groups, or tables. In the following sections you’ll learn the most important verbs for rows, columns, and groups, then we’ll come back to the join verbs that work on tables in Chapter 19. Let’s dive in!” 4.6 3.2 Row-wise functions 4.6.1 3.2.1 filter() The example filters the data based on month and day. jan1 &lt;- filter(flights, month == 1, day == 1) The double-equals ==sign implies “is equal to”; in the filter function above, all flights on the first day of January are saved as a new variable jan1. What is happening in the command below? filter(flights, month == 1) ## # A tibble: 27,004 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 26,994 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The reading also points out the use of the near function. Why is this important? Illustrate the example below in the code chunk below to reinforce the concept. Paste sqrt(1.9999999999999999999999)^2 in the code chunk and run it. If you keep removing the trailing 9s, when does the result not equal 2? What happens when you run sqrt(2)^2==2? Show me that you can have the computer make these equivalent using near(), and explain in one word—yes one word—the result of sqrt(2)^2==2 versus using the near function. (Hint: the word starts with P). 4.6.1.1 Logical Operators We learned about ==, “is equal to,” above. Other logical or Boolean operators that can be used as filters are &gt;, ==, &lt;, &lt;=, != (not equal). You can also combine these with other Logical or Boolean operators: &amp; (and), | (or), and ! (not). Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. x x &lt;- c(TRUE, TRUE, FALSE, FALSE) y &lt;- c(TRUE, FALSE, TRUE, FALSE) x | y ## [1] TRUE TRUE TRUE FALSE How would you select all flights in May and June? flights |&gt; filter(month == 5 | month == 6) ## # A tibble: 57,039 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 5 1 9 1655 434 308 2020 ## 2 2013 5 1 451 500 -9 641 640 ## 3 2013 5 1 537 540 -3 836 840 ## 4 2013 5 1 544 545 -1 818 827 ## 5 2013 5 1 548 600 -12 831 854 ## 6 2013 5 1 549 600 -11 804 810 ## 7 2013 5 1 553 600 -7 700 712 ## 8 2013 5 1 553 600 -7 655 701 ## 9 2013 5 1 554 600 -6 731 756 ## 10 2013 5 1 554 600 -6 707 725 ## # ℹ 57,029 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; sum(flights$month == 5) ## [1] 28796 sum(flights$month == 6) ## [1] 28243 sum(flights$month == 5 | flights$month == 6) ## [1] 57039 sum(flights$month == c(5,6)) ## [1] 28520 sum(flights$month %in% c(5,6)) ## [1] 57039 R also has another nifty logical operator %in%, which searches for a matches of one vector in another and return true for any matching values. So for example: # letters is simply the lowercase alphabet letters %in% c(&quot;a&quot;, &quot;b&quot;, &quot;z&quot;) ## [1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE TRUE So we could select all flights in May and June using this now. filter(flights, month %in% c(5, 6)) |&gt; tail(n = 100) ## # A tibble: 100 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 6 30 2334 1836 298 50 2015 ## 2 2013 6 30 2336 2029 187 231 2359 ## 3 2013 6 30 2343 2029 194 205 2303 ## 4 2013 6 30 2345 2146 119 229 30 ## 5 2013 6 30 2347 2125 142 105 2253 ## 6 2013 6 30 2348 2130 138 229 14 ## 7 2013 6 30 2354 2245 69 53 2359 ## 8 2013 6 30 2354 2245 69 117 1 ## 9 2013 6 30 2357 2112 165 223 2359 ## 10 2013 6 30 2358 2225 93 49 2330 ## # ℹ 90 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.6.1.2 Missing values We covered NAs in some of our exercises in previous weeks. Hopefully reading through this section helped reinforce in your mind how NAs are handled in R and in the dplyr::filter function. 4.6.2 3.2.3 Arranging rows We can arrange rows by a particular columns values using arrange. For example with the flights dataset we could arrange by departure time. flights |&gt; arrange(dep_time) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 13 1 2249 72 108 2357 ## 2 2013 1 31 1 2100 181 124 2225 ## 3 2013 11 13 1 2359 2 442 440 ## 4 2013 12 16 1 2359 2 447 437 ## 5 2013 12 20 1 2359 2 430 440 ## 6 2013 12 26 1 2359 2 437 440 ## 7 2013 12 30 1 2359 2 441 437 ## 8 2013 2 11 1 2100 181 111 2225 ## 9 2013 2 24 1 2245 76 121 2354 ## 10 2013 3 8 1 2355 6 431 440 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; By default arrange sorts the rows from low to high on the variable you pass. To sort high to low you put a - in front of the variable or use desc(variable). flights |&gt; arrange(-dep_time) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 10 30 2400 2359 1 327 337 ## 2 2013 11 27 2400 2359 1 515 445 ## 3 2013 12 5 2400 2359 1 427 440 ## 4 2013 12 9 2400 2359 1 432 440 ## 5 2013 12 9 2400 2250 70 59 2356 ## 6 2013 12 13 2400 2359 1 432 440 ## 7 2013 12 19 2400 2359 1 434 440 ## 8 2013 12 29 2400 1700 420 302 2025 ## 9 2013 2 7 2400 2359 1 432 436 ## 10 2013 2 7 2400 2359 1 443 444 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; This may seem a little awkward, but it allows you to easily provide multiple variable names for a complex sort. flights |&gt; arrange(desc(dep_time), sched_dep_time) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 12 29 2400 1700 420 302 2025 ## 2 2013 7 7 2400 1950 250 107 2130 ## 3 2013 9 12 2400 2000 240 203 2230 ## 4 2013 7 28 2400 2059 181 247 2322 ## 5 2013 2 11 2400 2135 145 251 35 ## 6 2013 7 17 2400 2142 138 54 2259 ## 7 2013 6 17 2400 2145 135 102 2315 ## 8 2013 7 13 2400 2155 125 225 43 ## 9 2013 7 13 2400 2245 75 101 2359 ## 10 2013 8 10 2400 2245 75 110 1 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.7 3.3 Columns 4.7.1 3.3.1 Mutate - Add new variables The function mutate is used to add new variables/columns to a data frame. Following the example at the beginning of section 5.5 in the book, add a new speed variable using mutate to your data frame. flights_sml &lt;- select(flights, year:day, ends_with(&quot;delay&quot;), distance, air_time ) #add a speed variable Next, pay attention to the Useful transformation functions and the modular arithmetic section to obtain hour and minutes from the departure data. Try for yourself below. This is pretty cool and can be useful. 4.7.2 3.3.2 Select It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. select() is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea: select(flights, year, month, day) ## # A tibble: 336,776 × 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ℹ 336,766 more rows 4.8 3.4 The pipe The pipe operator, which I’ve demonstrated just a few times above, is really fantastically justified by the The pipe section. I would definitely recommend reading this short section. The idea of piping is that it can make it easier to write, follow, and understand what the commands are doing. Think of each pipe command as “then”. The pipe command uses the following syntax : |&gt;. What it essentially does is take the result of the code on the left-hand side or previous line(s) and pass it as the first argument to the function on the right-hand side. We can recreate the example above with pipes. Written in words the code chunk below would be: ASSIGN a new object name, CHOOSE the dataset to operate on, THEN arrange the dataset by longest distance, THEN filter the December flights, THEN select the flight number, departure delay, and carrier columns. carrier_delay &lt;- # I like to use a new line here so that I can easily comment out this assignment line while building my pipe flights |&gt; arrange(-distance) |&gt; filter(month == 12) |&gt; select(flight, dep_delay, carrier) 4.9 3.5 Groups Grouped summaries are essentially what pivot tables are in Excel, if you have ever heard of those. By using the summarise() function with the group_by function we can, for example find the average flight delay by month. This becomes really awesome! This example starts with using group_by to group the data, then applies summarise. flights |&gt; group_by(month) |&gt; summarise( delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 12 × 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 Next week we’ll practice summarizing data a lot more as well as “pivoting” our data, which is We’ll get to making those sweet, sweet plots soon. "],["data-wrangling-examples.html", "Data Wrangling Examples", " Data Wrangling Examples These examples are adapted from R4DS https://r4ds.hadley.nz/data-transform. 4.9.1 Filter These are additional practice to those in the book to reinforce the reading and try by doing. Solutions for each are given below. Our suggestion is to try first and test your skill. nycflights13::flights ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Find all flights that: # 1.1 Had an arrival delay of two or more hours (10,034 flights) flights |&gt; # Had an arrival delay of two or more hours filter(arr_delay &gt; 120) ## # A tibble: 10,034 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 811 630 101 1047 830 ## 2 2013 1 1 848 1835 853 1001 1950 ## 3 2013 1 1 957 733 144 1056 853 ## 4 2013 1 1 1114 900 134 1447 1222 ## 5 2013 1 1 1505 1310 115 1638 1431 ## 6 2013 1 1 1525 1340 105 1831 1626 ## 7 2013 1 1 1549 1445 64 1912 1656 ## 8 2013 1 1 1558 1359 119 1718 1515 ## 9 2013 1 1 1732 1630 62 2028 1825 ## 10 2013 1 1 1803 1620 103 2008 1750 ## # ℹ 10,024 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # 1.2 Flew to Houston (IAH or HOU) (9,313 flights) flights |&gt; filter(dest %in% c(&quot;IAH&quot;,&quot;HOU&quot;)) ## # A tibble: 9,313 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 623 627 -4 933 932 ## 4 2013 1 1 728 732 -4 1041 1038 ## 5 2013 1 1 739 739 0 1104 1038 ## 6 2013 1 1 908 908 0 1228 1219 ## 7 2013 1 1 1028 1026 2 1350 1339 ## 8 2013 1 1 1044 1045 -1 1352 1351 ## 9 2013 1 1 1114 900 134 1447 1222 ## 10 2013 1 1 1205 1200 5 1503 1505 ## # ℹ 9,303 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # 1.3 Were operated by United, American, or Delta (139,504 flights) # 1.4 Departed in summer (July, August, and September) (86,326 flights) # 1.5 Arrived more than two hours late, but didn&#39;t leave late (3 flights) # 1.6 Were delayed by at least an hour, but made up over 30 minutes in flight (1,819 flights) # 1.7 Departed between midnight and 6am (inclusive) (9,373 flights) Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer 1.7? (hint: look up between in the help menu. You’ll see the required syntax, where x = vector, and left and right at the boundary values. You will also need to add an OR statement to include departure times at exactly 2400 since the dataframe has departures at both 0 and 2400) flights %&gt;% filter(dep_time |&gt; between(0, 600))|&gt; arrange(desc(month), -day) ## # A tibble: 9,344 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 12 31 13 2359 14 439 437 ## 2 2013 12 31 18 2359 19 449 444 ## 3 2013 12 31 26 2245 101 129 2353 ## 4 2013 12 31 459 500 -1 655 651 ## 5 2013 12 31 514 515 -1 814 812 ## 6 2013 12 31 549 551 -2 925 900 ## 7 2013 12 31 550 600 -10 725 745 ## 8 2013 12 31 552 600 -8 811 826 ## 9 2013 12 31 553 600 -7 741 754 ## 10 2013 12 31 554 550 4 1024 1027 ## # ℹ 9,334 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; How many flights have a missing dep_time? What other variables are missing? What might these rows represent? 4.9.1.1 solutions: 1.1 k &lt;- filter(flights,(arr_delay &gt; 120)) 1.2 k &lt;- filter(flights,dest == “IAH”|dest==“HOU”) 1.3 k &lt;- filter(flights,carrier==“DL”|carrier==“UA”|carrier==“AA”) 1.4 k &lt;- filter(flights,month==7 | month==8 | month==9) 1.5 k &lt;- filter(flights,arr_delay &gt;120 &amp; dep_delay == 0) 1.6 filter(flights,dep_delay &gt;60 &amp; arr_delay &lt;(dep_delay-30))) 1.7 k &lt;- filter(flights,dep_time==2400 | (dep_time&lt;0601)) 2. m &lt;- filter(flights,between(dep_time,0,0600)|dep_time==2400) 3. y &lt;- filter(flights, is.na(dep_time)) 4.9.2 Arrange Use desc() to re-order by a column in descending order: flights |&gt; arrange(desc(month), -day) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 12 31 13 2359 14 439 437 ## 2 2013 12 31 18 2359 19 449 444 ## 3 2013 12 31 26 2245 101 129 2353 ## 4 2013 12 31 459 500 -1 655 651 ## 5 2013 12 31 514 515 -1 814 812 ## 6 2013 12 31 549 551 -2 925 900 ## 7 2013 12 31 550 600 -10 725 745 ## 8 2013 12 31 552 600 -8 811 826 ## 9 2013 12 31 553 600 -7 741 754 ## 10 2013 12 31 554 550 4 1024 1027 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Sort flights to find the most delayed flights. Find the flights that left earliest. Sort flights to find the fastest (highest speed) flights. Here you are creating a metric by using the existing data in the dataframe to calculate speed. Which flights traveled the farthest? Which traveled the shortest? (flights 1632 and 51) arrange(flights, -distance) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 857 900 -3 1516 1530 ## 2 2013 1 2 909 900 9 1525 1530 ## 3 2013 1 3 914 900 14 1504 1530 ## 4 2013 1 4 900 900 0 1516 1530 ## 5 2013 1 5 858 900 -2 1519 1530 ## 6 2013 1 6 1019 900 79 1558 1530 ## 7 2013 1 7 1042 900 102 1620 1530 ## 8 2013 1 8 901 900 1 1504 1530 ## 9 2013 1 9 641 900 1301 1242 1530 ## 10 2013 1 10 859 900 -1 1449 1530 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.9.3 Mutate - adds a new column #flights1 &lt;- flights |&gt; mutate(check_arr_delay = sched_arr_time - arr_time, check_dep_delay = sched_dep_time - dep_time) #-&gt; ## # A tibble: 336,776 × 21 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 336,766 more rows ## # ℹ 13 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, check_arr_delay &lt;int&gt;, ## # check_dep_delay &lt;int&gt; # flights1 4.9.4 Select flights |&gt; mutate(check_arr_delay = arr_time - sched_arr_time, check_dep_delay = sched_dep_time - dep_time) |&gt; select(arr_time, sched_arr_time, check_arr_delay, arr_delay) |&gt; mutate(arr_check_boolean = arr_delay == check_arr_delay) |&gt; filter(arr_check_boolean == FALSE) ## # A tibble: 114,963 × 5 ## arr_time sched_arr_time check_arr_delay arr_delay arr_check_boolean ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 923 850 73 33 FALSE ## 2 913 854 59 19 FALSE ## 3 854 902 -48 -8 FALSE ## 4 858 910 -52 -12 FALSE ## 5 858 915 -57 -17 FALSE ## 6 807 735 72 32 FALSE ## 7 1039 1100 -61 -21 FALSE ## 8 909 840 69 29 FALSE ## 9 1016 947 69 29 FALSE ## 10 1028 940 88 48 FALSE ## # ℹ 114,953 more rows flights |&gt; mutate(arr_time = lubridate::hm(arr_time)) |&gt; head() ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `arr_time = lubridate::hm(arr_time)`. ## Caused by warning in `.parse_hms()`: ## ! Some strings failed to parse ## # A tibble: 6 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;Period&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 NA 819 ## 2 2013 1 1 533 529 4 NA 830 ## 3 2013 1 1 542 540 2 NA 850 ## 4 2013 1 1 544 545 -1 NA 1022 ## 5 2013 1 1 554 600 -6 NA 837 ## 6 2013 1 1 554 558 -4 NA 728 ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.9.5 Grouped summaries To make summary tables we will use the pipe combining group_by and summarize. summary_FlightDelay &lt;- # I like to use a new line here so that I can easily comment out the # assignment while building my pipe flights |&gt; group_by(month, carrier) |&gt; # group flights by month summarise(delay = mean(dep_delay, na.rm = TRUE)) # make a new column of average dep delay ## `summarise()` has grouped output by &#39;month&#39;. You can override using the ## `.groups` argument. summary_FlightDelay |&gt; ggplot(mapping = aes(x = month, y = delay, color = carrier)) + geom_point() We could also figure out which carrier had the longest and shortest delay in December, if we were trying to plan a timely winter break flight. carrier_delay &lt;- # I like to use a new line here so that I can easily comment out this assignment line while building my pipe flights |&gt; arrange(-distance) |&gt; filter(month == 12) |&gt; select(flight, dep_delay, carrier) carrier_delay ## # A tibble: 28,135 × 3 ## flight dep_delay carrier ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 51 -2 HA ## 2 51 -10 HA ## 3 51 3 HA ## 4 51 -10 HA ## 5 51 -6 HA ## 6 51 -3 HA ## 7 51 5 HA ## 8 51 -2 HA ## 9 51 0 HA ## 10 51 -1 HA ## # ℹ 28,125 more rows carrier_delay |&gt; group_by(carrier) |&gt; # want to find the average for each carrier summarise(average_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; # calculate average delay arrange(average_dep_delay) ## # A tibble: 15 × 2 ## carrier average_dep_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 HA -3.14 ## 2 US 4.94 ## 3 VX 6.10 ## 4 DL 10.8 ## 5 AA 11.7 ## 6 MQ 12.7 ## 7 YV 13.1 ## 8 F9 13.1 ## 9 B6 17.0 ## 10 UA 17.7 ## 11 AS 18.0 ## 12 9E 19.8 ## 13 WN 24.9 ## 14 FL 26.1 ## 15 EV 27.9 "],["summarizing-data.html", "5 Summarizing Data 5.1 Learning Objectives 5.2 Introduction 5.3 group_by function: Summarizing info based on type/characteristic 5.4 Grouping with multiple variables 5.5 Tidying and untidying data with pivot_ functions 5.6 Joins and Binds within dplyr", " 5 Summarizing Data 5.1 Learning Objectives This week we will: Practice summarizing large datasets by groups Tidy and untidy data with pivot_longer and pivot_wider Join datasets using different _join functions 5.2 Introduction This week we’re focused on building our ability to analyze data. We’ll incorporate new functions from the dplyr package, and explore table joins. The information below augments the readings. Following this, you’ll be in good shape to start this week’s exercises. Let’s get started! 5.2.1 Readings (complete by class on Monday) Required: R for Data Science, section 3.5: Groups R for Data Science, section 5.3 and 5.4: lengthening and Widening data R for Data Science, chapter 19: Joins As you read through, my suggestion is to have the markdown version of this document open in Posit where you can take notes on important functions/concepts and also code along with the examples. This way you’ll get a headstart on your cheat sheet for this week. The below summarizes and provides a different example, but feel free to copy in examples from the reading and tinker with them a bit. These optional readings will also be useful follow up: dplyr lesson in Data Carpentry dplyr vignette (long form example document): if you’re struggling, this provides additional examples that you can try on your own (and as a bonus they use a Star Wars data set). Package vignettes like this are an amazing resource for learning new packages, and evaluating their capabilities, once you get the basics. tidyr pivot vignette again provides some nice examples of the pivot_longer and pivot_wider functions with different data set (although none quite as fun as the Star Wars data, alas). 5.3 group_by function: Summarizing info based on type/characteristic Based on reading from R for Data Science, section 3.5: Groups group_by(data, column_to_group_by) or with pipes data |&gt; group_by(column_to_group_by) combines rows into groups based on a column characteristic provides ability to summarize information based on one or more variables in your dataset when combine with summarize Let’s start by activating the data sets package within R-studio. We’re going to explore the sleep data set. library(datasets) library(dplyr) sleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 ?sleep The dataset has 3 variables: extra, is a numeric variable representing the increase in hours of sleep group is a categorical factor variable representing the drug given ID is another factor for the patient ID Suppose you want to know how the 2 drug treatment groups varied with respect to the extra hours of sleep. First we group the data by group, the drug given. sleep |&gt; group_by(group) ## # A tibble: 20 × 3 ## # Groups: group [2] ## extra group ID ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0 1 9 ## 10 2 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 Note that this looks the same as sleep only with Groups: group [2] as an attribute. Now you can use summarize function to calculate statistics for each group within the dataset. Here, we only have 2 groups so it’s a simple example. # summarize the number of rows for each group sleep |&gt; group_by(group) |&gt; summarize(N = n()) ## # A tibble: 2 × 2 ## group N ## &lt;fct&gt; &lt;int&gt; ## 1 1 10 ## 2 2 10 We can save this summary table as a new object sleep_N &lt;- sleep |&gt; group_by(group) |&gt; summarize(N = n()) sleep_N ## # A tibble: 2 × 2 ## group N ## &lt;fct&gt; &lt;int&gt; ## 1 1 10 ## 2 2 10 The result here is trivial, but imagine if you had a larger dataset, like the Dipodomys survey from last week. One technique in coding is to start with something small, where you can easily hand-calculate the answer. Let’s look at other functions you can use within summarize: basic stats: https://www.dummies.com/education/math/statistics/base-r-statistical-functions/ sum() mean() var() sd() range() cor() min() summary() max() quantile() median() Let’s look at the median number of extra sleep hours: sleep |&gt; group_by(group) |&gt; summarize(median_extra_sleep = median(extra)) ## # A tibble: 2 × 2 ## group median_extra_sleep ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 0.35 ## 2 2 1.75 Now let’s look at what would happen if there was missing data by changing a value: # summarize the number of rows for each group sleep[1,1] = NA # Oh I used `=` here to assign this, # the assignment arrows, `&lt;-` or `-&gt;`, are generally best-coding # practice as they differentiate variables and objects, from function # arguments Now, if we implement the summarize command for the median sleep we’ll obtain a NA value: sleep |&gt; group_by(group) |&gt; summarize(median_sleep = median(extra)) ## # A tibble: 2 × 2 ## group median_sleep ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 NA ## 2 2 1.75 So what can we do? Start by adding a na.rm = TRUE command. sleep |&gt; group_by(group) |&gt; summarize(median_sleep = median(extra, na.rm = TRUE)) ## # A tibble: 2 × 2 ## group median_sleep ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 0 ## 2 2 1.75 If you still had NaN or some other troublesome value (depending on your data set), you could filter the data: filter(data, !is.na(variable)) 5.4 Grouping with multiple variables sleep is a really simple data set, but what if we have a slightly bigger data set with multiple variables we wanted to group by and summarize? Let’s look at the “Carbon Dioxide Uptake in Grass Plants” CO2 data set. This data set contains ambient CO2 (conc) and CO2 uptake rate (uptake) measurements for Echinochloa crus-galli grass species from Quebec or Mississippi (type) that have undergone a chilling treatment or not (treatment). head(CO2) ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 ## 4 Qn1 Quebec nonchilled 350 37.2 ## 5 Qn1 Quebec nonchilled 500 35.3 ## 6 Qn1 Quebec nonchilled 675 39.2 The obvious questions to ask of this data are: What effect does the origin/type of grass species have on CO2 uptake? What effect does the chilling treatment have on CO2 uptake? What effect does ambient CO2 conc have on CO2 uptake? Let’s try to answer these questions with one long pipe! The 3rd question is ideally a regression which we’ll save for after spring break, but we could separate the conc vector into a few groups and summarize the uptake for each group. So let’s make a plan. We want to group by type and treatment as well as conc, but first we need to make a simplified, group_conc variable. Remember we use mutate to make new variables. case_when or cut are a convenient functions to use in mutate to make categorical variables from numerical variables. The easiest way to figure out how case_when works is to check out the Examples section in ?case_when. So we will, Make a new group_conc to simplify data a bit (let’s say 95 ~ “low” and 1000 ~ “high”). Group by Type, Treatment, and group_conc. Summarize the mean and standard deviation CO2 uptake of each group. This table will allow us to compare the average uptake values of each of these groups. All we need to do to summarize all of these combinations of variables is pass the three variable names to group_by! CO2 |&gt; mutate(group_conc = case_when(conc == 1000 ~ &quot;high&quot;, conc == 95 ~ &quot;low&quot;, .default = NA)) |&gt; group_by(Type, Treatment, group_conc) |&gt; summarise(mean_uptake = mean(uptake), sd_uptake = sd(uptake))# -&gt; ## `summarise()` has grouped output by &#39;Type&#39;, &#39;Treatment&#39;. You can override using ## the `.groups` argument. ## # A tibble: 12 × 5 ## # Groups: Type, Treatment [4] ## Type Treatment group_conc mean_uptake sd_uptake ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Quebec nonchilled high 43.2 3.06 ## 2 Quebec nonchilled low 15.3 1.45 ## 3 Quebec nonchilled &lt;NA&gt; 37.8 4.91 ## 4 Quebec chilled high 40.8 1.91 ## 5 Quebec chilled low 12.9 3.12 ## 6 Quebec chilled &lt;NA&gt; 33.7 5.72 ## 7 Mississippi nonchilled high 31.6 3.85 ## 8 Mississippi nonchilled low 11.3 0.7 ## 9 Mississippi nonchilled &lt;NA&gt; 27.8 4.45 ## 10 Mississippi chilled high 18.7 3.88 ## 11 Mississippi chilled low 9.6 1.65 ## 12 Mississippi chilled &lt;NA&gt; 16.5 3.23 #CO2_summary To make it easier for us to make some conclusions from this we can clean it up a little bit by removing the NA rows and arrange-ing by mean_uptake. CO2 |&gt; mutate(group_conc = case_when(conc == 1000 ~ &quot;high&quot;, conc == 95 ~ &quot;low&quot;, .default = NA)) |&gt; group_by(Type, Treatment, group_conc) |&gt; summarise(mean_uptake = mean(uptake), sd_uptake = sd(uptake)) |&gt; filter(!is.na(group_conc)) |&gt; arrange(mean_uptake) ## `summarise()` has grouped output by &#39;Type&#39;, &#39;Treatment&#39;. You can override using ## the `.groups` argument. ## # A tibble: 8 × 5 ## # Groups: Type, Treatment [4] ## Type Treatment group_conc mean_uptake sd_uptake ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mississippi chilled low 9.6 1.65 ## 2 Mississippi nonchilled low 11.3 0.7 ## 3 Quebec chilled low 12.9 3.12 ## 4 Quebec nonchilled low 15.3 1.45 ## 5 Mississippi chilled high 18.7 3.88 ## 6 Mississippi nonchilled high 31.6 3.85 ## 7 Quebec chilled high 40.8 1.91 ## 8 Quebec nonchilled high 43.2 3.06 Now looking at this final summary table we can see Quebec varieties had higher uptake rates than Mississippi, the chilling treatment reduced uptake rates, and high ambient concentrations increase uptake rates. 5.5 Tidying and untidying data with pivot_ functions Based on reading from R for Data Science, section 5.3 and 5.4: Lengthening and Widening data Data can come your way in untidy forms which you will need to tidy up for analysis. Also, occasionally you may want to intentionally untidy data to do some analyses, or present the data in a shorter form table. For these tasks, so long as the data is organized systematically, you can make use of the tidyr package functions pivot_longer or pivot_wider. For tidying untidy data, we can use pivot_longer. For example if a variable is encoded in column names, like “cell_growth_5ug_ml_cefo” in which a column has cell growth measurements (or “observations”) at a specific concentration of an antibiotic, 5 µg/mL cefotaxime in this case. The reading uses a fun billboard data set. billboard ## # A tibble: 317 × 79 ## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA ## 2 2Ge+her The … 2000-09-02 91 87 92 NA NA NA NA NA ## 3 3 Doors D… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 ## 4 3 Doors D… Loser 2000-10-21 76 76 72 69 67 65 55 59 ## 5 504 Boyz Wobb… 2000-04-15 57 34 25 17 17 31 36 49 ## 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 ## 7 A*Teens Danc… 2000-07-08 97 97 96 95 100 NA NA NA ## 8 Aaliyah I Do… 2000-01-29 84 62 51 41 38 35 35 38 ## 9 Aaliyah Try … 2000-03-18 59 53 38 28 21 18 16 14 ## 10 Adams, Yo… Open… 2000-08-26 76 76 74 69 68 67 61 58 ## # ℹ 307 more rows ## # ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, ## # wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, ## # wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, ## # wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, ## # wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, ## # wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, … Looking at this dataset we can see each row is a song and there are columns for each week of the year which contains the position of the song on the billboard chart. Definitely not tidy, right?! The “week” columns are actually a variable themselves. An easy way to identify this sort of untidiness is that column names provide no indication of what values they contain. To tidy this we want to make the wide table of weeks into a long table that has “week” as a variable, and “position” as a variable. This would be an enormous task to do, but fortunately pivot_longer makes it pretty easy. The arguments to pivot_longer are cols which is list of column names you want to condense into a single column. To the cols argument we can pass helper “tidy-select” functions which are starts_with(\"a\"): names that start with \"a\". ends_with(\"z\"): names that end with \"z\". contains(\"b\"): names that contain \"b\". matches(\"x.y\"): names that match regular expression x.y. num_range(x, 1:4): names following the pattern, x1, x2, …, x4. `all_of(vars)`` will match just the variables that exist. everything(): all variables. last_col(): furthest column on the right. where(is.numeric): all variables where is.numeric() returns TRUE. Let’s use starts_with() to grab all the columns that start with “wk”. The other arguments that we need are names_to and values_to billboard |&gt; pivot_longer( cols = starts_with(&quot;wk&quot;), names_to = &quot;week&quot;, values_to = &quot;rank&quot; ) ## # A tibble: 24,092 × 5 ## artist track date.entered week rank ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk1 87 ## 2 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk2 82 ## 3 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk3 72 ## 4 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk4 77 ## 5 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk5 87 ## 6 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk6 94 ## 7 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk7 99 ## 8 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk8 NA ## 9 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk9 NA ## 10 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk10 NA ## # ℹ 24,082 more rows Note how long this longer table is now, 24 thousand rows! This tidied dataset can now be analyzed and plottes with ease. However we can’t fit this dataset (or at least a few songs) onto a page easily in it’s tidy form. For that we need to widen. It is rarely the case that we need to use pivot_wider except for trying to display data in a more convenient table. We could pivot_wider our billboard dataset back to the original for example. The arguments are just the opposite, names_from = \"week\" and values_from = \"rank\". billboard |&gt; pivot_longer( cols = starts_with(&quot;wk&quot;), names_to = &quot;week&quot;, values_to = &quot;rank&quot; ) |&gt; pivot_wider( names_from = &quot;week&quot;, values_from = &quot;rank&quot; ) ## # A tibble: 317 × 79 ## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA ## 2 2Ge+her The … 2000-09-02 91 87 92 NA NA NA NA NA ## 3 3 Doors D… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 ## 4 3 Doors D… Loser 2000-10-21 76 76 72 69 67 65 55 59 ## 5 504 Boyz Wobb… 2000-04-15 57 34 25 17 17 31 36 49 ## 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 ## 7 A*Teens Danc… 2000-07-08 97 97 96 95 100 NA NA NA ## 8 Aaliyah I Do… 2000-01-29 84 62 51 41 38 35 35 38 ## 9 Aaliyah Try … 2000-03-18 59 53 38 28 21 18 16 14 ## 10 Adams, Yo… Open… 2000-08-26 76 76 74 69 68 67 61 58 ## # ℹ 307 more rows ## # ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, ## # wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, ## # wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, ## # wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, ## # wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, ## # wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, … 5.6 Joins and Binds within dplyr Based on reading from R for Data Science, chapter 19: Joins Within Biological Systems Engineering and the larger field, there are many times when we use multiple tables to reduce redundant data, particularly as data is being collected. Imagine you have a very large table that repeats double-precision data over and over again, resulting in a dataset that occupies more computer memory. While modern computers are incredible in their storage capacity, I can attest that processing speed and memory allocation is and will still be a consideration in your work. Here are some examples of such tasks: Perhaps, your experiment spans many days and you collected each day’s data in a different sheet that you now want to compile. Perhaps, you didn’t want to write down complete experimental conditions for each sample and instead used a code or number to indicate different conditions, now you want to add the full info to the table. Or maybe you measured something at a place over time and want to add weather station data as new columns. The above are two different tasks: 1) just stitching datasets together is a bind_, whereas examples in 2) are adding new variables based on the values in some key/code variable, like the experiment code or date, which is a _join. In dplyr we can use bind_rows to add new observations to a dataset, like binding multiple days data tables together. bind_rows will return an error if the column names of the tables we are joining are not the same. You can also use bind_cols to add new columns to a dataset without any key or identifier variable, but this is risky, as there is no guarantee that the rows (observations) will be in the same order. It’s better to use a _join function which checks that the rows match based on shared variables. Let’s illustrate joins using data from the Problem Set. We’ll first read each of these into the workspace: plots &lt;- read_csv(&quot;data/plots.csv&quot;) ## Rows: 24 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): plot_type ## dbl (1): plot_id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. species &lt;- read_csv(&quot;data/species.csv&quot;) ## Rows: 54 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): species_id, genus, species, taxa ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. surveys &lt;- read_csv(&quot;data/surveys.csv&quot;) ## Rows: 35549 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): species_id, sex ## dbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Note that when you do this, there are 24 plots, 54 types of species, but 35,549 lines of data in the surveys dataset! These tables are all related the question is: “What is the column that is shared by, or links, the plots and surveys dataset?” plot_id! What is the column that links the surveys dataset with the species? species_id! If we want to combine the dataset, let’s look at how we would do this. We’ll employ the command: inner_join From help: The mutating joins add columns from dataset y to dataset x, matching rows based on the keys: inner_join(x,y): includes all rows in x and y. left_join(x,y): includes all rows in x. right_join(x,y): includes all rows in y. full_join(x,y): includes all rows in x or y. If a row in x matches multiple rows in y, all the rows in y will be returned once for each matching row in x. The dplyr cheat sheet (also up in the Help&gt;Cheat Sheets menu) demonstrates this visually. If you want to include the detailed names contained within the species table, use an inner join and join based on the common variable species_id, using the by argument to specify the column name(s), as a character vector, that you want to link the two datasets. In this case, you end up with 3 additional variables that are within the species table added to the surveys data. combo &lt;- inner_join(surveys, species, by = &quot;species_id&quot;) head(combo) ## # A tibble: 6 × 12 ## record_id month day year plot_id species_id sex hindfoot_length weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## # ℹ 3 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt; If you wanted to join all three tables together we could add this to the first combo: combo2 &lt;- inner_join(combo, plots, by = &quot;plot_id&quot;) Or use pipes. This is actually more efficient because you aren’t creating the intermediate combo object that takes up memory, plus, this way you don’t accidentally mix up combo and combo2. As always more descriptive names would be better! combo2 &lt;- surveys %&gt;% # oops this is what the pipe symbol used to be inner_join(species, by = &quot;species_id&quot;) |&gt; inner_join(plots, by = &quot;plot_id&quot;) head(combo2) ## # A tibble: 6 × 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## # ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt; While it is best practice to specify the columns you are using to uniquely match the datasets (called unique identifiers sometimes) the _join functions are pretty smart and will find variables that match between the datasets automatically as well! surveys |&gt; inner_join(species) |&gt; inner_join(plots) |&gt; head() ## Joining with `by = join_by(species_id)` ## Joining with `by = join_by(plot_id)` ## # A tibble: 6 × 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## # ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt; "],["summarizing-data-examples.html", "Summarizing Data Examples 5.7 Bioreactor Data Analysis 5.8 Root Growth Inhibition Example", " Summarizing Data Examples library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(readxl) 5.7 Bioreactor Data Analysis 5.7.1 Background In a bioprocess engineering lab, several bioreactor runs were performed to study the effect of different feed types on process performance. Two datasets were collected: Bioreactor Readings This dataset (bioreactor_readings.csv) contains time-series measurements recorded during each run. It is provided in a wide format with columns for the run number, time (in hours), and various sensor readings: Run: Identifier for the bioreactor run. Time: Time stamp (hours). Temp: Temperature (°C). pH: pH level. DO: Dissolved Oxygen (mg/L). Substrate: Substrate concentration (g/L). Experimental Metadata This dataset (experiment_info.csv) contains metadata about each run: Run: Run identifier (matching the bioreactor dataset). Feed: Type of feed (e.g., “Glucose”, “Glycerol”). Inoculum: Inoculum concentration (OD units). Operator: Name of the operator in charge. 5.7.2 Tasks 5.7.2.1 1. Import and Inspect the Data - Load both CSV files into R as data frames. - Use the `head()` function (or similar) to inspect the first few rows of each dataset. # Load libraries library(tidyverse) # Import data bioreactor &lt;- read_csv(&quot;data/bioreactor_readings.csv&quot;) ## Rows: 10 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): Run, Time, Temp, pH, DO, Substrate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. experiment &lt;- read_csv(&quot;data/experiment_info.csv&quot;) ## Rows: 2 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Feed, Operator ## dbl (2): Run, Inoculum ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(bioreactor) ## # A tibble: 6 × 6 ## Run Time Temp pH DO Substrate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 30 7 6.5 1.5 ## 2 1 1 30.5 7.1 6.4 1.3 ## 3 1 2 31 7.2 6.3 1.1 ## 4 1 3 31.2 7.1 6.2 0.9 ## 5 1 4 31 7 6 0.8 ## 6 2 0 29.5 6.8 6.8 1 head(experiment) ## # A tibble: 2 × 4 ## Run Feed Inoculum Operator ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 Glucose 0.15 Alice ## 2 2 Glycerol 0.12 Bob 5.7.2.2 2. Data Tidying (Pivoting) - **Pivot Longer:**\\ Convert the bioreactor readings from wide format into a long (tidy) format so that all measurements (Temp, pH, DO, Substrate) are in a single column named `Parameter` with corresponding values in a column named `Value`.\\ *Hint: Use `tidyr::pivot_longer()`.* - **Pivot Wider:**\\ After some analysis, you decide to reshape the long dataset back into a wide format, but now grouping by `Run` and `Time` such that each parameter becomes its own column again.\\ *Hint: Use `tidyr::pivot_wider()`.* # 2. Pivot longer: Tidy the bioreactor data bioreactor_long &lt;- bioreactor %&gt;% pivot_longer(cols = Temp:Substrate, names_to = &quot;Parameter&quot;, values_to = &quot;Value&quot;) # This would allow us to plot both parameters on the same plot ggplot(data = bioreactor_long, mapping = aes(x = Time, y = Value, color = Parameter, linetype = as.factor(Run))) + geom_line() 5.7.2.3 2b. Pivot wider: Reshape back to wide format if needed bioreactor_wide &lt;- bioreactor_long %&gt;% pivot_wider(names_from = Parameter, values_from = Value) 5.7.2.4 3. Joining Datasets - Merge the reshaped bioreactor dataset with the experimental metadata using the common `Run` column.\\ *Hint: Use one of the join functions (e.g., `dplyr::left_join()`).* # 3. Join datasets by &#39;Run&#39; merged_data &lt;- bioreactor_wide %&gt;% left_join(experiment, by = &quot;Run&quot;) 5.7.2.5 4. Grouping and Summarizing - Group the merged dataset by the `Feed` type. - Calculate summary statistics (e.g., mean and standard deviation, or min and max) for key measurements such as `Temp`, `pH`, and `Substrate` across all time points and runs for each feed type.\\ *Hint: Use `dplyr::group_by()` and `dplyr::summarize()`.* # 4. Group and summarize by &#39;Feed&#39; summary_stats &lt;- merged_data %&gt;% group_by(Feed) %&gt;% summarize(mean_Temp = mean(Temp, na.rm = TRUE), sd_Temp = sd(Temp, na.rm = TRUE), mean_pH = mean(pH, na.rm = TRUE), sd_pH = sd(pH, na.rm = TRUE), max_Subs = max(Substrate, na.rm = TRUE), min_Subs = min(Substrate, na.rm = TRUE)) # View summary statistics print(summary_stats) ## # A tibble: 2 × 7 ## Feed mean_Temp sd_Temp mean_pH sd_pH max_Subs min_Subs ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Glucose 30.7 0.488 7.08 0.0837 1.5 0.8 ## 2 Glycerol 30.2 0.488 6.88 0.0837 1.1 1 Interpreting the Results Discuss how different feed types glucose vs glycerol may be affecting the reactor conditions based on your summary statistics. What potential insights could an engineer draw from these analyses? 5.8 Root Growth Inhibition Example 5.8.1 Introduction In my research group we study plant genes and how they function. One interesting thing about plants is that their genomes are quite large compared to other organisms and one of the reasons for that is their genomes have been duplicated several times throughout the history of their evolution gnome duplication These genome duplications have generated families of genes which have similar but slightly different functions. Often we think about each of these genes in a family as playing a distinct role, and by adding together their function we could approximate their total function. However that’s not always the case because these genes might interact with one another—directly through binding or indirectly through competition—to perform their total function. These interactions might cause some deviation from our additive model of function; in genetics this is called epistasis. This dataset is from Prigge et al. 2020, who collected measurements of several phenotypes for combinations of mutants in the TIR1/AFB auxin receptor genes which my group studies and engineers. 5.8.2 Read in data root_growth_inh_0 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh EtOH&quot;, skip = 1) root_growth_inh_20 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh 20 nM&quot;, skip = 1) root_growth_inh_100 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh 100 nM&quot;, skip = 1) root_growth_inh_500 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh 500 nM&quot;, skip = 1) 5.8.3 Write a function for tidying this data root_growth_plonger &lt;- function(data){ data |&gt; pivot_longer(cols = -line, names_to = &quot;genotype&quot;, values_to = &quot;root_growth_mm&quot;) %&gt;% mutate(batch = str_extract(genotype, &quot;\\\\s(.*)$&quot;)) %&gt;% mutate(batch = str_remove_all(batch, &quot;\\\\s&quot;)) %&gt;% mutate(genotype = str_remove(genotype, &quot;\\\\s.*&quot;)) %&gt;% mutate(batch = if_else(is.na(batch), &quot;a&quot;, batch)) %&gt;% na.omit() } 5.8.4 Tidy the individual datasets root_growth_inh_0 &lt;- root_growth_plonger(root_growth_inh_0) root_growth_inh_0$treatment &lt;- 0 root_growth_inh_20 &lt;- root_growth_plonger(root_growth_inh_20) root_growth_inh_20$treatment &lt;- 20 root_growth_inh_100 &lt;- root_growth_plonger(root_growth_inh_100) root_growth_inh_100$treatment &lt;- 100 root_growth_inh_500 &lt;- root_growth_plonger(root_growth_inh_500) root_growth_inh_500$treatment &lt;- 500 root_growth_inh &lt;- bind_rows(root_growth_inh_0, root_growth_inh_20, root_growth_inh_100, root_growth_inh_500) root_growth_inh %&gt;% group_by(batch) %&gt;% mutate(percent_inh = root_growth_mm / mean(root_growth_mm[genotype == &quot;Col-0&quot; &amp; treatment == 0]) * 100) -&gt; root_growth_inh root_growth_inh_aov &lt;- aov(percent_inh ~ genotype*treatment + batch, data = root_growth_inh) summary(root_growth_inh_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 4 4807 1202 1.865 0.11536 ## treatment 1 423486 423486 657.249 &lt; 2e-16 *** ## batch 2 7356 3678 5.708 0.00355 ** ## genotype:treatment 4 31237 7809 12.120 2.16e-09 *** ## Residuals 479 308635 644 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 root_growth_inh$fac_treat &lt;- as.factor(root_growth_inh$treatment) root_growth_inh_aov &lt;- aov(percent_inh ~ genotype*fac_treat + batch, data = root_growth_inh) summary(root_growth_inh_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 4 4807 1202 10.951 1.69e-08 *** ## fac_treat 3 708852 236284 2153.098 &lt; 2e-16 *** ## batch 2 1084 542 4.939 0.00754 ** ## genotype:fac_treat 9 8981 998 9.093 9.95e-13 *** ## Residuals 472 51798 110 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 root_growth_inh_aov.HSD &lt;- broom::tidy(TukeyHSD(root_growth_inh_aov)) root_growth_inh_aov.HSD[which(root_growth_inh_aov.HSD$adj.p.value &lt; 0.05),] ## # A tibble: 121 × 7 ## term contrast null.value estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 genotype tir1-1-… 0 6.55 3.47 9.62 1.04e- 7 ## 2 genotype tir1-10… 0 7.57 3.42 11.7 7.93e- 6 ## 3 genotype tir1afb… 0 6.41 1.11 11.7 8.76e- 3 ## 4 fac_treat 20-0 0 -36.5 -40.2 -32.9 2.81e-11 ## 5 fac_treat 100-0 0 -75.8 -78.9 -72.7 2.81e-11 ## 6 fac_treat 500-0 0 -93.9 -97.4 -90.5 2.81e-11 ## 7 fac_treat 100-20 0 -39.3 -42.9 -35.6 2.81e-11 ## 8 fac_treat 500-20 0 -57.4 -61.4 -53.4 2.81e-11 ## 9 fac_treat 500-100 0 -18.2 -21.6 -14.7 2.81e-11 ## 10 genotype:fac_tre… tir1-10… 0 15.4 2.19 28.6 6.04e- 3 ## # ℹ 111 more rows Looks like there are no strong batch effects here. root_growth_inh %&gt;% filter(genotype != &quot;tir1-10&quot;) %&gt;% filter(treatment %in% c(0,100)) %&gt;% mutate(TIR1 = !str_detect(genotype, &quot;tir1&quot;), AFB1 = !str_detect(genotype, &quot;afb1&quot;)) -&gt; root_growth_inh For now let’s look at the whole dataset. root_growth_inh.int &lt;- aov(percent_inh ~ TIR1*AFB1*treatment, data = root_growth_inh) summary(root_growth_inh.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 6758 6758 48.681 2.69e-11 *** ## AFB1 1 470 470 3.386 0.06692 . ## treatment 1 369212 369212 2659.748 &lt; 2e-16 *** ## TIR1:AFB1 1 3158 3158 22.747 3.14e-06 *** ## TIR1:treatment 1 4190 4190 30.184 9.66e-08 *** ## AFB1:treatment 1 1456 1456 10.485 0.00137 ** ## TIR1:AFB1:treatment 1 7 7 0.047 0.82862 ## Residuals 250 34704 139 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 root_growth_inh.int &lt;- aov(percent_inh ~ TIR1*AFB1*fac_treat, data = root_growth_inh) summary(root_growth_inh.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 6758 6758 48.681 2.69e-11 *** ## AFB1 1 470 470 3.386 0.06692 . ## fac_treat 1 369212 369212 2659.748 &lt; 2e-16 *** ## TIR1:AFB1 1 3158 3158 22.747 3.14e-06 *** ## TIR1:fac_treat 1 4190 4190 30.184 9.66e-08 *** ## AFB1:fac_treat 1 1456 1456 10.485 0.00137 ** ## TIR1:AFB1:fac_treat 1 7 7 0.047 0.82862 ## Residuals 250 34704 139 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ggplot(data = root_growth_inh, aes(x = fac_treat, y = percent_inh, color = genotype)) + geom_boxplot() + geom_point(position = position_jitterdodge(jitter.width = 0.2)) Strong case for an interaction, but not dependent on treatment, interesting. To make this easier to explain we can repeat this analysis after stratifying by treatment. root_growth_inh0.int &lt;- aov(percent_inh ~ TIR1*AFB1, data = filter(root_growth_inh, treatment == 0)) summary(root_growth_inh0.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 43 42.5 0.26 0.611331 ## AFB1 1 2030 2030.3 12.40 0.000598 *** ## TIR1:AFB1 1 1700 1700.0 10.38 0.001619 ** ## Residuals 127 20800 163.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This matches expectations mostly, I would have expected AFB1 to have less effect, but in this model the double mutant is also contributing to this effect size. The interaction here is still a strong effect. 100 nM should be more telling even. root_growth_inh100.int &lt;- aov(percent_inh ~ TIR1*AFB1, data = filter(root_growth_inh, treatment == 100)) summary(root_growth_inh100.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 9623 9623 85.129 9.93e-16 *** ## AFB1 1 84 84 0.746 0.38945 ## TIR1:AFB1 1 1407 1407 12.445 0.00059 *** ## Residuals 123 13903 113 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Very interesting, TIR1 has a very strong effect as expected at this high auxin concentration, whereas AFB1 does not, but the interaction term is significant at both 0 nM and 100 nM IAA. "],["data-visualization.html", "6 Data Visualization 6.1 Reading (complete by class on Monday) 6.2 Tutorials to walk through on your own 6.3 More on Color Palettes", " 6 Data Visualization This week, we’ll start digging into visualizations of the data we have been wrangling. I should rephrase this - we are really only going to scratch the surface of visualizations. Figure from https://callingbullshit.org/tools/tools_misleading_axes.html, original source Swanson et al. (2014). Data visualizations are all around us. From what we read in the popular press to how we tackle problems within Biological Systems Engineering. In this unit, we’re going to focus on making some simple visualizations within RStudio. Prior to doing this, I wanted to begin with some tips to consider in preparing your own visualizations and spotting data visualizations that may inadvertently (or purposely) mislead the viewer. For example, the plot above shows the incidence of thyroid cancer with respect to time, and insinuates that glyphosate (Roundup) correlates to the rising rates of thyroid cancer. Key: Correlation is not causation. What else is wrong with the figure - specifically the secondary y-axis for glyphosate applied? Answer: you can’t have a negative value of glyphosate applied! Here, the authors adjusted the secondary y-axis scale so the red line followed thyroid cancer, which is clearly misleading. This is just one example that Bergstrom and West use in their recent book “Calling Bullshit”. In the table below are key points they implore us to learn and consider as we evaluate visualizations and make our own: CALLING BULLSHIT - TOP ISSUES WITH PLOTS Why? 1. Bar chart axes should include zero. Size gaps can mislead interpretation, and bar graphs meant to look at absolute magnnitude. [visual weight of each bar = value of bar, or proportional ink] 2. Line plots need not include zero. Line graphs emphasize the change in the dependent variable as the independent variable changes. 3. Multiple axes on a single graph Correlation is not causality! 4. Axis should not change scale mid-stream. Clearly can mislead! Learning Objectives: Identify and avoid misleading plots Become familiar with types of visualizations Effectively map data values into quantifiable features of the resulting graphic: these are called aesthetics. Practice basic plotting within R using ggplot 6.1 Reading (complete by class on Monday) This week we will use a resource developed by Clause Wilke, who wrote the book Fundamentals of Data Visualization. We’ll also start by looking at a section of another book by Carl Bergstrom and Jevin West - Calling Bullshit. Calling Bullshit page devoted to Visualizations. Here, read through this page to identify the common pitfalls associated with misleading plots. Correlation does not imply causation (be able to describe what this means) Rule of proportional ink Why a 0-axis for bar graphs, but not when plotting 2 lines on a x-y scatterplot? The below also have LearnR interactive examples you can find in the Files pane (files 06-0-1, 06-0-2, 06-0-3). You can Mapping data. “Whenever we visualize data, we take data values and convert them in a systematic and logical way into the visual elements that make up the final graphic. Even though there are many different types of data visualizations, and on first glance a scatter plot, a pie chart, and a heatmap don’t seem to have much in common, all these visualizations can be described with a common language that captures how data values are turned into blobs of ink on paper or colored pixels on screen.” The key insight is the following: All data visualizations map data values into quantifiable features of the resulting graphic. We refer to these features as aesthetics. Wilke’s cliffnote slides are here (optional). Types of visualizations. Here, you’ll see examples of different visualizations that are useful in our field: amounts, distributions, proportions, x-y scatterplots, uncertainty, and geospatial data. Visualizing distributions. We’ll follow this up with an exercise from C. Wilke. Optional reading/resource: Visualization chapter in R for Data Science. 6.2 Tutorials to walk through on your own Here, the idea is to reinforce the readings, and prepare you for success in the assignment. There are 3 exercises within the Posit workspace for this week. These are built using the learnr package which makes websites that have r code chunks in them so you can practice coding, in a cleaner and easier environment than Posit. You can also Dr. Scott previously created a short video for each that may be helpful: Exercise 1, Exercise 2, and Exercise 3. I am not going to expect any of you to memorize these functions and approaches; rather, I want you to be able to consider what type of visualization you can use, and have the background to dive into creating your own visualization using aesthetics and geoms. This will require you to do some reading in the help documentation of these different functions and refer back to the readings. From my perspective, simple, legible visualizations are best. Regardless, the exercises below will give you a great jumping off point. Aesthetics Exercise 1 - see Wilke’s slides here. Use this to try yourself; the solution for each follows. The point is to learn how the data is “mapped” - the aesthetics. Amounts Exercise 2 - see Wilke’s slides here. Again, this is for you to apply and practice, building on aesthetics but with bar data. Distributions Exercise 3 - see Wilke’s slides here. Lastly, this series highlights approaches to show distributions of data. 6.3 More on Color Palettes Also included in this weeks materials is a vignette from the Viridis package which has colorblind friendly color palletes. R Markdown enables you to weave together content and executable code into a finished document. To learn more about R Markdown see rmarkdown.rstudio.org. This template uses R Markdown to demonstrate the color palettes of R’s Viridis package in three mediums: as an HTML, PDF, or Word document in colors_document.Rmd as a slide deck in colors_presentation.Rmd as a web page with interactive Shiny components in colors_app.Rmd 6.3.1 Previewing 6.3.1.1 To preview the document Open the file colors_document.Rmd. Then click the Knit button that will appear above the opened file. This will display the document as an HTML file. To display the document as a pdf or MS Word file, click the drop down menu to the left of the Knit icon and select one of: Knit to PDF Knit to Word 6.3.1.2 To preview the presentation Open the file colors_presentation.Rmd. Then click the Knit button that will appear above the opened file. This will display the document as an ioslides HTML slide deck, which can be presented with any web browser. To display the presentation as a Slidy (HTML), beamer (PDF), or MS PowerPoint slide deck, click the drop down menu to the left of the Knit icon and select one of: Knit to HTML (Slidy) Knit to PDF (Beamer) Knit to PowerPoint 6.3.1.3 To preview the interactive document with Shiny components Open the file 06-0-4_colors_app.Rmd. Then click the Run Document button that will appear above the opened file. Because the file contains the YAML line runtime: shiny, R Markdown will run the file as an interactive Shiny app. "],["data-visualization-examples.html", "Data Visualization Examples 6.4 Background 6.5 Data Description 6.6 Tasks 6.7 Setup for the Homework", " Data Visualization Examples library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors 6.4 Background Understanding how bird populations vary across different habitats and seasons is an important aspect of ecological research. In this homework, you will explore data collected from bird surveys conducted in three habitat types—Forest, Grassland, and Wetland—over four seasons. The dataset includes both the total bird count and the species richness (i.e., the number of different species observed) for each survey. 6.5 Data Description You are provided with a CSV file named bird_survey_data.csv that contains the following columns: Season: The season when the survey was conducted (e.g., “Spring”, “Summer”, “Fall”, “Winter”). Habitat: The habitat type where the survey was carried out (“Forest”, “Grassland”, or “Wetland”). Bird_Count: The total number of birds counted during the survey. Species_Richness: The number of different bird species observed. 6.6 Tasks Data Import &amp; Preparation: Write an R script that reads the CSV file (birds.csv) into a data frame. birds &lt;- read_csv(&quot;data/birds.csv&quot;) ## Rows: 12 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Season, Habitat ## dbl (2): Bird_Count, Species_Richness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. - Ensure the data types are correctly set (e.g., `Season` and `Habitat` should be treated as factors with a logical ordering for `Season`). Basic Visualization: Use ggplot2 to create a grouped bar plot of Bird_Count versus Season. Differentiate the three habitat types using different fill colors. Adjust the position so that bars for each season are grouped side by side. ggplot(data = birds, mapping = aes(x = Habitat, y = Species_Richness, fill = Season, group = Season)) + labs(x = &quot;Habitat type&quot;) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + scale_fill_brewer(palette = 4) Enhanced Plot Features: Customize your plot with informative axis labels (e.g., “Season” and “Bird Count”), a descriptive title, and a clear legend. Apply an appropriate theme (e.g., theme_minimal() or theme_classic()). Optional Extension: Create a second plot visualizing Species_Richness versus Season using a similar bar plot. Alternatively, explore using facets (with facet_wrap()) to compare both Bird_Count and Species_Richness across the different habitats in one multi-panel figure. 6.7 Setup for the Homework Video introduction / help for homework Hand in: a single pdf document with part 1 and part 2. Be sure that you explain why you chose the particular plots, and that the plots contain proper axis labels, formatting, etc. Part 1. Dr. Senger is a Metabolic Engineer in our department; perhaps you’ll have him or had him for Thermo. His work includes developing biosensors, and he has a recent publication in Pubmed (see here). Imagine you’re an undergrad researcher in Dr. Senger’s group, and are asked to recreate Figure 2a from the publication within Rstudio. The raw data for the figure is available in the supplemental information, and in the Rstudio workspace for this week as an excel file (file contains data for all figures). Follow examples for bar graphs from the exercises (visualizing amounts). You’ll want your final bar graph to look similar to this one, but you can choose the theme/color scheme. Below, I used theme_economist(). Part 2. Dr. Shortridge is a Hydrologist in our department, and her work includes analyzing large hydrologic datasets. Here, imagine you’re asked to compare approaches for visualizing the distribution of monthly streamflow in 2020 for the Rappahannock River at Fredericksburg. Based on the distribution exercise, create a boxplot, violin plot, strip-jitter plot, and ridge plot. See the examples below for good and bad plots; you’ll want your plots to be “good”. Choose one figure that you like the best, and provide an explanation why. Axis labels are just abbreviations and are missing units data does not make sense (e.g. not grouped by month) Ticks for y-axis are not between 0.1 - 100. Used simple theme in cowplot package Used x-variable in factor form (by using month function within lubridate and include label and abbr; type ?month) Normalized y-axis to 1000 :: aes (x = xvar, y =yvar/1000) Added group attribute in aes statement Added theme (e.g. theme_cowplot). Check out the cowplot package and ggthemes. There are many to choose from! Why do I like the theme above? Simple, axis labels slightly larger than tick labels, clear. Here’s another example using theme_economist. When you create a plot, assign it a variable. Then you can easily change themes/attributes of the plot. For your ridge-line plot, choose a fill color that you like. You add the fill color within the geom_density_ridges function (e.g. geom_density_ridges(rel_min_height = 0.01,fill=\"dodgerblue2\"). A list of colors can be found here: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf "],["introduction-to-linear-regression-in-r.html", "7 Introduction to Linear Regression in R 7.1 Objectives 7.2 Reading 7.3 Active Package Libraries 7.4 Linear Regression 7.5 Now, let’s say you want to predict a y-value for a given age. 7.6 Example 2 from reading", " 7 Introduction to Linear Regression in R 7.1 Objectives This week our goals are to be able to: Linear Regression Fundamentals: Understand the basics of linear regression analysis in R. Learn how to interpret model output and assess model performance. Assumptions and Diagnostics: Identify and understand the assumptions of linear regression. Gain proficiency in diagnosing model assumptions using plots and tests. Workflow and Data Preparation: Develop a structured workflow for conducting linear regression analysis. Learn techniques for data preparation and visualization. Data Transformation and Model Improvement: Explore methods for transforming data to meet model assumptions. Understand how data transformations can enhance model fit. Interpretation of Diagnostic Plots: Learn to interpret diagnostic plots to assess model adequacy. Prediction and Inference: Use regression models for prediction and inference. Understand how to compute and interpret confidence and prediction intervals. 7.2 Reading Chapter 3 A Review of R Modeling Fundamentals from Tidy Modeling with R by Max Kuhn and Julia Silge 7.3 Active Package Libraries This tutorial is derive from https://www.datacamp.com/community/tutorials/linear-regression-R library(ggplot2) library(cowplot) library(readxl) 7.4 Linear Regression R-studio provides the ability to create linear regressions easily, sometimes too easily. While this is not meant to be a substitute for a statistics course, the objective in this short tutorial is to develop a workflow approach that allows you to test the validity of regressions. 7.4.1 Assumptions Below is a figure from https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf. alt text here 7.4.2 Workflow: Read in data Plot data &amp; visualize linearity Transform data as appropriate Create linear model using lm function. Assess assumption 1 review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior Test for homoscedasticity Assess assumption 3, the variability in the residuals does not vary over the range of predicted values if fails, transform data or choose an alternate model/independent variable Test for bias Assess assumption 4, e values generally plot equally above and below zero Test for normality 7.4.3 Example 7.4.3.1 Step 1. Read in data ageandheight &lt;- read_excel(&quot;data/ageandheight.xls&quot;,sheet=&quot;Hoja2&quot;) ## There is one data point that does not read in correctly, thus the statement below corrects for this. ageandheight$height[7] &lt;- 79.9 7.4.3.2 Step 2. Plot data &amp; visualize linearity ## here you can either start a ggplot or just use the simple plot command. Since we&#39;re practicing ggplot, let&#39;s stick with this. p &lt;- ggplot(ageandheight, aes(age,height)) + geom_point() + cowplot::theme_cowplot() + ## adds theme scale_y_continuous(breaks=seq(76,84,2)) + ## changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(16,31,2)) + ylab(&quot;Height [cm]&quot;) + ## adds y-label with units xlab(&quot;Age [months]&quot;) p Check: The resulting plot looks fairly linear; let’s proceed! 7.4.3.3 Step 3. Transform data ## not required here; if required, repeat step 2. 7.4.3.4 Step 4. Linear model Create linear model using the lm function. - Assess assumption 1 - review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. - review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior ## Create linear model model.lm &lt;- lm(height~age, data=ageandheight) summary(model.lm) ## ## Call: ## lm(formula = height ~ age, data = ageandheight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27238 -0.24248 -0.02762 0.16014 0.47238 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 64.9283 0.5084 127.71 &lt; 2e-16 *** ## age 0.6350 0.0214 29.66 4.43e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.256 on 10 degrees of freedom ## Multiple R-squared: 0.9888, Adjusted R-squared: 0.9876 ## F-statistic: 880 on 1 and 10 DF, p-value: 4.428e-11 Check: |t-values| &gt;&gt; 2, proceed ## review leverage/influence of data points on regression. Use plot of Cook&#39;s D, evaluate subset of Cooks D above threshold, and evaluate DFFITS (another influence diagnostic) plot(cooks.distance(model.lm), pch = 16, col = &quot;blue&quot;) #Plot the Cooks Distances. There are a few high points here at the beginning. Let’s see if any fall outside of the critical value on the F-distribution (the qf function determines the critical value for our number of observations and number of coefficients). n &lt;- length(model.lm$residuals) ## n = the number of observations p &lt;- length(model.lm$coefficients) # p = the number of coefficients subset(cooks.distance(model.lm), cooks.distance(model.lm) &gt; qf(0.1, p, n - p, lower.tail = FALSE)) # determines if there are any flagged observations from Cooks D ## named numeric(0) For SLR (simple linear regression) with more than about 30 observations, the critical value for D would be about 2.4. So we don’t get any values out, hence the named numeric(0), zero observations were flagged. What about DFFITS (difference in fits with and without that point)? subset(dffits(model.lm), dffits(model.lm) &gt; 2 * sqrt(p / n)) # determines if there are any flagged observations from DFFITS ## 3 ## 1.127423 Now, observation 3 was identified as having higher influence on the fit than other points. Consider options 1-3 described in workflow. Is there something wrong with this point? 7.4.3.5 Step 5. Test for homoscedasaticity # Here, the which variable provides the ability to create 4 plots of interest: &quot;Residuals vs Fitted&quot;, &quot;Normal Q-Q&quot;, &quot;Scale-Location&quot;, &quot;Cook&#39;s distance&quot;, &quot;Residuals vs Leverage&quot; # To test for homoscedasticity, review plot of standardized residuals plot(model.lm, which = 3, ask = FALSE) Check: Variability is not significant over fitted values 7.4.3.6 Step 6. Test for bias # To test for bias, review plot of residuals plot(model.lm, which = 1, ask = FALSE) Check: Variability above and below 0 is similar without a distinct pattern 7.4.3.7 Step 7. Test for Normality # To test for normality, review plot plot(model.lm, which = 2, ask = FALSE) Check: Most points (with exception of observation 1) fall on the line, suggesting a normal distribution of residuals 7.4.4 Predicting values: Applications of linear models The following workflow provides predicted values and confidence intervals of these estimates for new values based on a linear regression model. The final steps are to create a plot with uncertainty bounds and the ability to predict a value and associated uncertainty in that predicted value. 7.4.4.1 Workflow: Confidence intervals are computed using the predict command: predict(lmheight, newdata = data.frame(age=22.5), interval = &quot;confidence&quot;, level = 0.95) Prediction intervals are computed as follows: predict(lmheight, newdata = data.frame(age=22.5), interval = &quot;prediction&quot;, level = 0.95) Prediction intervals are always greater than confidence intervals. While they include the uncertainty in the regression coefficients, the slope and intercept, they also includes the unexplained variability in y within the original data. 7.4.4.2 Example # Use model to create prediction intervals model.predict &lt;- predict(model.lm, interval = &quot;predict&quot;) ## Warning in predict.lm(model.lm, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses # Use model to create confidence intervals model.confidence &lt;- predict(model.lm, interval = &quot;confidence&quot;) colnames(model.confidence) &lt;- c(&quot;cfit&quot;, &quot;clwr&quot;, &quot;cupr&quot;) #rename columns # Create dataset that merges dataset data.all &lt;- cbind(ageandheight,model.predict, model.confidence) # Create ggplot p &lt;- ggplot(data.all, aes(x = age, y = height)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;Height [cm]&quot;) + xlab(&quot;Age [months]&quot;) + scale_y_continuous(breaks=seq(76,84,2)) + scale_x_continuous(breaks=seq(16,31,2)) p The resulting plot contains the confidence and prediction intervals over the range of x-values. 7.5 Now, let’s say you want to predict a y-value for a given age. When making predictions, you’ll want to use predict and not confidence. The rationale is that this approach provides a better sense of incorporating not just the confidence in the intercept and slope, but also the unexplained variation in the y-values. a &lt;- data.frame(&quot;age&quot; = 18.1) # key here is to label column name the same as what is used in the model.lm! value.predict &lt;- predict(model.lm, newdata=a, interval = &quot;predict&quot;, level = 0.95) value.predict ## fit lwr upr ## 1 76.42119 75.77412 77.06826 Thus, for an age of 18.1 months, the predicted height is 76.4 (75.77 - 77.1, alpha = 95%).’ 7.6 Example 2 from reading # Read in data press &lt;- read_excel(&quot;data/pressure.xlsx&quot;) # Plot data p &lt;- ggplot(press,aes(Temperature,Pressure)) + geom_point() + geom_smooth(method = &quot;lm&quot;, level = 0.95) p ## `geom_smooth()` using formula = &#39;y ~ x&#39; What do you notice??? Are the residuals going to be random? Does negative pressure make sense? lmTemp = lm(Pressure~Temperature, data = press) #Create the linear regression plot(lmTemp$residuals, pch = 16, col = &quot;red&quot;) So what to do? Transformation!We will learn more about these on Wednesday. press$x2 &lt;- press$Temperature^2 lmTemp2 = lm(Pressure~Temperature + I(Temperature^2), data = press) #Create the linear regression plot(lmTemp2$residuals, pch = 16, col = &quot;red&quot;) summary(lmTemp2) ## ## Call: ## lm(formula = Pressure ~ Temperature + I(Temperature^2), data = press) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6045 -1.6330 0.5545 1.1795 4.8273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.750000 3.615591 9.335 3.36e-05 *** ## Temperature -1.731591 0.151002 -11.467 8.62e-06 *** ## I(Temperature^2) 0.052386 0.001338 39.158 1.84e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.074 on 7 degrees of freedom ## Multiple R-squared: 0.9996, Adjusted R-squared: 0.9994 ## F-statistic: 7859 on 2 and 7 DF, p-value: 1.861e-12 # plot fitted smooth line lmTemp2plot&lt;- data.frame(lmTemp2$fitted.values,press$Temperature) p &lt;- p + geom_line(data = lmTemp2plot, aes(x = press.Temperature,y=lmTemp2.fitted.values),color = &#39;#E51837&#39;) p ## `geom_smooth()` using formula = &#39;y ~ x&#39; "],["linear-regression-examples.html", "8 Linear Regression Examples 8.1 Active Libraries 8.2 Linear Regression 8.3 Assumptions 8.4 Example 8.5 Take 2, try second order!", " 8 Linear Regression Examples This tutorial is derived from https://www.datacamp.com/community/tutorials/linear-regression-R 8.1 Active Libraries library(ggplot2) library(cowplot) library(readxl) 8.2 Linear Regression R-studio provides the ability to create linear regressions easily, sometimes too easily. While this is not meant to be a substitute for a statistics course, the objective in this short tutorial is to develop a workflow approach that allows you to test the validity of regressions. 8.3 Assumptions Below is a figure from https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf. alt text here 8.3.0.1 Workflow: Read in data Plot data &amp; visualize linearity Transform data as appropriate Create linear model using lm function. * Assess assumption 1 review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior Test for homoscedasaticity * Assess assumption 3, the variability in the residuals does not vary over the range of predicted values if fails, transform data or choose an alternate model/independent variable Test for bias * Assess assumption 4, e values generally plot equally above and below zero Test for normality 8.4 Example 8.4.1 Step 1. Read in data S &lt;- c(1.3, 1.8, 3, 4.5, 6, 8, 9) v &lt;- c(.07, .13, .22, .275, .335, .35, .36) data.ex &lt;- data.frame(S,v) 8.4.2 Step 2. Plot data &amp; visualize linearity ## here you can either start a ggplot or just use the simple plot command. Since we&#39;re practicing ggplot, let&#39;s stick with this. p &lt;- ggplot(data.ex, aes(S,v)) + geom_point() + cowplot::theme_cowplot() + # adds theme scale_y_continuous(breaks=seq(0,0.8,.2)) + # changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(0,9,1)) + ylab(&quot;v&quot;) + # adds y-label with units xlab(&quot;[S]&quot;) p Check: The resulting plot does not look linear. We will compare 2 transformations: 8.4.3 Step 3. Transform data data.ex$vt &lt;- 1/data.ex$v data.ex$St &lt;- 1/data.ex$S # TODO remove 2nd order here and add actual Michaelis-menten equation and actual linearization examples data.ex$S2t &lt;- 1/data.ex$S^2 8.4.4 Step 4. Linear model Create linear model using lm function. * Assess assumption 1 - review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. - review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior # Create linear model, 1st order Michaelis Menton model.lm &lt;- lm(vt~St, data=data.ex) summary(model.lm) ## ## Call: ## lm(formula = vt ~ St, data = data.ex) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 1.47839 -1.61027 -1.11218 -0.19880 0.06114 0.61664 0.76508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.1902 0.7762 0.245 0.816147 ## St 16.4022 1.9435 8.440 0.000383 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.185 on 5 degrees of freedom ## Multiple R-squared: 0.9344, Adjusted R-squared: 0.9213 ## F-statistic: 71.23 on 1 and 5 DF, p-value: 0.0003833 vm &lt;- 1/model.lm$coefficients[1] ks &lt;- vm*model.lm$coefficients[2] vm ## (Intercept) ## 5.256963 ks ## (Intercept) ## 86.22596 Check: |t-values| &lt; 2 for intercept, questionable. When vm is calculated, it’s a lot higher than reported velocities, and the half-saturation constant ks is higher than the maximum substrate concentration. # review leverage/influence of data points on regression. Use plot of Cook&#39;s D, evaluate subset of CooksD above threshold, and evaluate DFFITS (another influence diagnostic) plot(cooks.distance(model.lm), pch = 16, col = &quot;blue&quot;) #Plot the Cooks Distances. n &lt;- length(model.lm$residuals) p &lt;- length(model.lm$coefficients) subset(cooks.distance(model.lm), cooks.distance(model.lm) &gt; qf(0.1, p, n - p, lower.tail = FALSE)) # determines if there are any flagged observations from CooksD ## 1 ## 4.809895 subset(dffits(model.lm), dffits(model.lm) &gt; 2 * sqrt(p / n)) # determines if there are any flagged observations from DFFITS ## 1 ## 11.80479 For SLR with more than about 30 observations, the critical value for Di would be about 2.4. In the example above, observation 1 was identified as having higher influence for both Cooks and DDFITS. Consider options 1-3 described in workflow, but also intercept was questionable. 8.4.5 Step 5. Test for homoscedasaticity # Here, the which variable provides the ability to create 4 plots of interest: &quot;Residuals vs Fitted&quot;, &quot;Normal Q-Q&quot;, &quot;Scale-Location&quot;, &quot;Cook&#39;s distance&quot;, &quot;Residuals vs Leverage&quot; # To test for homoscedasaticity, review plot of standardized residuals plot(model.lm, which = 3, ask = FALSE) Check: Variability is a bit more variable. 8.4.6 Step 6. Test for bias # To test for bias, review plot of residuals plot(model.lm, which = 1, ask = FALSE) Check: Variability above and below 0 does not appear random. 8.4.7 Step 7. Test for Normality # To test for normality, review plot plot(model.lm, which = 2, ask = FALSE) Check: Residuals are not normally distributed. Questionable!! 8.4.8 Application The workflow provides confidence of a reasonable linear regression model. The final steps are to create a plot with uncertainity bounds and the ability to predict a value and associated uncertainity. ###Workflow: Confidence intervals are computed using the predict command: predict(lmheight, newdata = data.frame(age=22.5), interval = “confidence”, level = 0.95) Prediction intervals are computed as follows: predict(lmheight, newdata = data.frame(age=22.5), interval = “prediction”, level = 0.95) Prediction intervals are always greater. While it includes the uncertainity in the regression uncertainties in the slope and intercept, it also includes the unexplained variability in y. # Use model to create prediction intervals model.predict &lt;- predict(model.lm, interval = &quot;predict&quot;) ## Warning in predict.lm(model.lm, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses # Use model to create confidence intervals model.confidence &lt;- predict(model.lm, interval = &quot;confidence&quot;) colnames(model.confidence) &lt;- c(&quot;cfit&quot;, &quot;clwr&quot;, &quot;cupr&quot;) #rename columns # Create dataset that merges dataset data.all &lt;- cbind(data.ex,model.predict, model.confidence) # Create ggplot p1 &lt;- ggplot(data.all, aes(x = St, y = vt)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;1/v&quot;) + xlab(&quot;1/S&quot;) p1 data.all$fit &lt;- 1/data.all$fit data.all$lwr &lt;- 1/data.all$lwr data.all$upr &lt;- 1/data.all$upr data.all$clwr &lt;- 1/data.all$clwr data.all$cupr &lt;- 1/data.all$cupr p &lt;- ggplot(data.all, aes(x = S, y = v)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;v&quot;) + xlab(&quot;S&quot;) + scale_y_continuous(breaks=seq(0,0.8,.2)) + # changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(0,9,1)) + ylim(0,0.8) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p ## Warning: Removed 4 rows containing missing values or values outside the scale range ## (`geom_line()`). 8.5 Take 2, try second order! 8.5.1 Step 4. Linear model Create linear model using lm function. * Assess assumption 1 - review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. - review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior # Create linear model, 1st order Michaelis Menton plot(data.ex$S2t,data.ex$vt) model.lm &lt;- lm(vt~S2t, data=data.ex) summary(model.lm) ## ## Call: ## lm(formula = vt ~ S2t, data = data.ex) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.371400 -0.737166 -0.056662 0.230297 -0.002375 0.105165 0.089340 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4492 0.1877 13.05 4.71e-05 *** ## S2t 19.3760 0.7310 26.51 1.43e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.389 on 5 degrees of freedom ## Multiple R-squared: 0.9929, Adjusted R-squared: 0.9915 ## F-statistic: 702.7 on 1 and 5 DF, p-value: 1.428e-06 vm &lt;- 1/model.lm$coefficients[1] ks &lt;- sqrt(vm*model.lm$coefficients[2]) vm ## (Intercept) ## 0.408292 ks ## (Intercept) ## 2.812661 Check: |t-values| &gt;&gt; 2, looks better The half-saturation constant and vm are also consistent with the data. # review leverage/influence of data points on regression. Use plot of Cook&#39;s D, evaluate subset of CooksD above threshold, and evaluate DFFITS (another influence diagnostic) plot(cooks.distance(model.lm), pch = 16, col = &quot;blue&quot;) #Plot the Cooks Distances. n &lt;- length(model.lm$residuals) p &lt;- length(model.lm$coefficients) subset(cooks.distance(model.lm), cooks.distance(model.lm) &gt; qf(0.1, p, n - p, lower.tail = FALSE)) # determines if there are any flagged observations from CooksD ## 1 ## 9.365224 subset(dffits(model.lm), dffits(model.lm) &gt; 2 * sqrt(p / n)) # determines if there are any flagged observations from DFFITS ## 1 ## 13.90953 For SLR with more than about 30 observations, the critical value for Di would be about 2.4. In the example above, observation 1 was identified as having higher influence for both Cooks and DDFITS. Consider options 1-3 described in workflow, but also intercept was questionable. 8.5.2 Step 5. Test for homoscedasaticity # Here, the which variable provides the ability to create 4 plots of interest: &quot;Residuals vs Fitted&quot;, &quot;Normal Q-Q&quot;, &quot;Scale-Location&quot;, &quot;Cook&#39;s distance&quot;, &quot;Residuals vs Leverage&quot; # To test for homoscedasaticity, review plot of standardized residuals plot(model.lm, which = 3, ask = FALSE) Check: Variability is variable. 8.5.3 Step 6. Test for bias # To test for bias, review plot of residuals plot(model.lm, which = 1, ask = FALSE) Check: Variability above and below 0 somewhat random. 8.5.4 Step 7. Test for Normality # To test for normality, review plot plot(model.lm, which = 2, ask = FALSE) Check: Residuals are not normally distributed. Questionable. 8.5.5 Application The workflow provides confidence of a reasonable linear regression model. The final steps are to create a plot with uncertainity bounds and the ability to predict a value and associated uncertainity. ###Workflow: Confidence intervals are computed using the predict command: predict(lmheight, newdata = data.frame(age=22.5), interval = “confidence”, level = 0.95) Prediction intervals are computed as follows: predict(lmheight, newdata = data.frame(age=22.5), interval = “prediction”, level = 0.95) Prediction intervals are always greater. While it includes the uncertainity in the regression uncertainties in the slope and intercept, it also includes the unexplained variability in y. # Use model to create prediction intervals model.predict &lt;- predict(model.lm, interval = &quot;predict&quot;) ## Warning in predict.lm(model.lm, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses # Use model to create confidence intervals model.confidence &lt;- predict(model.lm, interval = &quot;confidence&quot;) colnames(model.confidence) &lt;- c(&quot;cfit&quot;, &quot;clwr&quot;, &quot;cupr&quot;) #rename columns # Create dataset that merges dataset data.all &lt;- cbind(data.ex,model.predict, model.confidence) # Create ggplot p1 &lt;- ggplot(data.all, aes(x = S2t, y = vt)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;1/v&quot;) + xlab(&quot;1/S^2&quot;) p1 data.all$fit &lt;- (1/(data.all$fit)) data.all$lwr &lt;- (1/(data.all$lwr)) data.all$upr &lt;- (1/(data.all$upr)) data.all$clwr &lt;- (1/(data.all$clwr)) data.all$cupr &lt;- (1/(data.all$cupr)) p &lt;- ggplot(data.all, aes(x = S, y = v)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;v&quot;) + xlab(&quot;S&quot;) + scale_y_continuous(breaks=seq(0,0.4,.1)) + # changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(0,9,1)) + ylim(0,0.8) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p The resulting plot contains the confidence and prediction intervals over the range of x-values. Is transforming the data the best approach? The resulting transformation resulted in a linear model. Were the remainder of the tests valid? Not necessarily for the normality. The other approach would be to fit the data with a non-linear best-fit curve. Regardless, you can see how you’d apply linear regression to data. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
