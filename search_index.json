[["index.html", "Data Analysis and Numerical Methods for Biological Systems Engineers About this book", " Data Analysis and Numerical Methods for Biological Systems Engineers Durelle Scott R. Clay Wright 2025-04-07 About this book This book was developed as a companion text for the Virginia Tech Biological Systems Engineering course BSE3144: Engineering Analysis for Biological Systems using Numerical Methods. This course focuses on the process of solving engineering problems related to biological systems using numerical analysis including root finding, numerical integration, differentiation, interpolation and numerical solution of ordinary differential equations, error analysis and programming with engineering software. R is the programming language we will use in this course. We will spend the first portion of this course devoted to learning the basics of structured and modular programming. Although we focus on R, the described techniques are also applicable in any other computer languages. Furthermore, the techniques provide a logical approach to addressing complex problems, breaking them down into manageable (and solvable) pieces. The numerical computation techniques included in this course are utilized by many engineers. For biological systems engineers, the applications of the techniques include consideration of characteristics of biological systems, such as natural variability, growth/decay cycles, nonhomogeneity, anisotropy, and process uncertainty. Learning Objectives: Upon successful completion of this course, the student will be able to: Apply the following numerical techniques to solve problems in biological systems engineering: regression analysis root finding solving systems of linear equations interpolation differentiation and integration solving ordinary differential equations Define and quantify sources of error in numerical techniques Write programs, using engineering software, that involve loops, logical block constructs, function, plotting, and input/output "],["introduction-to-posit-rstudio-markdown.html", "1 Introduction to Posit, Rstudio, &amp; Markdown 1.1 Pre-work 1.2 Posit 1.3 Rstudio", " 1 Introduction to Posit, Rstudio, &amp; Markdown This week our goals are to be able to: Navigate the Posit and RStudio interface with proficiency. Articulate the process of using RStudio to develop markdown documents. Practice using markdown to format reproducible documents 1.1 Pre-work Read https://posit.cloud/learn/guide to learn a little bit about the Posit cloud space To get a quick intro to the Rstudio user interface watch this YouTube Video, or this one which is a little longer and provides some more motivation, or check out the Posit/Rstudio User Guide (if you prefer reading to videos). Check out the Posit/RStudio Introduction to Rmarkdown and Quarto Peruse the material below and complete week 1 concept check (in Canvas) Open the 01-2_Problem-Set.Rmd file from the files pane in the lower right and be ready to work through these materials on Monday. 1.2 Posit Posit, the company, used to be called RStudio, but as they expanded beyond offering services just for R they decided to change their name. I thought that RStudio, the software, was also going to change names, because it is not just for working with R, but it hasn’t yet changed names. Posit now makes RStudio (which is still open source!) and also hosts RStudio servers via Posit.cloud that we pay for to help you all learn R, basic programming, data science, and engineering data analytics. Within the Posit Cloud you will see “Spaces” to the left including the BSE 3144 workspace for this class. Within this workspace there are projects for each week. I can make “assignment” projects, which means that when you open the project it will make a copy of the project that you own and can edit and complete. Think about each of these new projects you make as your personal notebook for the class. Add your notes as you work through the material. Within the project is the RStudio interface. This is just like if you installed R and RStudio on your computer only it is in a website. You can download R and RStudio on your computer if you like, following the directions at https://posit.co/download/rstudio-desktop/, but especially as we get deeper into the material installing packages can take a good chunk of time. By working in the environment we have set up for you in the Posit Cloud you can hopefully spend time learning in class or at home as opposed to setting up your computing environment. We will cover installing packages next week, and setting up your environment is a useful skill, but it is largely an exercise in patience, with the occasional internet troubleshooting/debugging (which we will cover in detail by week 03). 1.3 Rstudio Rstudio (or Posit if they ever change it) is an integrated development environment, or IDE. An IDE is a program in which you can write code, view the outputs it creates, keep your code and outputs organized, and many other useful things. You can write R code in RStudio as well as many other programming languages. RStudio has lots of nice features. You can check out many of them by scanning through the menu bar. Tools&gt;Global Options… also has many nice customization options including the the coveted hacker dark mode themes in the “Appearance” tab by changing the Editor Theme. A good IDE can make learning a language much easier, and even if you are an experienced coder a good IDE can make you much more efficient. Let’s take a look around the RStudio window. You will see several panes within the RStudio window. You will see several panes within the RStudio window. 1.3.1 The Source pane If a file is open, in the top left of the screen perhaps this one, you will see the Source pane. (If you don’t see this pane, create a new file by pressing shift + command(or ctrl) + N, or click the universal ‘New Document’ icon and select ‘R script’). Source shows the files with which you are currently working. This is where you will spend most of your time building and testing lines of code and then combining them to make scripts for data analysis. In general you should be doing most all of your typing into the source pane to build a document that you can use to reproduce your analysis. In this class we will be mostly working in markdown documents in the source pane. 1.3.2 Markdown Markdown is a document formatting computer language. The name markdown comes from html, or HyperText Markup Language, what websites are written in (along with a lot of other languages these days). Markdown is meant to be a much more chill version of html, but can also be used to make just about any kind of document from websites to PDFs, PowerPoint, and Word docxs. Markdown uses very simple text-based symbols to format text into potentially very attractive and useful documents. Markdown is used in many different places these days, but we’ll use it in this class to help guide our data analyses. Remember: “If you are thinking without writing, you only think you are thinking!” -Leslie Lamport (emphasis my own). We will use markdown in the form of Rmarkdown or Quarto files. Rmarkdown is a product of Rstudio the company, and holding with the trend of changing names, the newest extended version of Rmarkdown is called Quarto, which really goes beyond R to include any programming language (or many at least). We will mostly use Rmarkdown in this class, for now, as most of the materials were developed before Quarto. But feel free to experiment with Quarto and I will do the same as I develop new materials. When you make a new R Markdown document the following useful template begins the file. 1.3.2.1 R markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 1.3.2.2 Including Plots You can also embed plots, for example: Note that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot. This provides a concise introduction to markdown and its power. Through simple text symbols your can format text to be headings (with #’s), make text bold, denote what is code, and offset code chunks with three back-ticks (the key next to 1 on your keyboard) and curly braces indicating the programming language. Code chunks (or blocks) also allow you to name them and changing some settings with how the code should be run and output. Note how the above and all of this document (in its markdown format) has empty lines between text elements. This makes sure that particularly headings and sections, like numbered/bullet lists, are formatted correctly. 1.3.3 More Markdown formatting There are lots of cool things you can do with markdown. Check out the resources available from links in the menu bar by going to Help&gt;Markdown Quick Reference as well as Help&gt;Cheat Sheets. There are cheat sheets for the RStudio IDE and a couple for R Markdown. We will practice with those a bit in Examples and your Problem Set. You can also click “Visual” next to “Source” up at the top of this file to use a more traditional rich text format editor using mouse clicks to format your text. I have found this function is still a little glitchy and you especially need to be careful pasting text into the Visual editor. There are some things the visual editor does make easier like inserting images and citations and formatting tables. As you’ll see below with the code chunk settings in Code chunk/block settings and global settings below. Links and images are particularly useful for documents. You can link to any website you simply sandwich the text you want to be linked in square brackets, followed by parentheses containing the link/web-address/URL (uniform resource locator). Links can also be to sections (defined by headings in your document) as you see above (figuring out how the names change can be tricky though, I had to look in the knitted html file for the section above). Images are similar but the link is the alt-text or caption text, and the link is a link or path to the image. The image can be somewhere in your project folder or anywhere on the internet. So for example we could use the code![This is a screenshot highlighting the path to this screenshot](images/Screenshot 2024-01-24 at 9.54.34 PM.png) to insert this screenshot which is in this projects images folder: This is a screenshot highlighting the path to this screenshot For all file paths that you use in a markdown document the path is relative to the markdown document. So this Rmd is in the 01_Posit-Rstudio-Markdown folder and we only need to specify that, relative to this file, the screenshot above is in the images folder/directory and then we also need to specify the file name of the image. 1.3.4 Running code Any line of code in a code chunk the source pane can be run in the Console pane by placing your cursor on that line and pressing control+enter (command+return on macs). Practice running the lines below in the code chunk. 2+2 ## [1] 4 9/3 ## [1] 3 3^3 ## [1] 27 2&gt;3 ## [1] FALSE 2&lt;3 ## [1] TRUE The result of the code is printed right below the chunk and is also shown in the Console pane below. You can also use the green play button (right-facing triangle) to run all of the lines in the chunk, or try to run either of the back-tick-containing top and bottom lines. 1.3.5 The Console pane The Console pane is in the bottom-left corner labeled Console at the top left. (If the Source pane is closed, the Console pane may take up the whole left side of the window). The Console is essentially an R command line window. You can type in anything following the &gt; then press return (or Enter) and whatever you typed will be interpreted by R and the appropriate output will be returned. R understands all basic algebra as well as logical expressions (aka Boolean expressions, such as 5&lt;7 or True &amp; False = False). Most of the time you want to type your code into the source pane, but occasionally you will want to test something out or do a quick calculation that might not be needed in the file you are working on. Then you can work directly in the console. In addition to being a basic calculator, R interprets functions or variables that can be assigned to and represented by words. Variables (any piece of data of any variety) will be single words possibly followed by a $ or square brackets [] if the variable is a matrix (a two dimensional array of data, where matrix$y would return column y and matrix[x,y] would return the piece of data in row x and column y). Functions (blocks of code that perform a specific function) are followed by parentheses containing the arguments/parameters on which that function will operate. For example, print(\"Hello World\"), uses the print function to print the text argument (an argument is anything we pass to a function) “Hello World” if you run that line in the console. Variables and Functions R is an object oriented language. This means we can use R to create abstracted objects that contain data (of any type, shape or size) called variables or procedures/methods (individual blocks of code) called functions. There are numerous functions and datasets included in the base R installation. Also, as an open source language countless programmers in the R community have written useful functions and created useful datasets that are freely available in the form of R-packages (more on these later). You can also write your own! But more on this in the The big difference between using RStudio and running R from the command line is that this pane has an auto-complete feature. Try typing pri into the console below (or in a code chunk or Source pane and pressing tab. RStudio automatically provides you with a list of all the available functions and variables beginning with ‘pri’! You can navigate this list using the arrow keys or your mouse. When you select a particular object, RStudio also gives you some information about that object. Navigate down to print (if there are multiple, select the one that has {base} at the far right) and press tab. You will see that RStudio has completed print in the console and added a set of parentheses because print is a function, print(). Now we can add arguments that this function will operate on, within the parentheses. But what does this function do? To figure out type ?print in the Console and press return. This opens the documentation for this function in the Help pane. A ? before any function name, or passing a function name to the help() function will do the same. 1.3.6 The Help pane This pane is essentially a browser window for R documentation. You can also search for functions or variables in R and all of the installed packages on your computer using the search box at the top. You can search within a documentation page using the Find in Topic box. Using this pane you should be able to answer almost any question you have about any R function. All R documentation follows standard formatting. Description is pretty self explanatory. Usage demonstrates how you use the function, sometimes with specifics for different variable types. For print this shows us that print takes the input argument x (an argument is just variable that is used in a function). If x is a ‘factor’ or a ‘table’, print will also take some additional arguments. In the Usage section the default value of each argument is listed (e.g. FALSE is the default value for argument quote). A description of each argument is listed below in the Arguments section. Value is the type of data returned by the function. There are a few other self-explanatory sections and finally Examples. This is often one of the most useful sections as it shows you how to use the function. The code in Examples can be copied and pasted into the console and run. 1.3.7 The Files/Plots/Packages/Help/Viewer pane The Help pane contains additional tabs that can also be quite helpful. Files allows you to navigate through folders on your computer and open files. Plots shows you the most recent plot your code has produced and allows you to save it. Packages allows you to install and load packages into memory. Packages are bundles of code that other people have written and shared with the community (more about packages later). Additionally, there is a search engine specific to R resources including the documentation, blogs, books and questions users have asked on discussion boards. This invaluable resource is at Rseek.org. This is especially helpful if you want to find a function to perform a specific task. 1.3.8 OK, back to the Source pane… You should have print() in there now. Put your cursor in the middle of the parentheses and press tab. RStudio will feed you all of the arguments of this function using auto-complete! Press tab again and x = will appear in the parentheses. Type \"hello world\" and press command/ctrl + return/enter. This will copy your line of code into the console and execute it. Amazing! Your first line of R code worked, hopefully… print(x = &quot;hello world&quot;) # prints hello world in the console ## [1] &quot;hello world&quot; Take note that if you are missing the quotes around hello world R will look for a variable named hello and return an error. If your code didn’t work, try to fix it and run it again. command + return (ctrl + enter on Windows machines) can be used to execute a whole line or any selected code. Now just highlight x = \"hello world\" within the print(x = \"hello world\") line and press command + return (or ctrl + enter). This will just execute the highlighted text. If everything worked properly, you have just created your first variable object in R! 1.3.9 The Environment pane See, over on the top right next to x (the variable object name) is “hello world” (the value assigned to that variable). You can now execute just print(x), and you will get [1] \"hello world\"! The Environment pane shows all of the objects you have created or stored in memory. You can view data sets or functions by clicking on them, but at the moment we only have the simple variable x. Don’t worry, we’ll practice this later. 1.3.10 The History/Connections/Git/Build/Tutorial pane The Environment pane also has many tabs that can do lots of cool things. For now all that I want to cover is the History pane/tab. This keeps track of all of the code you have run. So if you forgot what you typed into the console, or you want to transfer something you ran in the Console into the Source pane, the History pane will do that for you. 1.3.11 Terminal/Render/Background Jobs Also the Console pane has other tabs. Terminal can run Unix code. Render is for tracking markdown documents as they are rendering into whatever their output format is. We are going to expand on using these panes and practice writing some more R code next week but for now we are going to focus a little more on R Markdown. 1.3.12 YAML headers Each R Markdown or Quarto document starts with a YAML header like: --- title: &quot;Diamond sizes&quot; date: 2023-10-26 format: html draft: true --- (Note if you are reading in markdown that I used a code chunk to make sure this header text didn’t mess up our current document). YAML stands for Yet Another Markup Language and this header provides some data and variables or settings for how the document should be rendered, or knit. We could change the YAML header of this document to make an HTML webpage version of this document for example by just changing --- title: &quot;01-0 Introduction to Posit, Rstudio, &amp; Markdown&quot; author: &quot;Clay Wright&quot; date: &quot;2025-04-07&quot; output: pdf_document --- to --- title: &quot;01-0 Introduction to Posit, Rstudio, &amp; Markdown&quot; author: &quot;Clay Wright&quot; date: &quot;2025-04-07&quot; output: html_document --- We could also make this into a docx or slides, but slides work a little differently. I made the slides on the first day of class in Quarto, you can check them out in that project. In general this works quite well. But the YAML header is often a source of hard to track down errors. These errors can often be solved by replacing the header with the header from an RStudio template document. 1.3.13 Code chunk/block settings and global settings Code chunk or block settings, which set how a code block will be run or formatted is one area in which R Markdown and Quarto differ. (from https://www.stephaniehicks.com/jhustatprogramming2023/posts/2023-10-26-build-website/) 1.3.13.1 R Markdown vs Quarto Some high-level differences include Standardized YAML across formats Decoupled from RStudio More consistent presentation across formats Tab Panels Code Highlighting Another noticeable difference are settings/options for code blocks. Rather than being in the header of the code block, options are moved to within the code block using the #| (hash-pipe) for each line. This is a code block for R Markdown: ```{r setup, include=FALSE} library(tidyverse) library(tidytext) ``` (Note if you are reading in markdown that the text formatting is broken from here on. This is a known issue in RStudio https://community.rstudio.com/t/continued-issues-with-new-verbatim-in-rstudio/139737, and is possibly a reason to switch to Quarto). This is a code block for Quarto: ```{{r}} #| label: &quot;setup&quot; #| include: false library(tidyverse) library(tidytext) ``` 1.3.14 Output Options There are a wide variety of output options available for customizing output from executed code. All of these options can be specified either globally (in the document front-matter) or per code-block For example, here’s a modification of a Python-containing markdown example to specify that we don’t want to “echo” the code into the output document: --- title: &quot;My Document&quot; execute: echo: false jupyter: python3 --- Note that we can override this option on a per code-block basis. For example: ```{python} #| echo: true import matplotlib.pyplot as plt plt.plot([1,2,3,4]) plt.show() ``` Code block options available for customizing output include: Option Description eval Evaluate the code chunk (if false, just echos the code into the output). echo Include the source code in output output Include the results of executing the code in the output (true, false, or asis to indicate that the output is raw markdown and should not have any of Quarto’s standard enclosing markdown). warning Include warnings in the output. error Include errors in the output (note that this implies that errors executing code will not halt processing of the document). include Catch all for preventing any output (code or results) from being included (e.g. include: false suppresses all output from the code block). Here’s a example with r code blocks and some of these additional options included: --- title: &quot;Knitr Document&quot; execute: echo: false --- ```{{r}} #| warning: false library(ggplot2) ggplot(airquality, aes(Temp, Ozone)) + geom_point() + geom_smooth(method = &quot;loess&quot;, se = FALSE) ``` ```{{r}} summary(airquality) ``` When using the Knitr engine, you can also use any of the available native options (e.g. collapse, tidy, comment, etc.) which help format your code chunks nicely in the final document. See the Knitr options documentation for additional details. You can include these native options in option comment blocks as shown above, or on the same line as the {r} as shown in the Knitr documentation. 1.3.15 Troubleshooting Knit/Render problems Knitting or Rendering markdown documents can lead to lots of tough to identify error messages. It is many of the questions we recieve. Especially right before assignments are due. Here is a great write-up of common markdown problems: https://rmd4sci.njtierney.com/common-problems-with-rmarkdown-and-some-solutions. Probably the most common issue we see is something (like your name in the author: line) in the YAML header that is not in quotes to indicate it is a string, or that there is some variable in your environment pane that is not actually created in your markdown document. To avoid this second problem in the first place, I try and do the following: Develop code in chunks and execute the chunks until they work, then move on. knit the document regularly to check for errors. Then, if there is an error: recreate the error in an interactive session: restart R run all chunks below find the chunk that did not work, fix until it does run all chunks below explore working directory issues remember that the RMarkdown directory is wherever the .Rmd file is in your Files pane "],["rstudio-and-markdown-examples.html", "Rstudio and Markdown examples 1.4 Navigate the Posit and RStudio interface with proficiency. 1.5 Articulate the process of using RStudio to develop markdown documents. 1.6 Practice using markdown to format reproducible documents", " Rstudio and Markdown examples We will look at a few examples and do some live demos here in class. Remember our goals are to: Navigate the Posit and RStudio interface with proficiency. Articulate the process of using RStudio to develop markdown documents. Practice using markdown to format reproducible documents 1.4 Navigate the Posit and RStudio interface with proficiency. 1.4.1 Making new documents 1.4.2 Navigating the file pane and file paths 1.4.3 Console pane 1.4.4 Environment pane 1.4.5 History pane 1.5 Articulate the process of using RStudio to develop markdown documents. 1.5.1 Cheat Sheets! Yay! 1.5.2 Inserting code chunks 1.5.3 Changing code chunk settings 1.6 Practice using markdown to format reproducible documents The below from https://github.com/njtierney/rmd-errors/tree/master is an Rmd that contains many errors we will troubleshoot. 1.6.1 Tasks Get this rmarkdown document to compile hint: knit the document and look at the line for the error if there is an error: recreate the session in an interactive session: restart R run all chunks below (top section Run&gt;arrow&gt;run all chunks below) find the chunk that did not work, fix that chunk run all chunks below (top section Run&gt;arrow&gt;run all chunks below) explore working directory issues where is the packages.bib file? remember that the rmarkdown directory is where the .Rmd file lives ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE, dev = &quot;png&quot;}) ``` ```{r library, echo = } library(tidyverses) ``` # Introduction This is a simple analysis of the New York Air Quality Measurements using the R statistical programming language [@Rcore]. As stated in the helpfile `?airquality`, this dataset contains: &gt; Daily air quality measurements in New York, May to September 1973. And the dataset is sourced from: &gt; ... the New York State Department of Conservation (ozone data) and the National Weather Service (meteorological data). It contains the following variables - Ozone: Mean ozone in parts per billion from 1300 to 1500 hours at Roosevelt Island. - Solar.R: Solar radiation in Langleys in the frequency band 4000–7700 Angstroms from 0800 to 1200 hours at Central Park. - Wind: Average wind speed in miles per hour at 0700 and 1000 hours at LaGuardia Airport. - Temp: Maximum daily temperature in degrees Fahrenheit at La Guardia Airport. - Month: Month (1--12) - Day: Day of month (1--31) We are going to explore the relationship between solar radiation and other selected variables, solar radiation, wind, and temperature. # Method First, we tidy the names of the dataset, to provide information about the units of measurement for Ozone, Solar Radiation, Wind, and Temperature. We do this by renaming the variables and adding a suffix at the end to describe the units. To do this we use the `rename` function from the `dplyr` package[@dplyr]. ```{r tidy-data} tidy_aq &lt;-rename(.data = airquality, ozone_ppb = Ozone, solar_rad_lang = Solar.R, wind_mph = Wind, temp_fah = Temp, month = Month, day = Day) ``` We can see that there is an interesting relationship between ozone and solar radiation in figure 1 below, plotted using ggplot2 [@ggplot2] ```{r figure-1, fig.height = 4, fig.width=4} ggplot(tidy_aq, aes(x = ozone_ppb, y = solar_rad_lang)) + geom_point() ``` We can also see that there is an interesting relationship between Ozone and temperature. ```{r figure-2} ggplot(tidy_aq, aes(x = ozone_ppb, y = temp_fah)) + geom_point() ``` To explore the relationships between Ozone and all of the variables in the dataset, we can fit a basic linear model, with Ozone as the outcome, and all other variables as the predictors. We can express this as: $$ Ozone \\sim \\beta_0 + \\beta_1Solar.R + \\beta_2 Wind + \\beta_3Temp + \\epsilon $$ And we can fit this model using the code below. ```{r data-model} lm_aq &lt;- lm(ozone_ppb ~ solar_rad_lang + wind_mph + temp_fah, data = tidy_aq) ``` # Results The key results are given below, using the `tidy` function from the `broom` package [@broom] to clean up the data. ```{r broom-tidy} library(broom) tidy_lm_aq &lt;- tidy(lm_aq) tidy_lm_aq ``` We can present this result in a nice table using the `kable` function from the knitr package [@knitr] ```{r lm-kable} knitr::kable(tidy_lm_aq_broom, digits = 3, caption = &quot;Table of results from the linear model&quot;) ``` We can also refer to individual results of the model inside the text. For example, we can say that the estimated coefficient of Wind miles per hour is `r tidy_lm_aq$estimate[3]`, and the P value of this is `r tidy_lm_aq$p.value[3]`. # Conclusion We have explored the relationship of Ozone with other variables in the airquality dataset # References "],["the-r-language-and-tidy-data.html", "2 The R Language and Tidy Data 2.1 Reading and Pre-class Materials 2.2 Tidy data 2.3 R Basics 2.4 Functions 2.5 Objects - Variables and Functions 2.6 Tidyverse tables", " 2 The R Language and Tidy Data This week is devoted to basic tutorials on R programming and using Tidy Data, building on our intro to Rstudio/Posit and Markdown last week. Just like learning a foreign language, to learn programming and get comfortable, you’ll need to practice and immerse yourself! If you attempt to plow through in one sitting, unless you’re a genius, you’re unlikely to retain much. My suggestion is to start working early - start with either watching the assigned video or the reading. I’ve noted that you can choose one or the other; for extra reinforcement, read and watch (the video is only 15 minutes)! Then go ahead and get started with the in-class activities. There are so many excellent resources to augment what we’re working together on learning. If you want to dive deeper, check out this freely available online resource - R for Data Science. Lastly, I hope you all take time to reflect on this week, make adjustments, think about your time management, and lastly make some time to relax and unwind. With the skills you learned last week it would only take a little more learning to make your own personal github-hosted webpage. You can see mine here: &lt;wrightrc.github.io&gt; 2.0.1 Data Organization and manipulation in Spreadsheets Spreadsheets are commonly employed software applications for entering, storing, analyzing, and visualizing data. Concentrating on the data entry and storage components, this article provides practical suggestions on structuring spreadsheet data to minimize errors and facilitate subsequent analyses. 2.1 Reading and Pre-class Materials Reading - choose 1 based on your learning style: Read section 12.2 Tidy data of R for Data Science: https://r4ds.had.co.nz/tidy-data.html#tidy-data-1 Watch a video on Tidy data - The one from Posit, is pretty good, but in the quick check I did all of the first page for my search above were OK. Whichever reading or video you choose, I’d recommend opening up Posit/RStudio and following along with the reading material/video. For most of the content, you can duplicate the exercises in Rstudio. For example, once you create a student account, you’ll be able to create your own project (RUwithMe covers this aspect) Work through the Tidy Data and R Basics intro materials in this document below which are based in part on: Posit Cloud R Basics Recipes. Have a great weekend! Post any questions in Teams. 2.2 Tidy data Tidy data is pretty important for coding reproducible data analyses, because when data is tidy your code can operate on it in a consistent way. 2.2.1 Top 12 Tips for Data Organization (from Broman and Woo, 2018). Organize data as a series of rectangles in separate sheets. 1 row for column IDs., remaining rows the observations. One piece of data per cell. Imagine you are running an instrument with the racks to hold your samples. Each rack (A,B,C) has 10 sample slots. For the label, you could use A-1, A-2,… or have 2 columns: column 1 containing the rack ID, column 2 containing the sample slot ID. Using the latter (2-columns) is preferred. Why? In subsequent data analysis, this allows you to easily sort based on rackID (imagine there was a problem when the instrument got to rack B). Add notes in separate columns Add units in column name or separate column/data dictionary/annotation. Be consistent Use consistent “codes” for categorical variables. For example, imagine if you have a list of chemicals with a label for flammable or corrosive. As you enter data, sometimes you write out flammable, but other times just flam. What should you do? Choose one and stick with it! (Once we get into R there are some special jargon: “codes” are “levels” and “categorical variables” are “factors”). Use consistent code for missing values. Fill in every cell even if the value is unknown. While some datasets use a default number (e.g. -9999) to signify no data, it’s preferred to use NA. Never put a note in the data column; rather have a separate column just for notes. Use consistent variable names. When you have multiple spreadsheets representing the same type of data (e.g. flow record from different sites; experimental data from different days), keep your variable names the same and avoid spaces (e.g. “nitrate_mgperml” is preferred over “nitrate mg per ml) Keep layout consistent between sheets. One approach here is to set up a template, and then work from there. Use consistent file names. For example, Q_James_2015.csv and Potamac_Q_2015.csv are not consistent making it more difficult to sort, read into software, and view. Use consistent date formats. This is a big one! I prefer using yyyymmdd, or yyyy-mm-dd. If you sometimes show 01/22/2021 and other times 20210122, it’ll make it difficult to perform data analysis! Use consistent notes in your note column. Choose good names No spaces, choose with an underscore or hyphen, avoid special characters. Using CamelCase (e.g. ExampleFileName). Include units if possible, avoid abbreviations (but not at the expense of too long) See the table below from datacarpentry.org that illustrates this concept: Write dates as YYYY-MM-DD. This is a global standard (ISO 8601), and using this consistently is good practice. Unfortunately, Excel is not consistent with how dates are treated across platforms (e.g. mac vs PC). Furthermore, Excel will almost always try to convert the date into a number based on an arbitrary initial date (e.g. 1904). Here are approaches to get around this: Use plain text format for columns containing text: select column → in the menu bar, select format cells → choose “text” Add dates as YYYYMMDD (e.g. 20210122) Add dates in between apostrophes (‘2021-01-22’). This approach saves as text and not numeric. No empty cells - use “NA” when you do not have data for a given cell in a spreadsheet (for a column or row that is otherwise full of data) Are the following spreadsheets following good practice? Why or why not? Figures A &amp; B - you’d need to rearrange and figure out what is going on Figure C - data is already processed to some degree, and you don’t have access to raw data Figure D - Incomplete table, either fill out completely or break into 2 tables. See figure 6 in reading. Use a data dictionary. Keep data in a data file, use a complimentary README file (or a metadata/annotation sheet) that contains ‘metadata’. This should include explanation of the variable, units, expected values (minimum and maximum). It may also include information about how the data was collected and how the experiment was performed. There is an example README from the Vermont Covid dashboard in the images folder, “images/VT_COVID-19_Cases_by_County_Time_Series_modify_README.rtf”. This could be a txt file, rtf, html, csv or any other file type. Do not perform calculations in raw data files. Use a separate file for visualizations, calculations, analysis, etc. Do not use color or highlighting in data files. Rather, use a column that provides a note instead of highlighting cells. (This is because the color or highlighting is data! Every datum gets its own cell - #2.) Backup data. Use version control—if anything is added, updated, etc save a new version of the file. Validate data. Excel, Data → Validation (see icon at right), or Data → Data Validation in Google Sheets. Then you can set up logical rules to check that your columns of data contain valid values (for example a column of percent values should be between 0 and 100). Any values that don’t pass your logical test will be highlighted. To see the rules you have created follow these instructions. In R you can write code to validate data. More on R code below! Save data in plain text files. Save as -&gt; Format -&gt; csv. Why? Simplified archive, preservation of data for future scientists and engineers. When data is saved in a proprietary format, sharing across versions/platforms and the future may be hampered. Also check out the useful list of shortcut keys in Excel, or shortcut keys in Google Sheets. 2.3 R Basics While Excel certainly has its uses, it can often do more harm than good. And after all this course is, in part, about R. So here we are going to jump in to using R to analyze data. 2.3.1 Basic Math to Linear Algebra 2.3.1.1 Arithmetic with single numbers You want to use R to do simple arithmetic, as if R were a calculator. Step 1 - Write the mathematical expression you wish to evaluate. R recognizes the following operators: Operator Operation Example + Addition 1 + 2 - Subtraction 2 - 1 * Multiplication 2 * 3 / Division 4 / 2 ^ Exponentiation 4 ^ 2, i.e. \\(4^2\\) %% Modulo 5 %% 3, i.e what is the remainder when you divide five by three? %/% Integer division 5 %/% 3, i.e how many times does three go into five? Step 2 - Run the expression. R will return the result. 2.3.1.1.1 Example Suppose we want to divide the difference between 111 and 75 by 3. Using the operators above, we can write 111 - 75 / 3, but R will evaluate this as 111 minus 75 / 3 111 - 75 / 3 ## [1] 86 R isn’t doing anything funny, R is just following the standard order of mathematical operations. To correct the order of operations, we add parentheses. (111 - 75) / 3 ## [1] 12 Remember PEMDAS PEMDAS is an acronym that describes the order of operations in arithmetic. We first evaluate expressions that are grouped together by Parentheses. Next, we evaluate Exponentiation, followed by Multiplication or Division. Finally, we evaluate Addition and Subtraction. Use ( and ) when you need to control the order of mathematical operations in R. 2.4 Functions 2.4.1 What is a function? a way to reuse a chunk of code easily without having to copy and paste performs simple to complex operations often requires inputs \\[known as the arguments\\], some of which are optional used when a task is performed over and over again An example function is abs(x) where abs is the function name and x is the input argument. This function computes the absolute value of the input argument. You can learn more about this function by typing ?abs into the console. abs(-19) ## [1] 19 2.4.1.1 Call a function on a number R provides thousands of functions to use. Functions are prepackaged pieces of code that perform useful tasks. To use a function in R: Step 1 - Write the name of the function. Do not surround it in quotes. The name tells R which function you would like to run. Step 2 - Place a pair of parentheses after the name. Parentheses are the trigger that runs the function. Step 3 - If the function needs a piece of input to do its job, place the input inside of the parentheses. Your function call will now look something like this, factorial(4). Step 4 - Click Run. Or press Enter if you are using R from a command line. R will run your function and return the result. 2.4.1.2 Example Suppose we’d like to find the square root of 1764. We can do this with a function named sqrt, which is short for square root. To call sqrt, we first write its name. Notice that if we run just the name of sqrt, R shows us the code associated with sqrt. That’s not what we want. sqrt ## function (x) .Primitive(&quot;sqrt&quot;) Next, we place a pair of parentheses after sqrt. When we click Run, the parentheses will cause R to execute our function. Our function call is not yet complete because sqrt() requires a piece of input to do its job. Which number should square root take the square of? If we run our command in this unfinished state, R will return an error message. sqrt() ## Error in sqrt(): 0 arguments passed to &#39;sqrt&#39; which requires 1 We finish our command by giving sqrt() a number to take the square of, in this case, 1764. sqrt(1764) ## [1] 42 In markdown we will use a special convention whenever we mention a function: we will write the function’s name in code font and follow it with a pair of parentheses, like this sqrt(). R users have several names for the code that runs a function. You may see it referred to as: A command A function call An expression Or some variation of the above. They all mean the same thing: a bit of code to run. Watch our for this common mistake. it is easy to create an error by omitting the closing parenthesis of a function call: sqrt(1764 ## Error in parse(text = input): &lt;text&gt;:2:0: unexpected end of input ## 1: sqrt(1764 ## ^ 2.5 Objects - Variables and Functions R is an object-oriented programming language. This means we can use R to create abstracted objects that contain data (of any type, shape or size) called variables or procedures/methods (individual blocks of code) called functions. There are numerous functions and datasets included in the base R installation. Also, as an open source language countless programmers in the R community have written useful functions and created useful datasets that are freely available in the form of R-packages (more on these later). You can also write your own! 2.5.1 Obey R’s naming rules You want to give a valid name to an object in R. Step 1 - Choose a name that includes valid characters. Names in R may be made up of three types of characters: Capital and lowercase letters Numbers The symbols . and _ Other characters are forbidden because we use them with names to perform actions. R wouldn’t be able to distinguish the name x+y from the command x + y written without spaces. Step 2 - Double-check that the first character is a letter or .. Names in R may not start with a number or _, even though these symbols can appear anywhere else in the name. Step 3 - Assign an object to the name. Use the assignment arrow (&lt;-) with the name on the left and the object on the right. 2.5.1.1 Example Suppose we’d like to create an object storing the number of trials in an experiment. In our first attempt, we give the object the name #_of_trials. #_of_trials &lt;- 15 #_of_trials This doesn’t give us any output, because # is not a valid character for an object name. Let’s try again, using only valid characters: number_of_trials &lt;- 15 number_of_trials ## [1] 15 That worked! Now let’s record the observed value of our first trial: 1st_trial &lt;- 476.2 1st_trial ## Error in parse(text = input): &lt;text&gt;:1:2: unexpected symbol ## 1: 1st_trial ## ^ That didn’t work either. Even though all the characters were valid, object names canonly start with a letter or ., not a number or _. Let’s try again. first_trial &lt;- 476.2 first_trial ## [1] 476.2 Success! Object names can include capital and lowercase letters in any order, but names in R are case sensitive. Look what happens if you try to get the value of an object with the wrong capitalization: my_value &lt;- 100 My_Value ## Error: object &#39;My_Value&#39; not found Some names follow all of R’s rules, but can’t be used because they are reserved for a special purpose. Look what happens if you try to assign an object to the name TRUE. TRUE &lt;- 100 ## Error in TRUE &lt;- 100: invalid (do_set) left-hand side to assignment Names like TRUE, FALSE, NA, and function can’t be assigned to objects, because they already have important uses in R. 2.5.2 Variables and data types You can create objects (variables~values, large data structures~think spreadsheets and databases, and functions) using the =, &lt;- or -&gt; operators. You can see what type of data (or data type) a variable is using the class function. Go ahead, try class(x). Data in R can be of several different, basic types: Data Type aka Example Logical Boolean TRUE, FALSE Numeric float 42, 3.14, Character string ‘a’ , “good”, “TRUE”, ‘23.4’ Integer 2L, 34L, 0L Complex 3 + 2i Raw hexadecimal “Hello” is stored as 48 65 6c 6c 6f 2.5.3 Vectors Vectors in R are simply ordered lists of values. These values can be of any type (strings, numerics, Boolean, etc), but they must all be of the same type, or R will force them to be the same. We can construct vectors using the c() function. Let’s run through a quick example: col_names &lt;- c(&#39;plant&#39;, &#39;genotype&#39;) col_names ## [1] &quot;plant&quot; &quot;genotype&quot; #####???Question??? What is c abbreviating? (i.e. what is the title of the c() function?) answer here What are the arguments that you can pass to c()? answer here Now we have a vector of strings. We can access the individual elements (the values we put in our vector) using the square bracket operator. col_names[1] ## [1] &quot;plant&quot; col_names[2] ## [1] &quot;genotype&quot; #Note that the indices begin at 1 in R!!! col_names[0] ## character(0) We can also change elements or add elements to the vector using the bracket operator. col_names[2] &lt;- &#39;phenotype&#39; col_names[3] &lt;- &#39;root_length&#39; col_names ## [1] &quot;plant&quot; &quot;phenotype&quot; &quot;root_length&quot; col_names[4] &lt;- FALSE col_names ## [1] &quot;plant&quot; &quot;phenotype&quot; &quot;root_length&quot; &quot;FALSE&quot; #####???Question??? What happened to FALSE (is it a Boolean)? answer here Write a block of code to test what would happen if we instead added a character string to a vector of logical values (i.e. make a new variable containing a few Boolean values, then add a string to that vector)! What happens? answer here We can also do mathematical or logical operations on entire vectors. col_names == FALSE ## [1] FALSE FALSE FALSE TRUE vector &lt;- c(1, 2, 3, 4, 5) 6*vector ## [1] 6 12 18 24 30 vector^2 ## [1] 1 4 9 16 25 vector &gt; 2 ## [1] FALSE FALSE TRUE TRUE TRUE 2.5.4 Matrices, Arrays and Lists Matrices are two dimensional data sets and Arrays are N-dimensional data sets. Like vectors these must be made of a single data type. For more info ?matrix and ?array. Lists are more complex data structures that are similar to vectors but allow multiple data types. Lists can contain vectors as elements and even other lists! This makes them potentially N-dimensional but clunky to work with. You might encounter them if you use R in the future. For more info ?list. 2.5.5 Data frames Variables in R are not limited to just strings or integers or even matrices. You can store and operate on entire spreadsheets with columns of defined data types, using what R calls ‘data frames’. Data frames have columns that are made of vectors. The data frame is one of the most fundamental data structures used in R. ?data.frame provides a wealth of knowledge about data frames, but let’s just go ahead and make one! Run the following code. L3 &lt;- LETTERS[1:3] fac &lt;- sample(L3, 10, replace = TRUE) d &lt;- data.frame(x = 1, y = 1:10, fac = fac) #notice how the columns of the data frame can be named using &#39;=&#39;, just as if we were creating individual vectors d ## x y fac ## 1 1 1 A ## 2 1 2 B ## 3 1 3 A ## 4 1 4 C ## 5 1 5 B ## 6 1 6 C ## 7 1 7 A ## 8 1 8 C ## 9 1 9 C ## 10 1 10 B class(d) ## [1] &quot;data.frame&quot; #####???Questions??? What is LETTERS? What is L3? answer here What does sample do? answer here What does setting the replace argument of sample to TRUE do? Try sample(L3, 10, replace = FALSE) Now we have a data frame d with 10 rows and 3 columns. You can retrieve individual columns using the $ operator. Try it, d$fac!. Wait a minute, why is this no longer a column? The columns of a data frame are actually just vectors. #####???Question??? What class of data is d$fac? answer here You can also create new columns using the $ operator. For example we could make a column called new_column that contains \"new_column\" by executing d$new_column &lt;- \"new_column\". 2.5.5.1 Factors Factors used to be an efficient way of storing large vectors of repetitive discrete or categorical data. Factors do this by translating the potentially long individual pieces of data into integers, using a table called levels. Try levels(d$fac). This gives us a list of all the unique possible values in d$fac. R creates a key (1 = A, 2 = B, 3 = C) to read and write this factor. In this way long level values, like sentences, or large datasets, like thousands of lines, are compressed. To see the compressed version of d$fac we can use as.integer(d$fac). R now stores large data structures by indexing values like this regardless of whether it’s a factor or not. Despite this fact there are still some useful features of factors. For example, if we are adding a dataset from a new replicate of an experiment to an existing dataset, columns that are factors will only allow us to add values that match our existing levels. This will often help you find typos in your dataset. Additionally, some functions require factor variables, like the ANOVA functions we will use later. &gt; &gt;Giving ?factor a look, you will see that we can also assign a particular order to the levels of a factor. This can be handy for ordering variables when plotting. We can also assign labels to the levels, just in case your level names are too abstracted to be understandable. &gt; &gt;However when manipulating data frames containing factors you must be careful because some functions may interpret factors as their integer values! We could also avoid creating a factor in our data frame and just keep this column as characters by including stringsAsFactors = F in our call to data.frame(). Going back to our data frame d, similar to vectors we can access rows, columns and elements of the data frame using the square bracket operator. I’ll suppress the output below and let you run these examples yourself. #get the first row of d d[1,] #get the first column of d d[,1] #get the column named &#39;fac&#39; d[,&#39;fac&#39;] #or d[[&#39;fac&#39;]] #or (most efficient and readable) d$fac #get the element in the 5th row and 3rd column d[5,3] We can also perform calculations or other operations on the elements of a data frame. d[,2] + 1 d[[2]] + 1 d[,2] * 2 #similarly for logical operations, note that logical &#39;is equivalent to&#39; is &#39;==&#39; d[,3] == &#39;B&#39; d$y &lt;= 5 #we can also use functions to perform complex calculations mean(d$y) median(d$y) sum(d$y) Just like with vectors we can change elements or add elements to a data frame. #####???Question??? How would you add a column to d with the integer values representing d$fac? answer here What is the mean of your new column of d? Copy the code you used. answer here What is the median or your new column of d? Copy the code you used. answer here What is the sum of your new column? Copy the code you used. answer here What fraction of the sum of your new column is each row’s value? Make a new column for d showing this fraction. Copy the code you used. answer here 2.5.6 Run a function You want to run a function on a single value, for instance, to perform a calculation. Step 1 - Write the name of the function. Each function in R is saved as an object. To use an object, we call its name. Step 2 - Type a pair of parentheses ( ) after the name. Parentheses tell R that you would like to run the function that is stored in the object. Step 3 - Place any input inside of the parentheses. Most functions need a value to do their job, like abs(-4) or round(pi). Step 4 - Click Run. Or press Enter if you are using R from a command line. R will run your function and return the result. 2.5.6.1 Example Suppose we want to apply a trigonometric function, like sine, cosine, or tangent, to an angle (in radians). In R, the functions are sin(), cos(), and tan(). To call cosine, we first write its name cos. Next, we place a pair of parentheses after the name. Our call to the cos() function is not yet complete, since cosine needs an angle to do its computation. If we run our command in this unfinished state, R will return an error message. cos() ## Error in cos(): 0 arguments passed to &#39;cos&#39; which requires 1 We can complete the command in one of two ways. We can give cos() a specific angle to act on, in this case, 0. cos(0) ## [1] 1 Or we could give cos() an object that contains an angle to act on, such as pi. cos(pi) ## [1] -1 Some of the most common mathematical functions in R are: Function Operation Example sin() Sine sin(pi/2) cos() Cosine cos(3*pi/2) tan() Tangent tan(pi/4) sign() Sign sign(-2), i.e. is -2 positive or negative? log() Natural logarithm log(2.718282) exp() Exponential exp(1) floor() Integer floor floor(pi), i.e. what is the largest integer less than pi? ceiling() Integer ceiling ceiling(pi), i.e. what is the smallest integer greater than pi? 2.6 Tidyverse tables The programmer who wrote the R for Data Science book, along with his research group and now company Posit, have written and maintain several packages of R code functions to deal with data from reading and writing to tidying and wrangling. These packages are called the Tidyverse. You can install all of these packages at once using install.packages(&quot;tidyverse&quot;) We will use some of the functions from these packages below to deal with tables of data. 2.6.1 Create a Table Manually You want to create a data frame by typing in each value by hand. This is an alternative to reading a file that contains the data. Step 1 - Call tibble::tibble(). tibble() constructs a tibble, a type of data frame. Step 2 - Choose a column name for your tibble. Pass the name as an argument name to tibble(), e.g. tibble(col_1) Step 3 - Provide a vector of values for the column. Assign them to the column name, e.g. tibble(col_1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) Step 4 - Repeat for every column in your tibble. Every column should have the same number of values. If you pass a column a single value, tibble() will repeat that value for each row of the data frame. Remember to separate each new argument/column name with a comma. Step 5 - Save the tibble to an object, so you can access it later. data &lt;- tibble( col_1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), col_2 = 1:3 ) 2.6.1.1 Example We want to create a data frame to keep track of: The teachers at Grove Middle School The number of students in their classrooms The grade levels of the students We begin by loading the tibble package which contains tibble(). Next we input our data into tibble(). Our tibble will have the column names teacher, class_size, and grade with the values provided below. library(tibble) tibble(teacher = c(&quot;Gaines&quot;, &quot;Johnson&quot;, &quot;Hernandez&quot;), class_size = c(30, 26, 28), grade = c(6, 7, 8)) ## # A tibble: 3 × 3 ## teacher class_size grade ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gaines 30 6 ## 2 Johnson 26 7 ## 3 Hernandez 28 8 After checking out the created tibble, we realize it would be a good idea to include the school name in the data frame in case we decide to include other schools in the future. If we pass only a single value to this column, R will use its recycling rules to reuse that value for each row in the tibble. tibble( teacher = c(&quot;Gaines&quot;, &quot;Johnson&quot;, &quot;Hernandez&quot;), class_size = c(30, 26, 28), grade = c(6, 7, 8), school = &quot;Grove MS&quot;) ## # A tibble: 3 × 4 ## teacher class_size grade school ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Gaines 30 6 Grove MS ## 2 Johnson 26 7 Grove MS ## 3 Hernandez 28 8 Grove MS Lastly, we assign the created tibble to the object, teachers, so we can access is later. teachers &lt;- tibble( teacher = c(&quot;Gaines&quot;, &quot;Johnson&quot;, &quot;Hernandez&quot;), class_size = c(30, 26, 28), grade = c(6, 7, 8), school = &quot;Grove MS&quot;) 2.6.2 tribble() You can also define your tibble row by row with tibble:tribble(). Place a ~ before each value in the first row to indicate that the values are column names. If you watch your spaces, tribble() will provide a low-fi preview of your table as you write it. tribble( ~teacher, ~class_size, ~grade, ~school, &quot;Gaines&quot;, 30, 6, &quot;Grove MS&quot;, &quot;Johnson&quot;, 26, 7, &quot;Grove MS&quot;, &quot;Hernandez&quot;, 28, 8, &quot;Grove MS&quot;) ## # A tibble: 3 × 4 ## teacher class_size grade school ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Gaines 30 6 Grove MS ## 2 Johnson 26 7 Grove MS ## 3 Hernandez 28 8 Grove MS Other ways to read in raw data Any of the readr::read_* functions, such as readr::read_csv() or readr::read_delim(), can be used to create a table manually as a character string wrapped in I(). readr::read_csv(I(&quot;col_1,col_2\\na,1\\nb,2\\nc,3&quot;)) ## Rows: 3 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): col_1 ## dbl (1): col_2 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 3 × 2 ## col_1 col_2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 2 ## 3 c 3 2.6.3 Read a CSV file (.csv) You want to read a CSV file into your R session, where you can manipulate its contents. The file has the extension .csv. A CSV file is a text file that contains a table whose values are separated by commas, i.e. a Comma Separated Values file. Step 1 - Call readr::read_csv(). read_csv() is designed to read in .csv files with , as the field separator. Step 2 - Give read_csv() the filepath to your file as a character string. R will read the filepath as if it begins at your working directory. For example: read_csv(&quot;my/file.csv&quot;) Step 3 - Save the output to an object, so you can access it later. csv_table &lt;- read_csv(&quot;my/file.csv&quot;) 2.6.3.1 Example We want to read in the drought records dataset which is a CSV file. We have this file saved on cloud at /data/Drought_paneldata.csv. We begin by loading the readr package which contains read_csv(). Next, we pass read_csv() the filepath for our CSV file in order to read in the dataset. The file is in the data folder. The output looks like this: library(readr) read_csv(&quot;data/Drought_paneldata.csv&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## date (3): Time_fornow_enddate, StartDate, EndDate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 137 × 12 ## CountyFP Time_fornow_enddate StartDate EndDate DurationDays Hazard ## &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10003 2016-09-10 2016-09-10 2016-09-10 1 Heat ## 2 10003 2016-08-20 2016-08-20 2016-08-20 1 Heat ## 3 10005 2014-06-19 2014-06-18 2014-06-19 2 Heat ## 4 10003 2013-07-20 2013-07-15 2013-07-20 6 Heat ## 5 10003 2011-06-09 2011-06-08 2011-06-09 2 Heat ## 6 10001 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 7 24011 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 8 24015 2010-06-24 2010-06-23 2010-06-24 2 Heat ## 9 10001 2006-08-03 2006-08-01 2006-08-03 3 Heat ## 10 10003 2006-08-03 2006-08-01 2006-08-03 3 Heat ## # ℹ 127 more rows ## # ℹ 6 more variables: CropDmg &lt;dbl&gt;, avg_tmax &lt;dbl&gt;, avg_tmin &lt;dbl&gt;, ## # avg_ppt &lt;dbl&gt;, unique_id &lt;chr&gt;, duplicate &lt;lgl&gt; Notice that read_csv() automatically chose intelligent data types for each of the columns. Lastly, we assign the dataset read in by read_csv() to an object, Drought_rec, so we can access it later. Duoght_rec &lt;- read_csv(&quot;data/Drought_paneldata.csv&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## date (3): Time_fornow_enddate, StartDate, EndDate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Duoght_rec ## # A tibble: 137 × 12 ## CountyFP Time_fornow_enddate StartDate EndDate DurationDays Hazard ## &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10003 2016-09-10 2016-09-10 2016-09-10 1 Heat ## 2 10003 2016-08-20 2016-08-20 2016-08-20 1 Heat ## 3 10005 2014-06-19 2014-06-18 2014-06-19 2 Heat ## 4 10003 2013-07-20 2013-07-15 2013-07-20 6 Heat ## 5 10003 2011-06-09 2011-06-08 2011-06-09 2 Heat ## 6 10001 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 7 24011 2011-06-09 2011-06-09 2011-06-09 1 Heat ## 8 24015 2010-06-24 2010-06-23 2010-06-24 2 Heat ## 9 10001 2006-08-03 2006-08-01 2006-08-03 3 Heat ## 10 10003 2006-08-03 2006-08-01 2006-08-03 3 Heat ## # ℹ 127 more rows ## # ℹ 6 more variables: CropDmg &lt;dbl&gt;, avg_tmax &lt;dbl&gt;, avg_tmin &lt;dbl&gt;, ## # avg_ppt &lt;dbl&gt;, unique_id &lt;chr&gt;, duplicate &lt;lgl&gt; read_csv() comes with many arguments that you can use to customize which parts of the file will be read in and how. Here are a few of the most useful: Argument Description col_names Should the first row be read in as column names? Defaults to TRUE. Can also be a character vector of column names. col_types Explicitly set the data type for each column. skip Number of rows to skip before reading any data. guess_max Maximum number of lines to read for guessing column types. Read the help page at ?read_csv to learn more. 2.6.4 Read a character-delimited file (.txt) You want to read a delimited data file into your R session, where you can manipulate its contents. The file typically has the file extension .txt. Within each row of a delimited file, the column fields are separated by a character delimiter. Some common choices for the delimiter are |, :, ;, ,, or ~. Step 1 - Call readr::read_delim(). Step 2 - Give read_delim() the filepath to your dataset as a character string. For example: read_delim(&quot;my/file.txt&quot;) Step 3 - Specify the delim argument to tell read_delim() the delimiter character. read_delim(&quot;my/file.txt&quot;, delim = &quot;,&quot;) Step 4 - Save the output to an object, so you can access it later. delim_table &lt;- read_delim(&quot;my/file.txt&quot;, delim = &quot;,&quot;) 2.6.4.1 Example We want to read in the drought records dataset in text format which is a delimited dataset .txt file and uses the delimiting character ,. This dataset contains drought/heat hazard records and associated weather characteristics for different dates and counties in Delmarva Peninsula. We have this file saved on cloud as data/Drought_paneldatatx.txt. We begin by loading the readr package which contains read_delim(). Then, we pass read_delim() the filepath for our .txt file in order to read in the dataset. The file is in the data folder in our working directory. The output looks like this: library(readr) read_delim(&quot;data/Drought_paneldatatx.txt&quot;, delim = &quot;,&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Time_fornow_enddate, StartDate, EndDate, Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 137 × 12 ## CountyFP Time_fornow_enddate StartDate EndDate DurationDays Hazard CropDmg ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 10003 9/10/2016 9/10/2016 9/10/2016 1 Heat 0 ## 2 10003 8/20/2016 8/20/2016 8/20/2016 1 Heat 0 ## 3 10005 6/19/2014 6/18/2014 6/19/2014 2 Heat 0 ## 4 10003 7/20/2013 7/15/2013 7/20/2013 6 Heat 0 ## 5 10003 6/9/2011 6/8/2011 6/9/2011 2 Heat 0 ## 6 10001 6/9/2011 6/9/2011 6/9/2011 1 Heat 0 ## 7 24011 6/9/2011 6/9/2011 6/9/2011 1 Heat 0 ## 8 24015 6/24/2010 6/23/2010 6/24/2010 2 Heat 0 ## 9 10001 8/3/2006 8/1/2006 8/3/2006 3 Heat 0 ## 10 10003 8/3/2006 8/1/2006 8/3/2006 3 Heat 0 ## # ℹ 127 more rows ## # ℹ 5 more variables: avg_tmax &lt;dbl&gt;, avg_tmin &lt;dbl&gt;, avg_ppt &lt;dbl&gt;, ## # unique_id &lt;chr&gt;, duplicate &lt;lgl&gt; Notice that read_delim() automatically chose intelligent data types for each of the columns. Lastly, we assign the solar dataset read in by read_delim() to an object, solar, so we can access it later. data &lt;- read_delim(&quot;data/Drought_paneldatatx.txt&quot;, delim = &quot;,&quot;) ## Rows: 137 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Time_fornow_enddate, StartDate, EndDate, Hazard, unique_id ## dbl (6): CountyFP, DurationDays, CropDmg, avg_tmax, avg_tmin, avg_ppt ## lgl (1): duplicate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. read_delim() is a more general case of readr::read_csv() and readr::read_tsv() read_csv() is the equivalent of calling read_delim() with delim = \",\". read_tsv() is the equivalent of read_delim() with delim - \"\\t\". 2.6.5 Read an Excel file (.xls, .xlsx) You want to read a Microsoft Excel file into your R session, where you can manipulate its contents. The file has the extension .xls or .xlsx. Step 1 - Call readxl::read_excel(). read_excel() is designed to read in .xls or .xlsx files. Step 2 - Give read_excel() the filepath to your file as a character string. For example: read_excel(&quot;my/file.xlsx&quot;) Step 3 - Specify the sheet you want to read in with the sheet argument. By default, read_excel() reads in the first sheet of an Excel file. You can set sheet to the name of a different sheet (as a character string) or the location of a different sheet (as a number). read_excel(&quot;my/file.xlsx&quot;, sheet = &quot;Sheet_B&quot;) Step 4 - Save the output to an object, so you can access it later. xl_table &lt;- read_excel(&quot;my/file.xlsx&quot;, sheet = &quot;Sheet_B&quot;) 2.6.5.1 Example We want to read in an hazards dataset which is a Excel file containing financial agricultural loss records and associated weather conditions for three types of climatic hazards. Hazard column defines the hazard type (drought, storm, heat). We have saved the dataset on cloud at /data/hazards.xlsx. We begin by loading the readxl package which contains read_excel(). Next, we pass read_excel() the filepath for our Excel file in order to read in the dataset. The file is in our working directory, therefore we do not need the entire filepath. We only need the portion that goes from our working directory to the file. The output looks like this: library(readxl) read_excel(&quot;data/hazards.xlsx&quot;) ## # A tibble: 1,321 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1989-07-01 00:00:00 24045 7.93 3.75 3.29 ## 2 1990-06-01 00:00:00 24045 1.34 0.823 3.83 ## 3 1990-07-01 00:00:00 10003 4.29 1.31 1.90 ## 4 1990-07-01 00:00:00 24011 4.96 0.902 1.90 ## 5 1990-07-01 00:00:00 24019 4.24 0.919 1.65 ## 6 1990-07-01 00:00:00 24045 4.15 0.969 1.52 ## 7 1990-08-01 00:00:00 24015 7.53 1.49 1.65 ## 8 1990-08-01 00:00:00 10001 5.71 0.811 1.10 ## 9 1990-09-01 00:00:00 24045 1.82 0.705 3.03 ## 10 1990-10-01 00:00:00 24045 2.30 0.927 3.71 ## # ℹ 1,311 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; Looking at the “Hazard” column of the data frame, we realize read_excel() automatically read in the first sheet of the file. We are looking for data on storm hazard events, not drought. However, we cannot remember what the sheet was named. We can call readxl::excel_sheets() on the filepath to see the names of the sheets in the “hazards” file. excel_sheets(path = &quot;data/hazards.xlsx&quot;) ## [1] &quot;drought&quot; &quot;storm&quot; &quot;heat&quot; Now we can see that we wanted to read in Sheet 2 from the file. We can do this by specifying the sheet argument in read_excel(). read_excel(path = &quot;data/hazards.xlsx&quot;, sheet = 2) ## # A tibble: 1,609 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981-06-01 00:00:00 24015 4.21 0.826 1.23 ## 2 1982-05-01 00:00:00 24015 3.67 2.02 4.06 ## 3 1983-04-01 00:00:00 24019 7.03 2.21 2.33 ## 4 1983-07-01 00:00:00 24047 1.94 0.915 5.32 ## 5 1983-08-01 00:00:00 24015 2.00 0.992 6 ## 6 1984-03-01 00:00:00 51001 6.85 1.77 1.87 ## 7 1984-05-01 00:00:00 10005 7.64 2.60 3.81 ## 8 1985-09-01 00:00:00 51001 5.73 4.65 4.53 ## 9 1988-05-01 00:00:00 10003 6.03 2.12 1.68 ## 10 1989-03-01 00:00:00 10003 4.57 1.06 3.23 ## # ℹ 1,599 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; Lastly, we assign the earthquake data frame read in by read_excel() to an object so we can access it later. storm &lt;- read_excel(path = &quot;data/hazards.xlsx&quot;, sheet = 2) storm ## # A tibble: 1,609 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981-06-01 00:00:00 24015 4.21 0.826 1.23 ## 2 1982-05-01 00:00:00 24015 3.67 2.02 4.06 ## 3 1983-04-01 00:00:00 24019 7.03 2.21 2.33 ## 4 1983-07-01 00:00:00 24047 1.94 0.915 5.32 ## 5 1983-08-01 00:00:00 24015 2.00 0.992 6 ## 6 1984-03-01 00:00:00 51001 6.85 1.77 1.87 ## 7 1984-05-01 00:00:00 10005 7.64 2.60 3.81 ## 8 1985-09-01 00:00:00 51001 5.73 4.65 4.53 ## 9 1988-05-01 00:00:00 10003 6.03 2.12 1.68 ## 10 1989-03-01 00:00:00 10003 4.57 1.06 3.23 ## # ℹ 1,599 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; read_excel() comes with many arguments that you can use to customize which parts of the spreadsheet will be read in and how. Here are a few of the most useful: Argument Description col_names Should the first row be read in as column names? Defaults to TRUE. Can also be a character vector of column names. col_types Explicitly set the data type for each column. skip Number of rows to skip before reading any data. range Specify a subset of cells to read in. Read the help page at ?read_excel to learn more. "],["the-r-language-and-tidy-data-examples.html", "The R Language and Tidy Data Examples 2.7 Tidy Data 2.8 R Basics 2.9 Data, tibbles, dataframes", " The R Language and Tidy Data Examples 2.7 Tidy Data Each of the following datasets shows TB cases and some other variables per country organized in different ways. table1 ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 table3 ## # A tibble: 6 × 3 ## country year rate ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 # Spread across two tibbles table4a # cases ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4b # population ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 There are three interrelated rules which make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Which table above is Tidy? table1 ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 table3 ## # A tibble: 6 × 3 ## country year rate ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 # Spread across two tibbles table4a # cases ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 table4b # population ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 Why ensure that your data is tidy? There are two main advantages: There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows R’s vectorised nature to shine. As you learned in mutate and summary functions, most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural. 2.7.1 Let’s make a data dictionary for this dataset country = The country in which TB case data was reported. year = Calendar year cases = Test-positive cases with culture-based and ELISA-based tests in these countries with sample dates within the year above. population = The self-reported population of each country according to their census data. 2.8 R Basics 2.8.1 Data types You can create objects (variables~values, large data structures~think spreadsheets and databases, and functions) using the =, &lt;- or -&gt; operators. You can see what type of data (or data type) a variable is using the class function. Go ahead, try class(x). Data in R can be of several different, basic types: Data Type aka Example Logical Boolean TRUE, FALSE Numeric float 42, 3.14, Character string ‘a’ , “good”, “TRUE”, ‘23.4’ Integer 2L, 34L, 0L Complex 3 + 2i Raw hexadecimal “Hello” is stored as 48 65 6c 6c 6f 2.8.2 Functions What is a function? function_name(argument_name = argument_value) Using Tab-complete to make function calls will prevent errors! 2.8.3 Objects An object is essentially anything that shows up in the Environment pane! functions variables data objects 2.8.4 Vectors To demonstrate what a vector is let’s load some data! 2.9 Data, tibbles, dataframes 2.9.1 Reading in data library(readxl) storm &lt;- read_excel(path = &quot;data/hazards.xlsx&quot;, sheet = 2) storm ## # A tibble: 1,609 × 14 ## Date County monthly.ppt max.daily.ppt rain.shortage ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1981-06-01 00:00:00 24015 4.21 0.826 1.23 ## 2 1982-05-01 00:00:00 24015 3.67 2.02 4.06 ## 3 1983-04-01 00:00:00 24019 7.03 2.21 2.33 ## 4 1983-07-01 00:00:00 24047 1.94 0.915 5.32 ## 5 1983-08-01 00:00:00 24015 2.00 0.992 6 ## 6 1984-03-01 00:00:00 51001 6.85 1.77 1.87 ## 7 1984-05-01 00:00:00 10005 7.64 2.60 3.81 ## 8 1985-09-01 00:00:00 51001 5.73 4.65 4.53 ## 9 1988-05-01 00:00:00 10003 6.03 2.12 1.68 ## 10 1989-03-01 00:00:00 10003 4.57 1.06 3.23 ## # ℹ 1,599 more rows ## # ℹ 9 more variables: max.5.day.ppt &lt;dbl&gt;, monthly.tmin &lt;dbl&gt;, ## # monthly.tmax &lt;dbl&gt;, ndays.more.30tmax &lt;dbl&gt;, Hazard &lt;chr&gt;, TotalLoss &lt;dbl&gt;, ## # TotalLoss_drought &lt;dbl&gt;, TotalLoss_heat &lt;dbl&gt;, TotalLoss_storm &lt;dbl&gt; Each column is a vector! # check whether something is a vector with is.vector() # but some vectors are special like dates with formatting # use head(), summary(), or view() to look at data 2.9.2 Factors Factors are categorical variables. # look at a factor variable # can you add new values to factors? How would we check that a variable only contains certain values? storm$monthly.ppt &gt; 7 ## [1] FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [13] TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE TRUE ## [25] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE TRUE FALSE ## [37] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE FALSE ## [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## [157] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE ## [169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [181] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [253] TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [277] FALSE FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE ## [289] FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [313] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [325] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [337] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [349] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [361] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [373] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE ## [385] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [397] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [409] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [421] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [433] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [445] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [457] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [469] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE ## [481] FALSE FALSE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## [493] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [505] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [517] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [529] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [541] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [553] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE TRUE FALSE FALSE TRUE ## [565] TRUE FALSE FALSE TRUE FALSE FALSE TRUE FALSE TRUE FALSE TRUE TRUE ## [577] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [589] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [601] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [613] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [625] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [637] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [649] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [661] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE ## [673] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [685] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [697] TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [709] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [721] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [733] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [745] TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [757] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [769] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [781] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [793] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [805] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [817] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [829] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [841] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [853] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [865] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [877] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [889] FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE FALSE ## [901] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [913] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE FALSE ## [925] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE TRUE TRUE ## [937] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [949] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [961] FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [973] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [985] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [997] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1009] FALSE FALSE FALSE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE FALSE ## [1021] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1033] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1045] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1057] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1069] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1081] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE ## [1093] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [1105] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1117] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1129] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1141] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1153] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE TRUE TRUE FALSE ## [1165] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1177] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1189] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1201] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE ## [1213] TRUE FALSE TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE TRUE FALSE ## [1225] TRUE TRUE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1237] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1249] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1261] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1273] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1285] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1297] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1309] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1321] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1333] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1345] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1357] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1369] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1381] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1393] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1405] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1417] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1429] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## [1441] TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE ## [1453] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1465] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1477] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1489] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1501] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1513] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1525] FALSE FALSE FALSE FALSE TRUE FALSE FALSE TRUE TRUE FALSE FALSE TRUE ## [1537] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1549] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [1561] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1573] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE ## [1585] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE ## [1597] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [1609] FALSE "],["functions-best-coding-practices-and-debugging-intro.html", "3 Functions, Best-coding Practices, and Debugging Intro 3.1 Functions 3.2 Packages 3.3 Best coding practices 3.4 Debugging", " 3 Functions, Best-coding Practices, and Debugging Intro Now that we have introduced Rstudio, markdown, R basics, and Tidy data, it is our goal this week to improve our programming skills such that as we go forward you will be able to practice your problem solving efficiently by staying organized, avoiding common errors, and effectively debugging errors when they occur. This week our goals are to be able to: Understand the value of abstraction and “reusability” provided by functional programming Create basic functions Use Packages to access functions from the R community Organize code into logical blocks and use comments to help understand code Use RStudio’s formatting tools to avoid common errors Identify common errors and strategies for fixing/debugging them Reading: Skim https://r4ds.hadley.nz/program Skim https://rstudio-education.github.io/hopr/basics.html#functions down through 2.7 Summary Skim https://rstudio-education.github.io/hopr/packages.html Skim https://r4ds.hadley.nz/workflow-style Skim one of https://statsandr.com/blog/top-10-errors-in-r/ https://bookdown.org/yih_huynh/Guide-to-R-Book/trouble.html Ooof, videos on these topics were pretty rough. Let me know if you find any that you liked! 3.1 Functions Functions in programming are just like functions in math. In R they are even written just like they are in math \\(f(x)\\) in math is the same as F(x) in R Structured/Modular/Functional programming: a general programming concept where developers separate program functions into independent, modular, organized pieces, primarily functions. 3.1.1 Functions… why and how Write functions to encapsulate sections of code Allows you to avoid global variables that can be accessed and changed anywhere (functions only know about arguments you specifically pass to them) To avoid duplicating code: if you have multiple copies of almost identical code, put it into a function Create a function when R does not have a built-in function for your needs When running an analysis over and over again, can just call the function (one line) instead of running many lines of code 3.1.2 A simple function: CV() There is no R function for CV, or coefficient of variation (standard deviation divided by mean) # create the CV function CV &lt;- function(xvec) { CV.value &lt;- sd(xvec)/mean(xvec) return(CV.value) } Our CV function takes a single argument xvec which has no default value. (We could set a default value of 1 for example by using xvec = 1 inside the function(...) function). It calculates the standard deviation divided by the mean and returns the result. The return(CV.value) indicates what is returned by the function. If there is no return function the result of the last line is returned. 3.1.3 Now we need to apply our function. # pick 1000 random values from a normal distribution with mean 5 and sd 2 values &lt;- rnorm(n=1000, mean=5, sd=2) # now lets test our function CV(values) ## [1] 0.4060304 3.1.4 What objects are available? Check out the environment now. Only the new function CV and the object values are available in the Global Environment (they are global objects) Object CV.value (defined inside the function) is encapsulated within the function CV and not available outside it 3.1.5 Optional arguments Some functions have optional arguments. Optional arguments often have a default value, that is a value that is used if one isn’t provided by the user. For example, our CV function doesn’t work if xvec includes missing values. # Set the 100th value in values to NA values[100] &lt;- NA CV(values) ## [1] NA We could add an optional argument to allow it to remove NA values, as sd and mean both have na.rm optional arguments. When set to na.rm = TRUE, NA values are removed before the respective calculation. CVnew &lt;- function(xvec, na.rm=F) { CV.value &lt;- sd(xvec, na.rm=na.rm)/mean(xvec, na.rm=na.rm) return(CV.value) } ## this code could be tidied up, with Reformat Code ## try highlighting the code and using Code&gt;Reformat Code in the menu bar # Test out the new function CVnew(values) ## [1] NA CVnew(values, na.rm = T) ## [1] 0.4062672 3.2 Packages Packages, as we have discussed, are collections of functions, code, and data collected and curated by the R community. Packages are available through 3 primary sources CRAN https://cran.r-project.org/ see “Packages” on the left Bioconductor https://www.bioconductor.org/ GitHub CRAN and Bioconductor packages have undergone some peer review GitHub packages may or may not have been reviewed 3.2.1 Installing and using packages To install CRAN packages you can use the install.packages function with the argument being a character string of the package name use the Packages pane (which runs install.packages for you) Bioconductor and Github packages are a bit more tricky requiring packages to install their packages Bioconductor install instructions https://www.bioconductor.org/install/ To install Github packages you can use the remotes package https://cran.r-project.org/web/packages/remotes/index.html 3.2.2 Installing and using packages Clicking on the link to a package in the package pane takes you to the documentation of the package Documentation for most all CRAN and Bioconductor include description files help pages for their functions examples for their functions Many also have “vignettes” which are worked examples of how to use the package You can find all of this in the Help pane for that package. 3.3 Best coding practices 3.3.1 Reproducibility I (almost) never save my R workspace It contains global variables that could mess up your future code I regularly clear the workspace or run rm(list=ls()) Much better practice to rerun the entire script in an empty workspace Exception Results from analyses that take hours or days to run 3.3.2 Style guides Big companies have strict guidelines on how to organize your code, e.g. Google Style Guide https://google.github.io/styleguide/Rguide.html http://adv-r.had.co.nz/Style.html (Hadley Wickham’s guide) Indent your code inside a function, inside an if-statement, inside a for-loop, and if a statement goes over two lines Use meaningful object names Too long and you will get tired of typing Too short and you won’t know what the object contains Objects in R are global and available everywhere (more on this later) 3.3.3 Automated code styling/formatting Fortunately people have also written code to properly style your code according to common guidelines. In R-studio you can go to “Code” in the menu bar and “Reformat Code”. There is also a package called “styler” described a bit in https://r4ds.hadley.nz/workflow-style 3.4 Debugging The origin of debugging comes from literally removing a moth that shorted out one of the early computers at NASA. Admiral Grace Hopper said in a famous report they were “debugging” the computer and the term has stuck for all computers and code. 3.4.1 Verifying code Test your code, test your code, test your code! Write the smallest possible amount of code (a portion of one line if possible) Then try simple examples you know the answer to (zero, negative number, positive number) Show the results: is this what you expected? Pay special attention if you are copying sections of code and changing a variable name (common to forget to change all occurrences) 3.4.2 Commenting your code R ignores everything on a line that follows a # Comment at the top of your script/markdown What the code does, your name, email, date started Comment before each function or section of code What is the purpose of that section of code, what does it do Comment throughout Whenever an unusual function is used Whenever the code is hard to understand Whenever an algorithm is particularly useful 3.4.3 “Commenting out” code Instead of deleting code you might not need, or When you make modifications to your code, or During debugging Copy the code that works then comment it out by prefixing it with # Change the new copy of the code If you need to revert to the old code, just remove the # before each line (“uncomment”) Delete the old commented out code only once you have thoroughly tested the new code ctrl+shift+C is a shortcut in Rstudio to comment/uncomment large blocks of code #plot(iris$Sepal.Length) plot(iris$Petal.Length) 3.4.4 Common errors and How to Fix Them Semantic errors = mistyping errors Missing parentheses/brackets Missing quotes Misplaced commas Misspelled object names R is case-sensitive Resolving semantic errors Semantic errors generally have useful error messages, except missing parentheses brackets Always make sure there is a &gt; in the console! A + means the last line wasn’t completed. Use ESC to get from + back to &gt; 3.4.4.1 Missing Parentheses or Brackets If you forget a closing parenthesis ), bracket ], or curly brace }, R will show a continuation prompt in the console (+), meaning it’s waiting for you to finish the expression. Example (Missing Parenthesis) mean(c(1, 2, 3) # Missing closing parenthesis ## Error in parse(text = input): &lt;text&gt;:2:0: unexpected end of input ## 1: mean(c(1, 2, 3) # Missing closing parenthesis ## ^ Error Message: None, but look in the console + # Cursor is stuck at the continuation prompt Fix: Press ESC to return to the &gt; prompt, then add the missing ). mean(c(1, 2, 3)) # Corrected ## [1] 2 3.4.4.1.1 Missing or Mismatched Quotes If you forget to close a string with a quotation mark (\" or '), R doesn’t know where the string ends. Example (Missing Quote) x &lt;- &quot;Hello # Missing closing quote ## Error in parse(text = input): &lt;text&gt;:1:6: unexpected INCOMPLETE_STRING ## 1: x &lt;- &quot;Hello # Missing closing quote ## ^ Error Message: None, but again look in the console + # Cursor is stuck at the continuation prompt Fix: Press ESC to return to the &gt; prompt, then add the missing ). x &lt;- &quot;Hello&quot; # Corrected 3.4.4.2 Unexpected Symbols (e.g., Typos, Missing Commas) If you forget a comma between function arguments or mistype a variable name, R will return an “unexpected symbol” error. Example (Missing Comma) x &lt;- c(1 3, 5, 7) # Missing comma between 1 and 3 ## Error in parse(text = input): &lt;text&gt;:1:10: unexpected numeric constant ## 1: x &lt;- c(1 3 ## ^ Error Message: Error: unexpected numeric constant in \"x &lt;- c(1 3\" Fix: Add the missing comma. x &lt;- c(1, 3, 5, 7) # Corrected 3.4.4.3 Object Not Found (Using Undefined Variables and Functions) If you try to use a variable or function that hasn’t been defined, R will return an object not found error. Example (Misspelled Function) meann(c(1, 2, 3)) # Function name is incorrect ## Error in meann(c(1, 2, 3)): could not find function &quot;meann&quot; Error Message: Error in meann(c(1, 2, 3)) : could not find function \"meann\" Fix: Check for typos and use tab-completion. mean(c(1, 2, 3)) # Corrected ## [1] 2 Example (Undefined Variable) y &lt;- x + 10 # x has not been defined yet Error Message: Error: object 'x' not found Fix: Make sure x exists before using it. x &lt;- 5 y &lt;- x + 10 # Now it works 3.4.4.4 Type Mismatches (Coercion Issues) R automatically converts mixed types in vectors. If you try to perform numeric operations on character strings, you’ll get an error or warning. Example (Mixing Numbers and Text) x &lt;- c(1, 3, &quot;Emma&quot;) # Mixed data types as.numeric(x) # Convert to numeric ## Warning: NAs introduced by coercion ## [1] 1 3 NA Warning message: Warning: NAs introduced by coercion Fix: Ensure all elements are numeric or remove text values. x &lt;- c(1, 3, 5) # All numeric as.numeric(x) # Works fine ## [1] 1 3 5 If you must handle mixed data, filter out text: x &lt;- c(1, 3, &quot;Emma&quot;) x_numeric &lt;- as.numeric(x[!is.na(as.numeric(x))]) # Removes text ## Warning: NAs introduced by coercion 3.4.5 Warning or Error? Warnings are OK, just meant to inform you about something you might not intend to do. x &lt;- c(1, 3, 7, &quot;Emma&quot;) as.numeric(x) ## Warning: NAs introduced by coercion ## [1] 1 3 7 NA Errors mean the code didn’t run properly x &lt;- c(1 3, 7, &quot;Emma&quot;) as.numeric(x) ## Error in parse(text = input): &lt;text&gt;:1:10: unexpected numeric constant ## 1: x &lt;- c(1 3 ## ^ 3.4.6 Debugging is scientific Hypothesis: If I execute this line of code, then variable A will change from value x to value y Method: create an observation to report the value of A, then run the line of code Results: is A == y? Discussion: If A == y, then we move to our next investigation. A != y then you have found the bug (or your hypothesis is wrong) "],["functions-best-coding-practices-and-debugging-examples.html", "Functions, Best-coding Practices, and Debugging Examples 3.5 Functions 3.6 Best coding practices 3.7 Debugging", " Functions, Best-coding Practices, and Debugging Examples 3.5 Functions 3.5.1 Example 1 Practice turning the following code snippets into functions. Think about what each function does. What would you call it? How many arguments does it need? mean(is.na(x)) mean(is.na(y)) mean(is.na(z)) x / sum(x, na.rm = TRUE) y / sum(y, na.rm = TRUE) z / sum(z, na.rm = TRUE) round(x / sum(x, na.rm = TRUE) * 100, 1) round(y / sum(y, na.rm = TRUE) * 100, 1) round(z / sum(z, na.rm = TRUE) * 100, 1) percent_total &lt;- function(xvec) { mean(is.na(x)) x / sum(x, na.rm = TRUE) round(x / sum(x, na.rm = TRUE) * 100, 1) } percent_total(xvec = 1) ## Error in percent_total(xvec = 1): object &#39;x&#39; not found Note that clicking show traceback will allow you to see where the error occured. percent_total &lt;- function(xvec) { mean(is.na(xvec)) # % of values that are NA xvec / sum(xvec, na.rm = TRUE) return(round(xvec / sum(xvec, na.rm = TRUE) * 100, 1)) } percent_total(xvec = c(1, NA, 1)) ## [1] 50 NA 50 mean(is.na(c(1, NA, 1))) ## [1] 0.3333333 library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors percent_total &lt;- function(xvec) { tibble(# % of values that are NA mean(is.na(xvec)), # fraction of total for each value xvec / sum(xvec, na.rm = TRUE), # percent of total to 1 decimal round(xvec / sum(xvec, na.rm = TRUE) * 100, 1)) } percent_total(xvec = c(1, NA, 1)) ## # A tibble: 3 × 3 ## `mean(is.na(xvec))` `xvec/sum(xvec, na.rm = TRUE)` round(xvec/sum(xvec, na.r…¹ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.333 0.5 50 ## 2 0.333 NA NA ## 3 0.333 0.5 50 ## # ℹ abbreviated name: ¹​`round(xvec/sum(xvec, na.rm = TRUE) * 100, 1)` 3.5.2 Example 2 Create a function LW() with three arguments: a vector lengths, and values a and b It returns the weights of fishes using the length-weight equation: \\(W = aL^b\\) Use the function to calculate the weight (in g) of fish of length 100, 200, 300 cm for: Mola mola, a = 0.0454, b = 3.05 Regalecus glesne, a = 0.0039, b = 2.90 #&#39; Title #&#39; #&#39; @param lengths in centimeters #&#39; @param a constant #&#39; @param b exponent #&#39; #&#39; @return weight in grams #&#39; @export #&#39; #&#39; @examples #&#39; LW &lt;- function(lengths, a, b){ return(a * lengths ^ b) } LW(lengths = c(100, 200, 300), a = 0.0454, b = 3.05) ## [1] 57155.21 473366.30 1630330.60 3.6 Best coding practices 3.6.1 Code from inside out, running the smallest bits of code possible 3.6.2 Use RStudio to your advantage Tab-complete Reformat code Reindent lines Rainbow parentheses 3.7 Debugging Missing parentheses/brackets mean(x ## Error in parse(text = input): &lt;text&gt;:2:0: unexpected end of input ## 1: mean(x ## ^ Missing quotes print(&quot;Hello) ## Error in parse(text = input): &lt;text&gt;:1:7: unexpected INCOMPLETE_STRING ## 1: print(&quot;Hello) ## ^ Misplaced, missing commas x &lt;- c(&quot;1&quot; &quot;3&quot;, &quot;7&quot;, &quot;Emma&quot;) is.numeric(x) ## Error in parse(text = input): &lt;text&gt;:1:12: unexpected string constant ## 1: x &lt;- c(&quot;1&quot; &quot;3&quot; ## ^ Misspelled object names 3.7.1 Also use RStudio Debug menu is helpful for writing more advanced functions "],["data-wrangling.html", "4 Data Wrangling 4.1 Connection to previous work on Data Organization 4.2 Source 4.3 3.1 Introduction - Example dataset nycflights13 4.4 3.1.2 4.5 3.1.3 dplyr basics 4.6 3.2 Row-wise functions 4.7 3.3 Columns 4.8 3.4 The pipe 4.9 3.5 Groups", " 4 Data Wrangling This week our goals are to be able to: Use the dplyr package to perform basic data transformation and analysis Filter and arrange rows of datasets Create new columns with mutate Select columns Use pipes to make our code more readable Summarize data by groups using summarize 4.1 Connection to previous work on Data Organization This week, we will finally see why organized data is worth the effort. We’ll follow an exercise using a data source with over 300,000 rows! The work this week will show us (1) why R is awesome and fast for analysis, and (2) reinforce the purpose of organized data (following the 12 best practices we learned in Week 1). Because we are dealing with large datasets now, make sure that your Problem Set does not include pages and pages of data by just showing the top of the final result using head(dataset) 4.2 Source This exercise follows along with the reading for this week R for Data Science Chapter 3 https://r4ds.hadley.nz/data-transform (this was chapter 5 in the old version https://r4ds.had.co.nz/transform.html, hopefully I’ve updated everything but incase I haven’t there’s the link). The template below is for you to be able to follow along in the reading and complete the exercises. 4.3 3.1 Introduction - Example dataset nycflights13 I’ve gone ahead and installed the 2 packages, but you need to load them into the environment using: library(nycflights13) library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors Why is it important to do this? When you are creating code, explicitly turning on packages that are required is considered good practice. This goes along with the importance of being intentional and making your code reproducible by anyone, anywhere. Tell the computer what to do…explicitly! Tell everyone explicitly what you have done to get to your results. This also keeps your R sessions memory low and prevents duplicate functions from being loaded from different packages. Notice above when we load tidyverse we get the message that ✖ dplyr::filter() masks stats::filter() and ✖ dplyr::lag() masks stats::lag(), that is because the stats package also has filter and lag functions as well as the dplyr package which is part of the tidyverse package. The tidyverse is actually a package of packages including ggplot2, purrr, tibble, dplyr, tidyr, stringr, readr, and forcats (and maybe more since writing this). We will learn more about all of these in coming weeks. In our case, because we more recently loaded tidyverse if we call filter(some_argument...) this will run the tidyverse/dplyr version of the function. As it says in the reading, if you want to use the base, or stats, version of these functions after loading dplyr, you’ll need to specify the package that the function comes from using two colons :: as in stats::filter() and stats::lag(). 4.4 3.1.2 Run flights in the code chunk below. The output should match the reading. Note that you can find a nice README/data dictionary/documentation of this dataset by viewing its help documentation ?flights. flights ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; As described, flights is a data frame called a tibble. What does int mean on the third line of the table? Or dbl? These are types of variables. Be sure to familiarize yourself with the various types as you move forward, so focus on this section in the reading. You can also check out Vectors and data types in Data Carpentry. 4.5 3.1.3 dplyr basics https://r4ds.hadley.nz/data-transform#dplyr-basics Most of the tidyverse aims to make programming make “grammatical” sense in that it is easy to read, understand, and talk about using typical language. One of the really tricky parts of many programming languages including R that you have already experienced is how nested accessors (like [] and $) combined with functions and logical statements are used to do operations on parts of datasets (like finding the mean of certain columns from certain rows). This can make reading a line of code really difficult. You have to read the code from the inside out. For example, from last week, we can run from inside to out. mean(is.na(c(1, NA, 1))) ## [1] 0.3333333 # Inner parentheses c(1, NA, 1) ## [1] 1 NA 1 # next set is.na(c(1, NA, 1)) ## [1] FALSE TRUE FALSE #full line mean(is.na(c(1, NA, 1))) ## [1] 0.3333333 From the dplyr homepage: “dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges” where verbs are functions that operate on nouns, which are your dataset and elements within it. For all dplyr “verbs”: The first argument is always a data frame. The subsequent arguments typically describe which columns to operate on, using the variable names (without quotes). The output is always a new data frame. Direct from R4DS: “Because each verb does one thing well, solving complex problems will usually require combining multiple verbs, and we’ll do so with the pipe, |&gt;. We’ll discuss the pipe more in Section 3.4, but in brief, the pipe takes the thing on its left and passes it along to the function on its right so that x |&gt; f(y) is equivalent to f(x, y), and x |&gt; f(y) |&gt; g(z) is equivalent to g(f(x, y), z). The easiest way to pronounce the pipe is “then”. That makes it possible to get a sense of the following code even though you haven’t yet learned the details: flights |&gt; filter(dest == &quot;IAH&quot;) |&gt; group_by(year, month, day) |&gt; summarize( arr_delay = mean(arr_delay, na.rm = TRUE) ) ## `summarise()` has grouped output by &#39;year&#39;, &#39;month&#39;. You can override using the ## `.groups` argument. ## # A tibble: 365 × 4 ## # Groups: year, month [12] ## year month day arr_delay ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 1 1 17.8 ## 2 2013 1 2 7 ## 3 2013 1 3 18.3 ## 4 2013 1 4 -3.2 ## 5 2013 1 5 20.2 ## 6 2013 1 6 9.28 ## 7 2013 1 7 -7.74 ## 8 2013 1 8 7.79 ## 9 2013 1 9 18.1 ## 10 2013 1 10 6.68 ## # ℹ 355 more rows dplyr’s verbs are organized into four groups based on what they operate on: rows, columns, groups, or tables. In the following sections you’ll learn the most important verbs for rows, columns, and groups, then we’ll come back to the join verbs that work on tables in Chapter 19. Let’s dive in!” 4.6 3.2 Row-wise functions 4.6.1 3.2.1 filter() The example filters the data based on month and day. jan1 &lt;- filter(flights, month == 1, day == 1) The double-equals ==sign implies “is equal to”; in the filter function above, all flights on the first day of January are saved as a new variable jan1. What is happening in the command below? filter(flights, month == 1) ## # A tibble: 27,004 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 26,994 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; The reading also points out the use of the near function. Why is this important? Illustrate the example below in the code chunk below to reinforce the concept. Paste sqrt(1.9999999999999999999999)^2 in the code chunk and run it. If you keep removing the trailing 9s, when does the result not equal 2? What happens when you run sqrt(2)^2==2? Show me that you can have the computer make these equivalent using near(), and explain in one word—yes one word—the result of sqrt(2)^2==2 versus using the near function. (Hint: the word starts with P). 4.6.1.1 Logical Operators We learned about ==, “is equal to,” above. Other logical or Boolean operators that can be used as filters are &gt;, ==, &lt;, &lt;=, != (not equal). You can also combine these with other Logical or Boolean operators: &amp; (and), | (or), and ! (not). Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. x x &lt;- c(TRUE, TRUE, FALSE, FALSE) y &lt;- c(TRUE, FALSE, TRUE, FALSE) x | y ## [1] TRUE TRUE TRUE FALSE How would you select all flights in May and June? flights |&gt; filter(month == 5 | month == 6) ## # A tibble: 57,039 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 5 1 9 1655 434 308 2020 ## 2 2013 5 1 451 500 -9 641 640 ## 3 2013 5 1 537 540 -3 836 840 ## 4 2013 5 1 544 545 -1 818 827 ## 5 2013 5 1 548 600 -12 831 854 ## 6 2013 5 1 549 600 -11 804 810 ## 7 2013 5 1 553 600 -7 700 712 ## 8 2013 5 1 553 600 -7 655 701 ## 9 2013 5 1 554 600 -6 731 756 ## 10 2013 5 1 554 600 -6 707 725 ## # ℹ 57,029 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; sum(flights$month == 5) ## [1] 28796 sum(flights$month == 6) ## [1] 28243 sum(flights$month == 5 | flights$month == 6) ## [1] 57039 sum(flights$month == c(5,6)) ## [1] 28520 sum(flights$month %in% c(5,6)) ## [1] 57039 R also has another nifty logical operator %in%, which searches for a matches of one vector in another and return true for any matching values. So for example: # letters is simply the lowercase alphabet letters %in% c(&quot;a&quot;, &quot;b&quot;, &quot;z&quot;) ## [1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE TRUE So we could select all flights in May and June using this now. filter(flights, month %in% c(5, 6)) |&gt; tail(n = 100) ## # A tibble: 100 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 6 30 2334 1836 298 50 2015 ## 2 2013 6 30 2336 2029 187 231 2359 ## 3 2013 6 30 2343 2029 194 205 2303 ## 4 2013 6 30 2345 2146 119 229 30 ## 5 2013 6 30 2347 2125 142 105 2253 ## 6 2013 6 30 2348 2130 138 229 14 ## 7 2013 6 30 2354 2245 69 53 2359 ## 8 2013 6 30 2354 2245 69 117 1 ## 9 2013 6 30 2357 2112 165 223 2359 ## 10 2013 6 30 2358 2225 93 49 2330 ## # ℹ 90 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.6.1.2 Missing values We covered NAs in some of our exercises in previous weeks. Hopefully reading through this section helped reinforce in your mind how NAs are handled in R and in the dplyr::filter function. 4.6.2 3.2.3 Arranging rows We can arrange rows by a particular columns values using arrange. For example with the flights dataset we could arrange by departure time. flights |&gt; arrange(dep_time) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 13 1 2249 72 108 2357 ## 2 2013 1 31 1 2100 181 124 2225 ## 3 2013 11 13 1 2359 2 442 440 ## 4 2013 12 16 1 2359 2 447 437 ## 5 2013 12 20 1 2359 2 430 440 ## 6 2013 12 26 1 2359 2 437 440 ## 7 2013 12 30 1 2359 2 441 437 ## 8 2013 2 11 1 2100 181 111 2225 ## 9 2013 2 24 1 2245 76 121 2354 ## 10 2013 3 8 1 2355 6 431 440 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; By default arrange sorts the rows from low to high on the variable you pass. To sort high to low you put a - in front of the variable or use desc(variable). flights |&gt; arrange(-dep_time) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 10 30 2400 2359 1 327 337 ## 2 2013 11 27 2400 2359 1 515 445 ## 3 2013 12 5 2400 2359 1 427 440 ## 4 2013 12 9 2400 2359 1 432 440 ## 5 2013 12 9 2400 2250 70 59 2356 ## 6 2013 12 13 2400 2359 1 432 440 ## 7 2013 12 19 2400 2359 1 434 440 ## 8 2013 12 29 2400 1700 420 302 2025 ## 9 2013 2 7 2400 2359 1 432 436 ## 10 2013 2 7 2400 2359 1 443 444 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; This may seem a little awkward, but it allows you to easily provide multiple variable names for a complex sort. flights |&gt; arrange(desc(dep_time), sched_dep_time) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 12 29 2400 1700 420 302 2025 ## 2 2013 7 7 2400 1950 250 107 2130 ## 3 2013 9 12 2400 2000 240 203 2230 ## 4 2013 7 28 2400 2059 181 247 2322 ## 5 2013 2 11 2400 2135 145 251 35 ## 6 2013 7 17 2400 2142 138 54 2259 ## 7 2013 6 17 2400 2145 135 102 2315 ## 8 2013 7 13 2400 2155 125 225 43 ## 9 2013 7 13 2400 2245 75 101 2359 ## 10 2013 8 10 2400 2245 75 110 1 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.7 3.3 Columns 4.7.1 3.3.1 Mutate - Add new variables The function mutate is used to add new variables/columns to a data frame. Following the example at the beginning of section 5.5 in the book, add a new speed variable using mutate to your data frame. flights_sml &lt;- select(flights, year:day, ends_with(&quot;delay&quot;), distance, air_time ) #add a speed variable Next, pay attention to the Useful transformation functions and the modular arithmetic section to obtain hour and minutes from the departure data. Try for yourself below. This is pretty cool and can be useful. 4.7.2 3.3.2 Select It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. select() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. select() is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea: select(flights, year, month, day) ## # A tibble: 336,776 × 3 ## year month day ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 ## 2 2013 1 1 ## 3 2013 1 1 ## 4 2013 1 1 ## 5 2013 1 1 ## 6 2013 1 1 ## 7 2013 1 1 ## 8 2013 1 1 ## 9 2013 1 1 ## 10 2013 1 1 ## # ℹ 336,766 more rows 4.8 3.4 The pipe The pipe operator, which I’ve demonstrated just a few times above, is really fantastically justified by the The pipe section. I would definitely recommend reading this short section. The idea of piping is that it can make it easier to write, follow, and understand what the commands are doing. Think of each pipe command as “then”. The pipe command uses the following syntax : |&gt;. What it essentially does is take the result of the code on the left-hand side or previous line(s) and pass it as the first argument to the function on the right-hand side. We can recreate the example above with pipes. Written in words the code chunk below would be: ASSIGN a new object name, CHOOSE the dataset to operate on, THEN arrange the dataset by longest distance, THEN filter the December flights, THEN select the flight number, departure delay, and carrier columns. carrier_delay &lt;- # I like to use a new line here so that I can easily comment out this assignment line while building my pipe flights |&gt; arrange(-distance) |&gt; filter(month == 12) |&gt; select(flight, dep_delay, carrier) 4.9 3.5 Groups Grouped summaries are essentially what pivot tables are in Excel, if you have ever heard of those. By using the summarise() function with the group_by function we can, for example find the average flight delay by month. This becomes really awesome! This example starts with using group_by to group the data, then applies summarise. flights |&gt; group_by(month) |&gt; summarise( delay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 12 × 2 ## month delay ## &lt;int&gt; &lt;dbl&gt; ## 1 1 10.0 ## 2 2 10.8 ## 3 3 13.2 ## 4 4 13.9 ## 5 5 13.0 ## 6 6 20.8 ## 7 7 21.7 ## 8 8 12.6 ## 9 9 6.72 ## 10 10 6.24 ## 11 11 5.44 ## 12 12 16.6 Next week we’ll practice summarizing data a lot more as well as “pivoting” our data, which is We’ll get to making those sweet, sweet plots soon. "],["data-wrangling-examples.html", "Data Wrangling Examples", " Data Wrangling Examples These examples are adapted from R4DS https://r4ds.hadley.nz/data-transform. 4.9.1 Filter These are additional practice to those in the book to reinforce the reading and try by doing. Solutions for each are given below. Our suggestion is to try first and test your skill. nycflights13::flights ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Find all flights that: # 1.1 Had an arrival delay of two or more hours (10,034 flights) flights |&gt; # Had an arrival delay of two or more hours filter(arr_delay &gt; 120) ## # A tibble: 10,034 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 811 630 101 1047 830 ## 2 2013 1 1 848 1835 853 1001 1950 ## 3 2013 1 1 957 733 144 1056 853 ## 4 2013 1 1 1114 900 134 1447 1222 ## 5 2013 1 1 1505 1310 115 1638 1431 ## 6 2013 1 1 1525 1340 105 1831 1626 ## 7 2013 1 1 1549 1445 64 1912 1656 ## 8 2013 1 1 1558 1359 119 1718 1515 ## 9 2013 1 1 1732 1630 62 2028 1825 ## 10 2013 1 1 1803 1620 103 2008 1750 ## # ℹ 10,024 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # 1.2 Flew to Houston (IAH or HOU) (9,313 flights) flights |&gt; filter(dest %in% c(&quot;IAH&quot;,&quot;HOU&quot;)) ## # A tibble: 9,313 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 623 627 -4 933 932 ## 4 2013 1 1 728 732 -4 1041 1038 ## 5 2013 1 1 739 739 0 1104 1038 ## 6 2013 1 1 908 908 0 1228 1219 ## 7 2013 1 1 1028 1026 2 1350 1339 ## 8 2013 1 1 1044 1045 -1 1352 1351 ## 9 2013 1 1 1114 900 134 1447 1222 ## 10 2013 1 1 1205 1200 5 1503 1505 ## # ℹ 9,303 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; # 1.3 Were operated by United, American, or Delta (139,504 flights) # 1.4 Departed in summer (July, August, and September) (86,326 flights) # 1.5 Arrived more than two hours late, but didn&#39;t leave late (3 flights) # 1.6 Were delayed by at least an hour, but made up over 30 minutes in flight (1,819 flights) # 1.7 Departed between midnight and 6am (inclusive) (9,373 flights) Another useful dplyr filtering helper is between(). What does it do? Can you use it to simplify the code needed to answer 1.7? (hint: look up between in the help menu. You’ll see the required syntax, where x = vector, and left and right at the boundary values. You will also need to add an OR statement to include departure times at exactly 2400 since the dataframe has departures at both 0 and 2400) flights %&gt;% filter(dep_time |&gt; between(0, 600))|&gt; arrange(desc(month), -day) ## # A tibble: 9,344 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 12 31 13 2359 14 439 437 ## 2 2013 12 31 18 2359 19 449 444 ## 3 2013 12 31 26 2245 101 129 2353 ## 4 2013 12 31 459 500 -1 655 651 ## 5 2013 12 31 514 515 -1 814 812 ## 6 2013 12 31 549 551 -2 925 900 ## 7 2013 12 31 550 600 -10 725 745 ## 8 2013 12 31 552 600 -8 811 826 ## 9 2013 12 31 553 600 -7 741 754 ## 10 2013 12 31 554 550 4 1024 1027 ## # ℹ 9,334 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; How many flights have a missing dep_time? What other variables are missing? What might these rows represent? 4.9.1.1 solutions: 1.1 k &lt;- filter(flights,(arr_delay &gt; 120)) 1.2 k &lt;- filter(flights,dest == “IAH”|dest==“HOU”) 1.3 k &lt;- filter(flights,carrier==“DL”|carrier==“UA”|carrier==“AA”) 1.4 k &lt;- filter(flights,month==7 | month==8 | month==9) 1.5 k &lt;- filter(flights,arr_delay &gt;120 &amp; dep_delay == 0) 1.6 filter(flights,dep_delay &gt;60 &amp; arr_delay &lt;(dep_delay-30))) 1.7 k &lt;- filter(flights,dep_time==2400 | (dep_time&lt;0601)) 2. m &lt;- filter(flights,between(dep_time,0,0600)|dep_time==2400) 3. y &lt;- filter(flights, is.na(dep_time)) 4.9.2 Arrange Use desc() to re-order by a column in descending order: flights |&gt; arrange(desc(month), -day) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 12 31 13 2359 14 439 437 ## 2 2013 12 31 18 2359 19 449 444 ## 3 2013 12 31 26 2245 101 129 2353 ## 4 2013 12 31 459 500 -1 655 651 ## 5 2013 12 31 514 515 -1 814 812 ## 6 2013 12 31 549 551 -2 925 900 ## 7 2013 12 31 550 600 -10 725 745 ## 8 2013 12 31 552 600 -8 811 826 ## 9 2013 12 31 553 600 -7 741 754 ## 10 2013 12 31 554 550 4 1024 1027 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Sort flights to find the most delayed flights. Find the flights that left earliest. Sort flights to find the fastest (highest speed) flights. Here you are creating a metric by using the existing data in the dataframe to calculate speed. Which flights traveled the farthest? Which traveled the shortest? (flights 1632 and 51) arrange(flights, -distance) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 857 900 -3 1516 1530 ## 2 2013 1 2 909 900 9 1525 1530 ## 3 2013 1 3 914 900 14 1504 1530 ## 4 2013 1 4 900 900 0 1516 1530 ## 5 2013 1 5 858 900 -2 1519 1530 ## 6 2013 1 6 1019 900 79 1558 1530 ## 7 2013 1 7 1042 900 102 1620 1530 ## 8 2013 1 8 901 900 1 1504 1530 ## 9 2013 1 9 641 900 1301 1242 1530 ## 10 2013 1 10 859 900 -1 1449 1530 ## # ℹ 336,766 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.9.3 Mutate - adds a new column #flights1 &lt;- flights |&gt; mutate(check_arr_delay = sched_arr_time - arr_time, check_dep_delay = sched_dep_time - dep_time) #-&gt; ## # A tibble: 336,776 × 21 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 830 819 ## 2 2013 1 1 533 529 4 850 830 ## 3 2013 1 1 542 540 2 923 850 ## 4 2013 1 1 544 545 -1 1004 1022 ## 5 2013 1 1 554 600 -6 812 837 ## 6 2013 1 1 554 558 -4 740 728 ## 7 2013 1 1 555 600 -5 913 854 ## 8 2013 1 1 557 600 -3 709 723 ## 9 2013 1 1 557 600 -3 838 846 ## 10 2013 1 1 558 600 -2 753 745 ## # ℹ 336,766 more rows ## # ℹ 13 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, check_arr_delay &lt;int&gt;, ## # check_dep_delay &lt;int&gt; # flights1 4.9.4 Select flights |&gt; mutate(check_arr_delay = arr_time - sched_arr_time, check_dep_delay = sched_dep_time - dep_time) |&gt; select(arr_time, sched_arr_time, check_arr_delay, arr_delay) |&gt; mutate(arr_check_boolean = arr_delay == check_arr_delay) |&gt; filter(arr_check_boolean == FALSE) ## # A tibble: 114,963 × 5 ## arr_time sched_arr_time check_arr_delay arr_delay arr_check_boolean ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 923 850 73 33 FALSE ## 2 913 854 59 19 FALSE ## 3 854 902 -48 -8 FALSE ## 4 858 910 -52 -12 FALSE ## 5 858 915 -57 -17 FALSE ## 6 807 735 72 32 FALSE ## 7 1039 1100 -61 -21 FALSE ## 8 909 840 69 29 FALSE ## 9 1016 947 69 29 FALSE ## 10 1028 940 88 48 FALSE ## # ℹ 114,953 more rows flights |&gt; mutate(arr_time = lubridate::hm(arr_time)) |&gt; head() ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `arr_time = lubridate::hm(arr_time)`. ## Caused by warning in `.parse_hms()`: ## ! Some strings failed to parse ## # A tibble: 6 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;Period&gt; &lt;int&gt; ## 1 2013 1 1 517 515 2 NA 819 ## 2 2013 1 1 533 529 4 NA 830 ## 3 2013 1 1 542 540 2 NA 850 ## 4 2013 1 1 544 545 -1 NA 1022 ## 5 2013 1 1 554 600 -6 NA 837 ## 6 2013 1 1 554 558 -4 NA 728 ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.9.5 Grouped summaries To make summary tables we will use the pipe combining group_by and summarize. summary_FlightDelay &lt;- # I like to use a new line here so that I can easily comment out the # assignment while building my pipe flights |&gt; group_by(month, carrier) |&gt; # group flights by month summarise(delay = mean(dep_delay, na.rm = TRUE)) # make a new column of average dep delay ## `summarise()` has grouped output by &#39;month&#39;. You can override using the ## `.groups` argument. summary_FlightDelay |&gt; ggplot(mapping = aes(x = month, y = delay, color = carrier)) + geom_point() We could also figure out which carrier had the longest and shortest delay in December, if we were trying to plan a timely winter break flight. carrier_delay &lt;- # I like to use a new line here so that I can easily comment out this assignment line while building my pipe flights |&gt; arrange(-distance) |&gt; filter(month == 12) |&gt; select(flight, dep_delay, carrier) carrier_delay ## # A tibble: 28,135 × 3 ## flight dep_delay carrier ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 51 -2 HA ## 2 51 -10 HA ## 3 51 3 HA ## 4 51 -10 HA ## 5 51 -6 HA ## 6 51 -3 HA ## 7 51 5 HA ## 8 51 -2 HA ## 9 51 0 HA ## 10 51 -1 HA ## # ℹ 28,125 more rows carrier_delay |&gt; group_by(carrier) |&gt; # want to find the average for each carrier summarise(average_dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt; # calculate average delay arrange(average_dep_delay) ## # A tibble: 15 × 2 ## carrier average_dep_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 HA -3.14 ## 2 US 4.94 ## 3 VX 6.10 ## 4 DL 10.8 ## 5 AA 11.7 ## 6 MQ 12.7 ## 7 YV 13.1 ## 8 F9 13.1 ## 9 B6 17.0 ## 10 UA 17.7 ## 11 AS 18.0 ## 12 9E 19.8 ## 13 WN 24.9 ## 14 FL 26.1 ## 15 EV 27.9 "],["summarizing-data.html", "5 Summarizing Data 5.1 Learning Objectives 5.2 Introduction 5.3 group_by function: Summarizing info based on type/characteristic 5.4 Grouping with multiple variables 5.5 Tidying and untidying data with pivot_ functions 5.6 Joins and Binds within dplyr", " 5 Summarizing Data 5.1 Learning Objectives This week we will: Practice summarizing large datasets by groups Tidy and untidy data with pivot_longer and pivot_wider Join datasets using different _join functions 5.2 Introduction This week we’re focused on building our ability to analyze data. We’ll incorporate new functions from the dplyr package, and explore table joins. The information below augments the readings. Following this, you’ll be in good shape to start this week’s exercises. Let’s get started! 5.2.1 Readings (complete by class on Monday) Required: R for Data Science, section 3.5: Groups R for Data Science, section 5.3 and 5.4: lengthening and Widening data R for Data Science, chapter 19: Joins As you read through, my suggestion is to have the markdown version of this document open in Posit where you can take notes on important functions/concepts and also code along with the examples. This way you’ll get a headstart on your cheat sheet for this week. The below summarizes and provides a different example, but feel free to copy in examples from the reading and tinker with them a bit. These optional readings will also be useful follow up: dplyr lesson in Data Carpentry dplyr vignette (long form example document): if you’re struggling, this provides additional examples that you can try on your own (and as a bonus they use a Star Wars data set). Package vignettes like this are an amazing resource for learning new packages, and evaluating their capabilities, once you get the basics. tidyr pivot vignette again provides some nice examples of the pivot_longer and pivot_wider functions with different data set (although none quite as fun as the Star Wars data, alas). 5.3 group_by function: Summarizing info based on type/characteristic Based on reading from R for Data Science, section 3.5: Groups group_by(data, column_to_group_by) or with pipes data |&gt; group_by(column_to_group_by) combines rows into groups based on a column characteristic provides ability to summarize information based on one or more variables in your dataset when combine with summarize Let’s start by activating the data sets package within R-studio. We’re going to explore the sleep data set. library(datasets) library(dplyr) sleep ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0.0 1 9 ## 10 2.0 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 ?sleep The dataset has 3 variables: extra, is a numeric variable representing the increase in hours of sleep group is a categorical factor variable representing the drug given ID is another factor for the patient ID Suppose you want to know how the 2 drug treatment groups varied with respect to the extra hours of sleep. First we group the data by group, the drug given. sleep |&gt; group_by(group) ## # A tibble: 20 × 3 ## # Groups: group [2] ## extra group ID ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 ## 7 3.7 1 7 ## 8 0.8 1 8 ## 9 0 1 9 ## 10 2 1 10 ## 11 1.9 2 1 ## 12 0.8 2 2 ## 13 1.1 2 3 ## 14 0.1 2 4 ## 15 -0.1 2 5 ## 16 4.4 2 6 ## 17 5.5 2 7 ## 18 1.6 2 8 ## 19 4.6 2 9 ## 20 3.4 2 10 Note that this looks the same as sleep only with Groups: group [2] as an attribute. Now you can use summarize function to calculate statistics for each group within the dataset. Here, we only have 2 groups so it’s a simple example. # summarize the number of rows for each group sleep |&gt; group_by(group) |&gt; summarize(N = n()) ## # A tibble: 2 × 2 ## group N ## &lt;fct&gt; &lt;int&gt; ## 1 1 10 ## 2 2 10 We can save this summary table as a new object sleep_N &lt;- sleep |&gt; group_by(group) |&gt; summarize(N = n()) sleep_N ## # A tibble: 2 × 2 ## group N ## &lt;fct&gt; &lt;int&gt; ## 1 1 10 ## 2 2 10 The result here is trivial, but imagine if you had a larger dataset, like the Dipodomys survey from last week. One technique in coding is to start with something small, where you can easily hand-calculate the answer. Let’s look at other functions you can use within summarize: basic stats: https://www.dummies.com/education/math/statistics/base-r-statistical-functions/ sum() mean() var() sd() range() cor() min() summary() max() quantile() median() Let’s look at the median number of extra sleep hours: sleep |&gt; group_by(group) |&gt; summarize(median_extra_sleep = median(extra)) ## # A tibble: 2 × 2 ## group median_extra_sleep ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 0.35 ## 2 2 1.75 Now let’s look at what would happen if there was missing data by changing a value: # summarize the number of rows for each group sleep[1,1] = NA # Oh I used `=` here to assign this, # the assignment arrows, `&lt;-` or `-&gt;`, are generally best-coding # practice as they differentiate variables and objects, from function # arguments Now, if we implement the summarize command for the median sleep we’ll obtain a NA value: sleep |&gt; group_by(group) |&gt; summarize(median_sleep = median(extra)) ## # A tibble: 2 × 2 ## group median_sleep ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 NA ## 2 2 1.75 So what can we do? Start by adding a na.rm = TRUE command. sleep |&gt; group_by(group) |&gt; summarize(median_sleep = median(extra, na.rm = TRUE)) ## # A tibble: 2 × 2 ## group median_sleep ## &lt;fct&gt; &lt;dbl&gt; ## 1 1 0 ## 2 2 1.75 If you still had NaN or some other troublesome value (depending on your data set), you could filter the data: filter(data, !is.na(variable)) 5.4 Grouping with multiple variables sleep is a really simple data set, but what if we have a slightly bigger data set with multiple variables we wanted to group by and summarize? Let’s look at the “Carbon Dioxide Uptake in Grass Plants” CO2 data set. This data set contains ambient CO2 (conc) and CO2 uptake rate (uptake) measurements for Echinochloa crus-galli grass species from Quebec or Mississippi (type) that have undergone a chilling treatment or not (treatment). head(CO2) ## Plant Type Treatment conc uptake ## 1 Qn1 Quebec nonchilled 95 16.0 ## 2 Qn1 Quebec nonchilled 175 30.4 ## 3 Qn1 Quebec nonchilled 250 34.8 ## 4 Qn1 Quebec nonchilled 350 37.2 ## 5 Qn1 Quebec nonchilled 500 35.3 ## 6 Qn1 Quebec nonchilled 675 39.2 The obvious questions to ask of this data are: What effect does the origin/type of grass species have on CO2 uptake? What effect does the chilling treatment have on CO2 uptake? What effect does ambient CO2 conc have on CO2 uptake? Let’s try to answer these questions with one long pipe! The 3rd question is ideally a regression which we’ll save for after spring break, but we could separate the conc vector into a few groups and summarize the uptake for each group. So let’s make a plan. We want to group by type and treatment as well as conc, but first we need to make a simplified, group_conc variable. Remember we use mutate to make new variables. case_when or cut are a convenient functions to use in mutate to make categorical variables from numerical variables. The easiest way to figure out how case_when works is to check out the Examples section in ?case_when. So we will, Make a new group_conc to simplify data a bit (let’s say 95 ~ “low” and 1000 ~ “high”). Group by Type, Treatment, and group_conc. Summarize the mean and standard deviation CO2 uptake of each group. This table will allow us to compare the average uptake values of each of these groups. All we need to do to summarize all of these combinations of variables is pass the three variable names to group_by! CO2 |&gt; mutate(group_conc = case_when(conc == 1000 ~ &quot;high&quot;, conc == 95 ~ &quot;low&quot;, .default = NA)) |&gt; group_by(Type, Treatment, group_conc) |&gt; summarise(mean_uptake = mean(uptake), sd_uptake = sd(uptake))# -&gt; ## `summarise()` has grouped output by &#39;Type&#39;, &#39;Treatment&#39;. You can override using ## the `.groups` argument. ## # A tibble: 12 × 5 ## # Groups: Type, Treatment [4] ## Type Treatment group_conc mean_uptake sd_uptake ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Quebec nonchilled high 43.2 3.06 ## 2 Quebec nonchilled low 15.3 1.45 ## 3 Quebec nonchilled &lt;NA&gt; 37.8 4.91 ## 4 Quebec chilled high 40.8 1.91 ## 5 Quebec chilled low 12.9 3.12 ## 6 Quebec chilled &lt;NA&gt; 33.7 5.72 ## 7 Mississippi nonchilled high 31.6 3.85 ## 8 Mississippi nonchilled low 11.3 0.7 ## 9 Mississippi nonchilled &lt;NA&gt; 27.8 4.45 ## 10 Mississippi chilled high 18.7 3.88 ## 11 Mississippi chilled low 9.6 1.65 ## 12 Mississippi chilled &lt;NA&gt; 16.5 3.23 #CO2_summary To make it easier for us to make some conclusions from this we can clean it up a little bit by removing the NA rows and arrange-ing by mean_uptake. CO2 |&gt; mutate(group_conc = case_when(conc == 1000 ~ &quot;high&quot;, conc == 95 ~ &quot;low&quot;, .default = NA)) |&gt; group_by(Type, Treatment, group_conc) |&gt; summarise(mean_uptake = mean(uptake), sd_uptake = sd(uptake)) |&gt; filter(!is.na(group_conc)) |&gt; arrange(mean_uptake) ## `summarise()` has grouped output by &#39;Type&#39;, &#39;Treatment&#39;. You can override using ## the `.groups` argument. ## # A tibble: 8 × 5 ## # Groups: Type, Treatment [4] ## Type Treatment group_conc mean_uptake sd_uptake ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mississippi chilled low 9.6 1.65 ## 2 Mississippi nonchilled low 11.3 0.7 ## 3 Quebec chilled low 12.9 3.12 ## 4 Quebec nonchilled low 15.3 1.45 ## 5 Mississippi chilled high 18.7 3.88 ## 6 Mississippi nonchilled high 31.6 3.85 ## 7 Quebec chilled high 40.8 1.91 ## 8 Quebec nonchilled high 43.2 3.06 Now looking at this final summary table we can see Quebec varieties had higher uptake rates than Mississippi, the chilling treatment reduced uptake rates, and high ambient concentrations increase uptake rates. 5.5 Tidying and untidying data with pivot_ functions Based on reading from R for Data Science, section 5.3 and 5.4: Lengthening and Widening data Data can come your way in untidy forms which you will need to tidy up for analysis. Also, occasionally you may want to intentionally untidy data to do some analyses, or present the data in a shorter form table. For these tasks, so long as the data is organized systematically, you can make use of the tidyr package functions pivot_longer or pivot_wider. For tidying untidy data, we can use pivot_longer. For example if a variable is encoded in column names, like “cell_growth_5ug_ml_cefo” in which a column has cell growth measurements (or “observations”) at a specific concentration of an antibiotic, 5 µg/mL cefotaxime in this case. The reading uses a fun billboard data set. billboard ## # A tibble: 317 × 79 ## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA ## 2 2Ge+her The … 2000-09-02 91 87 92 NA NA NA NA NA ## 3 3 Doors D… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 ## 4 3 Doors D… Loser 2000-10-21 76 76 72 69 67 65 55 59 ## 5 504 Boyz Wobb… 2000-04-15 57 34 25 17 17 31 36 49 ## 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 ## 7 A*Teens Danc… 2000-07-08 97 97 96 95 100 NA NA NA ## 8 Aaliyah I Do… 2000-01-29 84 62 51 41 38 35 35 38 ## 9 Aaliyah Try … 2000-03-18 59 53 38 28 21 18 16 14 ## 10 Adams, Yo… Open… 2000-08-26 76 76 74 69 68 67 61 58 ## # ℹ 307 more rows ## # ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, ## # wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, ## # wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, ## # wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, ## # wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, ## # wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, … Looking at this dataset we can see each row is a song and there are columns for each week of the year which contains the position of the song on the billboard chart. Definitely not tidy, right?! The “week” columns are actually a variable themselves. An easy way to identify this sort of untidiness is that column names provide no indication of what values they contain. To tidy this we want to make the wide table of weeks into a long table that has “week” as a variable, and “position” as a variable. This would be an enormous task to do, but fortunately pivot_longer makes it pretty easy. The arguments to pivot_longer are cols which is list of column names you want to condense into a single column. To the cols argument we can pass helper “tidy-select” functions which are starts_with(\"a\"): names that start with \"a\". ends_with(\"z\"): names that end with \"z\". contains(\"b\"): names that contain \"b\". matches(\"x.y\"): names that match regular expression x.y. num_range(x, 1:4): names following the pattern, x1, x2, …, x4. `all_of(vars)`` will match just the variables that exist. everything(): all variables. last_col(): furthest column on the right. where(is.numeric): all variables where is.numeric() returns TRUE. Let’s use starts_with() to grab all the columns that start with “wk”. The other arguments that we need are names_to and values_to billboard |&gt; pivot_longer( cols = starts_with(&quot;wk&quot;), names_to = &quot;week&quot;, values_to = &quot;rank&quot; ) ## # A tibble: 24,092 × 5 ## artist track date.entered week rank ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk1 87 ## 2 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk2 82 ## 3 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk3 72 ## 4 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk4 77 ## 5 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk5 87 ## 6 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk6 94 ## 7 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk7 99 ## 8 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk8 NA ## 9 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk9 NA ## 10 2 Pac Baby Don&#39;t Cry (Keep... 2000-02-26 wk10 NA ## # ℹ 24,082 more rows Note how long this longer table is now, 24 thousand rows! This tidied dataset can now be analyzed and plottes with ease. However we can’t fit this dataset (or at least a few songs) onto a page easily in it’s tidy form. For that we need to widen. It is rarely the case that we need to use pivot_wider except for trying to display data in a more convenient table. We could pivot_wider our billboard dataset back to the original for example. The arguments are just the opposite, names_from = \"week\" and values_from = \"rank\". billboard |&gt; pivot_longer( cols = starts_with(&quot;wk&quot;), names_to = &quot;week&quot;, values_to = &quot;rank&quot; ) |&gt; pivot_wider( names_from = &quot;week&quot;, values_from = &quot;rank&quot; ) ## # A tibble: 317 × 79 ## artist track date.entered wk1 wk2 wk3 wk4 wk5 wk6 wk7 wk8 ## &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 Pac Baby… 2000-02-26 87 82 72 77 87 94 99 NA ## 2 2Ge+her The … 2000-09-02 91 87 92 NA NA NA NA NA ## 3 3 Doors D… Kryp… 2000-04-08 81 70 68 67 66 57 54 53 ## 4 3 Doors D… Loser 2000-10-21 76 76 72 69 67 65 55 59 ## 5 504 Boyz Wobb… 2000-04-15 57 34 25 17 17 31 36 49 ## 6 98^0 Give… 2000-08-19 51 39 34 26 26 19 2 2 ## 7 A*Teens Danc… 2000-07-08 97 97 96 95 100 NA NA NA ## 8 Aaliyah I Do… 2000-01-29 84 62 51 41 38 35 35 38 ## 9 Aaliyah Try … 2000-03-18 59 53 38 28 21 18 16 14 ## 10 Adams, Yo… Open… 2000-08-26 76 76 74 69 68 67 61 58 ## # ℹ 307 more rows ## # ℹ 68 more variables: wk9 &lt;dbl&gt;, wk10 &lt;dbl&gt;, wk11 &lt;dbl&gt;, wk12 &lt;dbl&gt;, ## # wk13 &lt;dbl&gt;, wk14 &lt;dbl&gt;, wk15 &lt;dbl&gt;, wk16 &lt;dbl&gt;, wk17 &lt;dbl&gt;, wk18 &lt;dbl&gt;, ## # wk19 &lt;dbl&gt;, wk20 &lt;dbl&gt;, wk21 &lt;dbl&gt;, wk22 &lt;dbl&gt;, wk23 &lt;dbl&gt;, wk24 &lt;dbl&gt;, ## # wk25 &lt;dbl&gt;, wk26 &lt;dbl&gt;, wk27 &lt;dbl&gt;, wk28 &lt;dbl&gt;, wk29 &lt;dbl&gt;, wk30 &lt;dbl&gt;, ## # wk31 &lt;dbl&gt;, wk32 &lt;dbl&gt;, wk33 &lt;dbl&gt;, wk34 &lt;dbl&gt;, wk35 &lt;dbl&gt;, wk36 &lt;dbl&gt;, ## # wk37 &lt;dbl&gt;, wk38 &lt;dbl&gt;, wk39 &lt;dbl&gt;, wk40 &lt;dbl&gt;, wk41 &lt;dbl&gt;, wk42 &lt;dbl&gt;, … 5.6 Joins and Binds within dplyr Based on reading from R for Data Science, chapter 19: Joins Within Biological Systems Engineering and the larger field, there are many times when we use multiple tables to reduce redundant data, particularly as data is being collected. Imagine you have a very large table that repeats double-precision data over and over again, resulting in a dataset that occupies more computer memory. While modern computers are incredible in their storage capacity, I can attest that processing speed and memory allocation is and will still be a consideration in your work. Here are some examples of such tasks: Perhaps, your experiment spans many days and you collected each day’s data in a different sheet that you now want to compile. Perhaps, you didn’t want to write down complete experimental conditions for each sample and instead used a code or number to indicate different conditions, now you want to add the full info to the table. Or maybe you measured something at a place over time and want to add weather station data as new columns. The above are two different tasks: 1) just stitching datasets together is a bind_, whereas examples in 2) are adding new variables based on the values in some key/code variable, like the experiment code or date, which is a _join. In dplyr we can use bind_rows to add new observations to a dataset, like binding multiple days data tables together. bind_rows will return an error if the column names of the tables we are joining are not the same. You can also use bind_cols to add new columns to a dataset without any key or identifier variable, but this is risky, as there is no guarantee that the rows (observations) will be in the same order. It’s better to use a _join function which checks that the rows match based on shared variables. Let’s illustrate joins using data from the Problem Set. We’ll first read each of these into the workspace: plots &lt;- read_csv(&quot;data/plots.csv&quot;) ## Rows: 24 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): plot_type ## dbl (1): plot_id ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. species &lt;- read_csv(&quot;data/species.csv&quot;) ## Rows: 54 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): species_id, genus, species, taxa ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. surveys &lt;- read_csv(&quot;data/surveys.csv&quot;) ## Rows: 35549 Columns: 9 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): species_id, sex ## dbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Note that when you do this, there are 24 plots, 54 types of species, but 35,549 lines of data in the surveys dataset! These tables are all related the question is: “What is the column that is shared by, or links, the plots and surveys dataset?” plot_id! What is the column that links the surveys dataset with the species? species_id! If we want to combine the dataset, let’s look at how we would do this. We’ll employ the command: inner_join From help: The mutating joins add columns from dataset y to dataset x, matching rows based on the keys: inner_join(x,y): includes all rows in x and y. left_join(x,y): includes all rows in x. right_join(x,y): includes all rows in y. full_join(x,y): includes all rows in x or y. If a row in x matches multiple rows in y, all the rows in y will be returned once for each matching row in x. The dplyr cheat sheet (also up in the Help&gt;Cheat Sheets menu) demonstrates this visually. If you want to include the detailed names contained within the species table, use an inner join and join based on the common variable species_id, using the by argument to specify the column name(s), as a character vector, that you want to link the two datasets. In this case, you end up with 3 additional variables that are within the species table added to the surveys data. combo &lt;- inner_join(surveys, species, by = &quot;species_id&quot;) head(combo) ## # A tibble: 6 × 12 ## record_id month day year plot_id species_id sex hindfoot_length weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## # ℹ 3 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt; If you wanted to join all three tables together we could add this to the first combo: combo2 &lt;- inner_join(combo, plots, by = &quot;plot_id&quot;) Or use pipes. This is actually more efficient because you aren’t creating the intermediate combo object that takes up memory, plus, this way you don’t accidentally mix up combo and combo2. As always more descriptive names would be better! combo2 &lt;- surveys %&gt;% # oops this is what the pipe symbol used to be inner_join(species, by = &quot;species_id&quot;) |&gt; inner_join(plots, by = &quot;plot_id&quot;) head(combo2) ## # A tibble: 6 × 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## # ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt; While it is best practice to specify the columns you are using to uniquely match the datasets (called unique identifiers sometimes) the _join functions are pretty smart and will find variables that match between the datasets automatically as well! surveys |&gt; inner_join(species) |&gt; inner_join(plots) |&gt; head() ## Joining with `by = join_by(species_id)` ## Joining with `by = join_by(plot_id)` ## # A tibble: 6 × 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## # ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt; "],["summarizing-data-examples.html", "Summarizing Data Examples 5.7 Bioreactor Data Analysis 5.8 Root Growth Inhibition Example", " Summarizing Data Examples library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(readxl) 5.7 Bioreactor Data Analysis 5.7.1 Background In a bioprocess engineering lab, several bioreactor runs were performed to study the effect of different feed types on process performance. Two datasets were collected: Bioreactor Readings This dataset (bioreactor_readings.csv) contains time-series measurements recorded during each run. It is provided in a wide format with columns for the run number, time (in hours), and various sensor readings: Run: Identifier for the bioreactor run. Time: Time stamp (hours). Temp: Temperature (°C). pH: pH level. DO: Dissolved Oxygen (mg/L). Substrate: Substrate concentration (g/L). Experimental Metadata This dataset (experiment_info.csv) contains metadata about each run: Run: Run identifier (matching the bioreactor dataset). Feed: Type of feed (e.g., “Glucose”, “Glycerol”). Inoculum: Inoculum concentration (OD units). Operator: Name of the operator in charge. 5.7.2 Tasks 5.7.2.1 1. Import and Inspect the Data - Load both CSV files into R as data frames. - Use the `head()` function (or similar) to inspect the first few rows of each dataset. # Load libraries library(tidyverse) # Import data bioreactor &lt;- read_csv(&quot;data/bioreactor_readings.csv&quot;) ## Rows: 10 Columns: 6 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (6): Run, Time, Temp, pH, DO, Substrate ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. experiment &lt;- read_csv(&quot;data/experiment_info.csv&quot;) ## Rows: 2 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Feed, Operator ## dbl (2): Run, Inoculum ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(bioreactor) ## # A tibble: 6 × 6 ## Run Time Temp pH DO Substrate ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 30 7 6.5 1.5 ## 2 1 1 30.5 7.1 6.4 1.3 ## 3 1 2 31 7.2 6.3 1.1 ## 4 1 3 31.2 7.1 6.2 0.9 ## 5 1 4 31 7 6 0.8 ## 6 2 0 29.5 6.8 6.8 1 head(experiment) ## # A tibble: 2 × 4 ## Run Feed Inoculum Operator ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 Glucose 0.15 Alice ## 2 2 Glycerol 0.12 Bob 5.7.2.2 2. Data Tidying (Pivoting) - **Pivot Longer:**\\ Convert the bioreactor readings from wide format into a long (tidy) format so that all measurements (Temp, pH, DO, Substrate) are in a single column named `Parameter` with corresponding values in a column named `Value`.\\ *Hint: Use `tidyr::pivot_longer()`.* - **Pivot Wider:**\\ After some analysis, you decide to reshape the long dataset back into a wide format, but now grouping by `Run` and `Time` such that each parameter becomes its own column again.\\ *Hint: Use `tidyr::pivot_wider()`.* # 2. Pivot longer: Tidy the bioreactor data bioreactor_long &lt;- bioreactor %&gt;% pivot_longer(cols = Temp:Substrate, names_to = &quot;Parameter&quot;, values_to = &quot;Value&quot;) # This would allow us to plot both parameters on the same plot ggplot(data = bioreactor_long, mapping = aes(x = Time, y = Value, color = Parameter, linetype = as.factor(Run))) + geom_line() 5.7.2.3 2b. Pivot wider: Reshape back to wide format if needed bioreactor_wide &lt;- bioreactor_long %&gt;% pivot_wider(names_from = Parameter, values_from = Value) 5.7.2.4 3. Joining Datasets - Merge the reshaped bioreactor dataset with the experimental metadata using the common `Run` column.\\ *Hint: Use one of the join functions (e.g., `dplyr::left_join()`).* # 3. Join datasets by &#39;Run&#39; merged_data &lt;- bioreactor_wide %&gt;% left_join(experiment, by = &quot;Run&quot;) 5.7.2.5 4. Grouping and Summarizing - Group the merged dataset by the `Feed` type. - Calculate summary statistics (e.g., mean and standard deviation, or min and max) for key measurements such as `Temp`, `pH`, and `Substrate` across all time points and runs for each feed type.\\ *Hint: Use `dplyr::group_by()` and `dplyr::summarize()`.* # 4. Group and summarize by &#39;Feed&#39; summary_stats &lt;- merged_data %&gt;% group_by(Feed) %&gt;% summarize(mean_Temp = mean(Temp, na.rm = TRUE), sd_Temp = sd(Temp, na.rm = TRUE), mean_pH = mean(pH, na.rm = TRUE), sd_pH = sd(pH, na.rm = TRUE), max_Subs = max(Substrate, na.rm = TRUE), min_Subs = min(Substrate, na.rm = TRUE)) # View summary statistics print(summary_stats) ## # A tibble: 2 × 7 ## Feed mean_Temp sd_Temp mean_pH sd_pH max_Subs min_Subs ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Glucose 30.7 0.488 7.08 0.0837 1.5 0.8 ## 2 Glycerol 30.2 0.488 6.88 0.0837 1.1 1 Interpreting the Results Discuss how different feed types glucose vs glycerol may be affecting the reactor conditions based on your summary statistics. What potential insights could an engineer draw from these analyses? 5.8 Root Growth Inhibition Example 5.8.1 Introduction In my research group we study plant genes and how they function. One interesting thing about plants is that their genomes are quite large compared to other organisms and one of the reasons for that is their genomes have been duplicated several times throughout the history of their evolution gnome duplication These genome duplications have generated families of genes which have similar but slightly different functions. Often we think about each of these genes in a family as playing a distinct role, and by adding together their function we could approximate their total function. However that’s not always the case because these genes might interact with one another—directly through binding or indirectly through competition—to perform their total function. These interactions might cause some deviation from our additive model of function; in genetics this is called epistasis. This dataset is from Prigge et al. 2020, who collected measurements of several phenotypes for combinations of mutants in the TIR1/AFB auxin receptor genes which my group studies and engineers. 5.8.2 Read in data root_growth_inh_0 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh EtOH&quot;, skip = 1) root_growth_inh_20 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh 20 nM&quot;, skip = 1) root_growth_inh_100 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh 100 nM&quot;, skip = 1) root_growth_inh_500 &lt;- read_xlsx(&quot;data/elife-54740-fig1-figsupp3-data1-v2_data-only_tir1afb1.xlsx&quot;, sheet = &quot;Primary Root Growth Inh 500 nM&quot;, skip = 1) 5.8.3 Write a function for tidying this data root_growth_plonger &lt;- function(data){ data |&gt; pivot_longer(cols = -line, names_to = &quot;genotype&quot;, values_to = &quot;root_growth_mm&quot;) %&gt;% mutate(batch = str_extract(genotype, &quot;\\\\s(.*)$&quot;)) %&gt;% mutate(batch = str_remove_all(batch, &quot;\\\\s&quot;)) %&gt;% mutate(genotype = str_remove(genotype, &quot;\\\\s.*&quot;)) %&gt;% mutate(batch = if_else(is.na(batch), &quot;a&quot;, batch)) %&gt;% na.omit() } 5.8.4 Tidy the individual datasets root_growth_inh_0 &lt;- root_growth_plonger(root_growth_inh_0) root_growth_inh_0$treatment &lt;- 0 root_growth_inh_20 &lt;- root_growth_plonger(root_growth_inh_20) root_growth_inh_20$treatment &lt;- 20 root_growth_inh_100 &lt;- root_growth_plonger(root_growth_inh_100) root_growth_inh_100$treatment &lt;- 100 root_growth_inh_500 &lt;- root_growth_plonger(root_growth_inh_500) root_growth_inh_500$treatment &lt;- 500 root_growth_inh &lt;- bind_rows(root_growth_inh_0, root_growth_inh_20, root_growth_inh_100, root_growth_inh_500) root_growth_inh %&gt;% group_by(batch) %&gt;% mutate(percent_inh = root_growth_mm / mean(root_growth_mm[genotype == &quot;Col-0&quot; &amp; treatment == 0]) * 100) -&gt; root_growth_inh root_growth_inh_aov &lt;- aov(percent_inh ~ genotype*treatment + batch, data = root_growth_inh) summary(root_growth_inh_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 4 4807 1202 1.865 0.11536 ## treatment 1 423486 423486 657.249 &lt; 2e-16 *** ## batch 2 7356 3678 5.708 0.00355 ** ## genotype:treatment 4 31237 7809 12.120 2.16e-09 *** ## Residuals 479 308635 644 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 root_growth_inh$fac_treat &lt;- as.factor(root_growth_inh$treatment) root_growth_inh_aov &lt;- aov(percent_inh ~ genotype*fac_treat + batch, data = root_growth_inh) summary(root_growth_inh_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## genotype 4 4807 1202 10.951 1.69e-08 *** ## fac_treat 3 708852 236284 2153.098 &lt; 2e-16 *** ## batch 2 1084 542 4.939 0.00754 ** ## genotype:fac_treat 9 8981 998 9.093 9.95e-13 *** ## Residuals 472 51798 110 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 root_growth_inh_aov.HSD &lt;- broom::tidy(TukeyHSD(root_growth_inh_aov)) root_growth_inh_aov.HSD[which(root_growth_inh_aov.HSD$adj.p.value &lt; 0.05),] ## # A tibble: 121 × 7 ## term contrast null.value estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 genotype tir1-1-… 0 6.55 3.47 9.62 1.04e- 7 ## 2 genotype tir1-10… 0 7.57 3.42 11.7 7.93e- 6 ## 3 genotype tir1afb… 0 6.41 1.11 11.7 8.76e- 3 ## 4 fac_treat 20-0 0 -36.5 -40.2 -32.9 2.81e-11 ## 5 fac_treat 100-0 0 -75.8 -78.9 -72.7 2.81e-11 ## 6 fac_treat 500-0 0 -93.9 -97.4 -90.5 2.81e-11 ## 7 fac_treat 100-20 0 -39.3 -42.9 -35.6 2.81e-11 ## 8 fac_treat 500-20 0 -57.4 -61.4 -53.4 2.81e-11 ## 9 fac_treat 500-100 0 -18.2 -21.6 -14.7 2.81e-11 ## 10 genotype:fac_tre… tir1-10… 0 15.4 2.19 28.6 6.04e- 3 ## # ℹ 111 more rows Looks like there are no strong batch effects here. root_growth_inh %&gt;% filter(genotype != &quot;tir1-10&quot;) %&gt;% filter(treatment %in% c(0,100)) %&gt;% mutate(TIR1 = !str_detect(genotype, &quot;tir1&quot;), AFB1 = !str_detect(genotype, &quot;afb1&quot;)) -&gt; root_growth_inh For now let’s look at the whole dataset. root_growth_inh.int &lt;- aov(percent_inh ~ TIR1*AFB1*treatment, data = root_growth_inh) summary(root_growth_inh.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 6758 6758 48.681 2.69e-11 *** ## AFB1 1 470 470 3.386 0.06692 . ## treatment 1 369212 369212 2659.748 &lt; 2e-16 *** ## TIR1:AFB1 1 3158 3158 22.747 3.14e-06 *** ## TIR1:treatment 1 4190 4190 30.184 9.66e-08 *** ## AFB1:treatment 1 1456 1456 10.485 0.00137 ** ## TIR1:AFB1:treatment 1 7 7 0.047 0.82862 ## Residuals 250 34704 139 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 root_growth_inh.int &lt;- aov(percent_inh ~ TIR1*AFB1*fac_treat, data = root_growth_inh) summary(root_growth_inh.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 6758 6758 48.681 2.69e-11 *** ## AFB1 1 470 470 3.386 0.06692 . ## fac_treat 1 369212 369212 2659.748 &lt; 2e-16 *** ## TIR1:AFB1 1 3158 3158 22.747 3.14e-06 *** ## TIR1:fac_treat 1 4190 4190 30.184 9.66e-08 *** ## AFB1:fac_treat 1 1456 1456 10.485 0.00137 ** ## TIR1:AFB1:fac_treat 1 7 7 0.047 0.82862 ## Residuals 250 34704 139 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ggplot(data = root_growth_inh, aes(x = fac_treat, y = percent_inh, color = genotype)) + geom_boxplot() + geom_point(position = position_jitterdodge(jitter.width = 0.2)) Strong case for an interaction, but not dependent on treatment, interesting. To make this easier to explain we can repeat this analysis after stratifying by treatment. root_growth_inh0.int &lt;- aov(percent_inh ~ TIR1*AFB1, data = filter(root_growth_inh, treatment == 0)) summary(root_growth_inh0.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 43 42.5 0.26 0.611331 ## AFB1 1 2030 2030.3 12.40 0.000598 *** ## TIR1:AFB1 1 1700 1700.0 10.38 0.001619 ** ## Residuals 127 20800 163.8 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This matches expectations mostly, I would have expected AFB1 to have less effect, but in this model the double mutant is also contributing to this effect size. The interaction here is still a strong effect. 100 nM should be more telling even. root_growth_inh100.int &lt;- aov(percent_inh ~ TIR1*AFB1, data = filter(root_growth_inh, treatment == 100)) summary(root_growth_inh100.int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## TIR1 1 9623 9623 85.129 9.93e-16 *** ## AFB1 1 84 84 0.746 0.38945 ## TIR1:AFB1 1 1407 1407 12.445 0.00059 *** ## Residuals 123 13903 113 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Very interesting, TIR1 has a very strong effect as expected at this high auxin concentration, whereas AFB1 does not, but the interaction term is significant at both 0 nM and 100 nM IAA. "],["data-visualization.html", "6 Data Visualization 6.1 Reading (complete by class on Monday) 6.2 Tutorials to walk through on your own 6.3 More on Color Palettes", " 6 Data Visualization This week, we’ll start digging into visualizations of the data we have been wrangling. I should rephrase this - we are really only going to scratch the surface of visualizations. Figure from https://callingbullshit.org/tools/tools_misleading_axes.html, original source Swanson et al. (2014). Data visualizations are all around us. From what we read in the popular press to how we tackle problems within Biological Systems Engineering. In this unit, we’re going to focus on making some simple visualizations within RStudio. Prior to doing this, I wanted to begin with some tips to consider in preparing your own visualizations and spotting data visualizations that may inadvertently (or purposely) mislead the viewer. For example, the plot above shows the incidence of thyroid cancer with respect to time, and insinuates that glyphosate (Roundup) correlates to the rising rates of thyroid cancer. Key: Correlation is not causation. What else is wrong with the figure - specifically the secondary y-axis for glyphosate applied? Answer: you can’t have a negative value of glyphosate applied! Here, the authors adjusted the secondary y-axis scale so the red line followed thyroid cancer, which is clearly misleading. This is just one example that Bergstrom and West use in their recent book “Calling Bullshit”. In the table below are key points they implore us to learn and consider as we evaluate visualizations and make our own: CALLING BULLSHIT - TOP ISSUES WITH PLOTS Why? 1. Bar chart axes should include zero. Size gaps can mislead interpretation, and bar graphs meant to look at absolute magnnitude. [visual weight of each bar = value of bar, or proportional ink] 2. Line plots need not include zero. Line graphs emphasize the change in the dependent variable as the independent variable changes. 3. Multiple axes on a single graph Correlation is not causality! 4. Axis should not change scale mid-stream. Clearly can mislead! Learning Objectives: Identify and avoid misleading plots Become familiar with types of visualizations Effectively map data values into quantifiable features of the resulting graphic: these are called aesthetics. Practice basic plotting within R using ggplot 6.1 Reading (complete by class on Monday) This week we will use a resource developed by Clause Wilke, who wrote the book Fundamentals of Data Visualization. We’ll also start by looking at a section of another book by Carl Bergstrom and Jevin West - Calling Bullshit. Calling Bullshit page devoted to Visualizations. Here, read through this page to identify the common pitfalls associated with misleading plots. Correlation does not imply causation (be able to describe what this means) Rule of proportional ink Why a 0-axis for bar graphs, but not when plotting 2 lines on a x-y scatterplot? The below also have LearnR interactive examples you can find in the Files pane (files 06-0-1, 06-0-2, 06-0-3). You can Mapping data. “Whenever we visualize data, we take data values and convert them in a systematic and logical way into the visual elements that make up the final graphic. Even though there are many different types of data visualizations, and on first glance a scatter plot, a pie chart, and a heatmap don’t seem to have much in common, all these visualizations can be described with a common language that captures how data values are turned into blobs of ink on paper or colored pixels on screen.” The key insight is the following: All data visualizations map data values into quantifiable features of the resulting graphic. We refer to these features as aesthetics. Wilke’s cliffnote slides are here (optional). Types of visualizations. Here, you’ll see examples of different visualizations that are useful in our field: amounts, distributions, proportions, x-y scatterplots, uncertainty, and geospatial data. Visualizing distributions. We’ll follow this up with an exercise from C. Wilke. Optional reading/resource: Visualization chapter in R for Data Science. 6.2 Tutorials to walk through on your own Here, the idea is to reinforce the readings, and prepare you for success in the assignment. There are 3 exercises within the Posit workspace for this week. These are built using the learnr package which makes websites that have r code chunks in them so you can practice coding, in a cleaner and easier environment than Posit. You can also Dr. Scott previously created a short video for each that may be helpful: Exercise 1, Exercise 2, and Exercise 3. I am not going to expect any of you to memorize these functions and approaches; rather, I want you to be able to consider what type of visualization you can use, and have the background to dive into creating your own visualization using aesthetics and geoms. This will require you to do some reading in the help documentation of these different functions and refer back to the readings. From my perspective, simple, legible visualizations are best. Regardless, the exercises below will give you a great jumping off point. Aesthetics Exercise 1 - see Wilke’s slides here. Use this to try yourself; the solution for each follows. The point is to learn how the data is “mapped” - the aesthetics. Amounts Exercise 2 - see Wilke’s slides here. Again, this is for you to apply and practice, building on aesthetics but with bar data. Distributions Exercise 3 - see Wilke’s slides here. Lastly, this series highlights approaches to show distributions of data. 6.3 More on Color Palettes Also included in this weeks materials is a vignette from the Viridis package which has colorblind friendly color palletes. R Markdown enables you to weave together content and executable code into a finished document. To learn more about R Markdown see rmarkdown.rstudio.org. This template uses R Markdown to demonstrate the color palettes of R’s Viridis package in three mediums: as an HTML, PDF, or Word document in colors_document.Rmd as a slide deck in colors_presentation.Rmd as a web page with interactive Shiny components in colors_app.Rmd 6.3.1 Previewing 6.3.1.1 To preview the document Open the file colors_document.Rmd. Then click the Knit button that will appear above the opened file. This will display the document as an HTML file. To display the document as a pdf or MS Word file, click the drop down menu to the left of the Knit icon and select one of: Knit to PDF Knit to Word 6.3.1.2 To preview the presentation Open the file colors_presentation.Rmd. Then click the Knit button that will appear above the opened file. This will display the document as an ioslides HTML slide deck, which can be presented with any web browser. To display the presentation as a Slidy (HTML), beamer (PDF), or MS PowerPoint slide deck, click the drop down menu to the left of the Knit icon and select one of: Knit to HTML (Slidy) Knit to PDF (Beamer) Knit to PowerPoint 6.3.1.3 To preview the interactive document with Shiny components Open the file 06-0-4_colors_app.Rmd. Then click the Run Document button that will appear above the opened file. Because the file contains the YAML line runtime: shiny, R Markdown will run the file as an interactive Shiny app. "],["data-visualization-examples.html", "Data Visualization Examples 6.4 Background 6.5 Data Description 6.6 Tasks 6.7 Setup for the Homework", " Data Visualization Examples library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.4 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors 6.4 Background Understanding how bird populations vary across different habitats and seasons is an important aspect of ecological research. In this homework, you will explore data collected from bird surveys conducted in three habitat types—Forest, Grassland, and Wetland—over four seasons. The dataset includes both the total bird count and the species richness (i.e., the number of different species observed) for each survey. 6.5 Data Description You are provided with a CSV file named bird_survey_data.csv that contains the following columns: Season: The season when the survey was conducted (e.g., “Spring”, “Summer”, “Fall”, “Winter”). Habitat: The habitat type where the survey was carried out (“Forest”, “Grassland”, or “Wetland”). Bird_Count: The total number of birds counted during the survey. Species_Richness: The number of different bird species observed. 6.6 Tasks Data Import &amp; Preparation: Write an R script that reads the CSV file (birds.csv) into a data frame. birds &lt;- read_csv(&quot;data/birds.csv&quot;) ## Rows: 12 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Season, Habitat ## dbl (2): Bird_Count, Species_Richness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. - Ensure the data types are correctly set (e.g., `Season` and `Habitat` should be treated as factors with a logical ordering for `Season`). Basic Visualization: Use ggplot2 to create a grouped bar plot of Bird_Count versus Season. Differentiate the three habitat types using different fill colors. Adjust the position so that bars for each season are grouped side by side. ggplot(data = birds, mapping = aes(x = Habitat, y = Species_Richness, fill = Season, group = Season)) + labs(x = &quot;Habitat type&quot;) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + scale_fill_brewer(palette = 4) Enhanced Plot Features: Customize your plot with informative axis labels (e.g., “Season” and “Bird Count”), a descriptive title, and a clear legend. Apply an appropriate theme (e.g., theme_minimal() or theme_classic()). Optional Extension: Create a second plot visualizing Species_Richness versus Season using a similar bar plot. Alternatively, explore using facets (with facet_wrap()) to compare both Bird_Count and Species_Richness across the different habitats in one multi-panel figure. 6.7 Setup for the Homework Video introduction / help for homework Hand in: a single pdf document with part 1 and part 2. Be sure that you explain why you chose the particular plots, and that the plots contain proper axis labels, formatting, etc. Part 1. Dr. Senger is a Metabolic Engineer in our department; perhaps you’ll have him or had him for Thermo. His work includes developing biosensors, and he has a recent publication in Pubmed (see here). Imagine you’re an undergrad researcher in Dr. Senger’s group, and are asked to recreate Figure 2a from the publication within Rstudio. The raw data for the figure is available in the supplemental information, and in the Rstudio workspace for this week as an excel file (file contains data for all figures). Follow examples for bar graphs from the exercises (visualizing amounts). You’ll want your final bar graph to look similar to this one, but you can choose the theme/color scheme. Below, I used theme_economist(). Part 2. Dr. Shortridge is a Hydrologist in our department, and her work includes analyzing large hydrologic datasets. Here, imagine you’re asked to compare approaches for visualizing the distribution of monthly streamflow in 2020 for the Rappahannock River at Fredericksburg. Based on the distribution exercise, create a boxplot, violin plot, strip-jitter plot, and ridge plot. See the examples below for good and bad plots; you’ll want your plots to be “good”. Choose one figure that you like the best, and provide an explanation why. Axis labels are just abbreviations and are missing units data does not make sense (e.g. not grouped by month) Ticks for y-axis are not between 0.1 - 100. Used simple theme in cowplot package Used x-variable in factor form (by using month function within lubridate and include label and abbr; type ?month) Normalized y-axis to 1000 :: aes (x = xvar, y =yvar/1000) Added group attribute in aes statement Added theme (e.g. theme_cowplot). Check out the cowplot package and ggthemes. There are many to choose from! Why do I like the theme above? Simple, axis labels slightly larger than tick labels, clear. Here’s another example using theme_economist. When you create a plot, assign it a variable. Then you can easily change themes/attributes of the plot. For your ridge-line plot, choose a fill color that you like. You add the fill color within the geom_density_ridges function (e.g. geom_density_ridges(rel_min_height = 0.01,fill=\"dodgerblue2\"). A list of colors can be found here: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf "],["assessment-1.html", "7 Assessment 1 7.1 Directions 7.2 Finding a data set 7.3 Questions/Tasks 7.4 Guideline on the Use of AI Tools", " 7 Assessment 1 In today’s data-driven world, the ability to work with real-world datasets and draw meaningful insights is an essential skill for engineers and scientists. This assessment is designed to give you hands-on experience in exploring, cleaning, transforming, and visualizing data in R. Unlike traditional problem sets, this project empowers you to choose a dataset that aligns with your interests—whether related to health, environment, agriculture, or another topic—and apply the data wrangling and visualization skills you’ve developed in class. The goal of this project is to help you move beyond following code examples to thinking critically and creatively about data: formulating your own questions, designing a strategy to answer them, and communicating your results effectively. Along the way, you’ll also reflect on the role of emerging tools like AI in the data analysis process, learning to use them thoughtfully and responsibly. By the end of this assessment, you should be able to: Identify and obtain a relevant and well-structured dataset Tidy and transform data using tools from the tidyverse Pose and answer meaningful questions through summary statistics and visualizations Document and reflect on your data analysis process, including any use of AI tools This is an opportunity to demonstrate not only your technical skills, but your curiosity, creativity, and ability to engage with data in a real-world context. You can download the project template here: https://github.com/VT-BSE3144/07_Assessment1/blob/main/Assessment1.Rmd 7.1 Directions In this project-based assessment you will identify a data set you are interested in and demonstrate your skills of data wrangling and visualization through asking questions about the dataset. 7.2 Finding a data set Data is everywhere these days. Below we have some recommendations about where to find datasets to use for this assessment, but there are many more options out there, from different governmental agencies, to research papers, to data repositories. Google a topic you are interested in plus “data” and see what you can find, or check out the resources below. The critical things a dataset must have for this assessment are: ability to be downloaded (as a csv/tsv/excel/sas/stata/spss file ideally) at least 1 numerical variable at least 1 categorical variable something that you care about, so you are willing to put some time into this assessment 7.2.1 General https://data.world/datasets - data.world has thousands of data sets on all kinds of different topics that are all open and freely available, but you have to make an account. https://datadryad.org/stash - This site is a repository for all kinds of research data, use the search tool to find something you are interested in. https://udc.vt.edu/ - Virginia Tech’s University data commons has all kinds of data about our campus community over time that can be visualized in different ways and downloaded as CSVs. https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html - The R datasets package has lots of different data sets on a wide variety of topics https://www.vdh.virginia.gov/data/ - Virginia Department of Health has lots of datasets, mostly health related but also some environmental datasets. 7.2.2 Health Safe Drinking Water Information System (SDWIS) - data on centralized water service design and violations throughout the United States. (https://www.epa.gov/ground-water-and-drinking-water/safe-drinking-water-information-system-sdwis-federal-reporting) https://www.who.int/data - The World Health organization has lots of very interesting datasets that are pretty accessible. We would recommend in particular, Child mortality and causes of death and Life expectancy data by country. https://healthdata.gov/ - This site is dedicated to making high value health data more accessible to entrepreneurs, researchers, and policy makers in the hopes of better health outcomes. https://www.nyam.org/library/collections-and-resources/data-sets/ - The New York Academy of Medicine has links to many publicly available, medical data sets. https://www.cdc.gov/datastatistics/index.html - CDC has many datasets, some may be difficult to access or read into R, as they often have their own data visualization tools, but with some digging you can find the raw datasets. 7.2.3 Environment https://echo.epa.gov/ - USEPA Enforcement and Compliance System (ECHO) - all permitted releases to surface waters in the United States. https://vtstreamlab.weebly.com/live-data.html - StREAM Lab - real-time monitoring of water quantity and quality at Stroubles Creek. https://www.deq.virginia.gov/topics-of-interest/per-and-polyfluoroalkyl-substances-pfas - VADEQ PFAS monitoring data - Forever chemicals PFAS, perfluoroalkyl substances, measured in water sources throughout Virginia. https://waterdata.usgs.gov/nwis - These pages provide access to water-resources data collected at approximately 1.9 million sites across the US and its territories. https://data.noaa.gov/datasetsearch/ - NOAA has many datasets related to the environment from weather and water, to ecology and environmental health. These are generally pretty accessible too. 7.2.4 Agriculture https://datl-chandel.github.io/Agroclimate/ - Agroclimate Viewer &amp; Planner App - This tools enables monitoring crop health (from satellite imagery), weather history and 16 day forecast, and soil properties. https://data.nal.usda.gov/ - The USDA has a large collection of agriculturally relevant data sets. 7.3 Questions/Tasks Any text that is flanked by two asterisk’s ** (which causes text to appear in bold face in the “Visual” version of the document) is a prompt for you to answer or fill in details below. Outside of code chunks, this text will appear as bold. We have still added the ** inside of comments in code chunks so you will know where you need to fill in. 7.4 Guideline on the Use of AI Tools In this assignment, you are welcome to use AI tools (such as ChatGPT, Claude, Copilot, automated data cleaning packages, etc) to support your brainstorming, code generation, data wrangling, or visualization tasks. However, it is essential that you use these tools critically and responsibly. Specifically consider: Verification and Validation: Always verify and validate any output generated by AI tools. Ensure that the code and analysis you submit is accurate, reproducible, and truly reflects your own understanding. Documentation: Clearly document any instance where AI tools were used. Describe the process, the suggestions provided by the AI, and any modifications you made to tailor the output to your project’s needs. Ethical Considerations: Reflect on and acknowledge the benefits and limitations of using AI. Consider issues such as potential biases in AI-generated outputs, the transparency of the process, and the ethical implications of relying on automated tools in data analysis. By adhering to these guidelines, you will demonstrate both technical proficiency and a critical, ethical approach to the integration of AI in your data analysis process. "],["introduction-to-linear-regression-in-r.html", "8 Introduction to Linear Regression in R 8.1 Objectives 8.2 Reading 8.3 Active Package Libraries 8.4 Linear Regression 8.5 Now, let’s say you want to predict a y-value for a given age. 8.6 Example 2 from reading", " 8 Introduction to Linear Regression in R 8.1 Objectives This week our goals are to be able to: Linear Regression Fundamentals: Understand the basics of linear regression analysis in R. Learn how to interpret model output and assess model performance. Assumptions and Diagnostics: Identify and understand the assumptions of linear regression. Gain proficiency in diagnosing model assumptions using plots and tests. Workflow and Data Preparation: Develop a structured workflow for conducting linear regression analysis. Learn techniques for data preparation and visualization. Data Transformation and Model Improvement: Explore methods for transforming data to meet model assumptions. Understand how data transformations can enhance model fit. Interpretation of Diagnostic Plots: Learn to interpret diagnostic plots to assess model adequacy. Prediction and Inference: Use regression models for prediction and inference. Understand how to compute and interpret confidence and prediction intervals. 8.2 Reading Chapter 3 A Review of R Modeling Fundamentals from Tidy Modeling with R by Max Kuhn and Julia Silge 8.3 Active Package Libraries This tutorial is derive from https://www.datacamp.com/community/tutorials/linear-regression-R library(ggplot2) library(cowplot) library(readxl) 8.4 Linear Regression R-studio provides the ability to create linear regressions easily, sometimes too easily. While this is not meant to be a substitute for a statistics course, the objective in this short tutorial is to develop a workflow approach that allows you to test the validity of regressions. 8.4.1 Assumptions Below is a figure from https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf. alt text here 8.4.2 Workflow: Read in data Plot data &amp; visualize linearity Transform data as appropriate Create linear model using lm function. Assess assumption 1 review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior Test for homoscedasticity Assess assumption 3, the variability in the residuals does not vary over the range of predicted values if fails, transform data or choose an alternate model/independent variable Test for bias Assess assumption 4, e values generally plot equally above and below zero Test for normality 8.4.3 Example 8.4.3.1 Step 1. Read in data ageandheight &lt;- read_excel(&quot;data/ageandheight.xls&quot;,sheet=&quot;Hoja2&quot;) ## There is one data point that does not read in correctly, thus the statement below corrects for this. ageandheight$height[7] &lt;- 79.9 8.4.3.2 Step 2. Plot data &amp; visualize linearity ## here you can either start a ggplot or just use the simple plot command. Since we&#39;re practicing ggplot, let&#39;s stick with this. p &lt;- ggplot(ageandheight, aes(age,height)) + geom_point() + cowplot::theme_cowplot() + ## adds theme scale_y_continuous(breaks=seq(76,84,2)) + ## changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(16,31,2)) + ylab(&quot;Height [cm]&quot;) + ## adds y-label with units xlab(&quot;Age [months]&quot;) p Check: The resulting plot looks fairly linear; let’s proceed! 8.4.3.3 Step 3. Transform data ## not required here; if required, repeat step 2. 8.4.3.4 Step 4. Linear model Create linear model using the lm function. - Assess assumption 1 - review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. - review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior ## Create linear model model.lm &lt;- lm(height~age, data=ageandheight) summary(model.lm) ## ## Call: ## lm(formula = height ~ age, data = ageandheight) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.27238 -0.24248 -0.02762 0.16014 0.47238 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 64.9283 0.5084 127.71 &lt; 2e-16 *** ## age 0.6350 0.0214 29.66 4.43e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.256 on 10 degrees of freedom ## Multiple R-squared: 0.9888, Adjusted R-squared: 0.9876 ## F-statistic: 880 on 1 and 10 DF, p-value: 4.428e-11 Check: |t-values| &gt;&gt; 2, proceed ## review leverage/influence of data points on regression. Use plot of Cook&#39;s D, evaluate subset of Cooks D above threshold, and evaluate DFFITS (another influence diagnostic) plot(cooks.distance(model.lm), pch = 16, col = &quot;blue&quot;) #Plot the Cooks Distances. There are a few high points here at the beginning. Let’s see if any fall outside of the critical value on the F-distribution (the qf function determines the critical value for our number of observations and number of coefficients). n &lt;- length(model.lm$residuals) ## n = the number of observations p &lt;- length(model.lm$coefficients) # p = the number of coefficients subset(cooks.distance(model.lm), cooks.distance(model.lm) &gt; qf(0.1, p, n - p, lower.tail = FALSE)) # determines if there are any flagged observations from Cooks D ## named numeric(0) For SLR (simple linear regression) with more than about 30 observations, the critical value for D would be about 2.4. So we don’t get any values out, hence the named numeric(0), zero observations were flagged. What about DFFITS (difference in fits with and without that point)? subset(dffits(model.lm), dffits(model.lm) &gt; 2 * sqrt(p / n)) # determines if there are any flagged observations from DFFITS ## 3 ## 1.127423 Now, observation 3 was identified as having higher influence on the fit than other points. Consider options 1-3 described in workflow. Is there something wrong with this point? 8.4.3.5 Step 5. Test for homoscedasaticity # Here, the which variable provides the ability to create 4 plots of interest: &quot;Residuals vs Fitted&quot;, &quot;Normal Q-Q&quot;, &quot;Scale-Location&quot;, &quot;Cook&#39;s distance&quot;, &quot;Residuals vs Leverage&quot; # To test for homoscedasticity, review plot of standardized residuals plot(model.lm, which = 3, ask = FALSE) Check: Variability is not significant over fitted values 8.4.3.6 Step 6. Test for bias # To test for bias, review plot of residuals plot(model.lm, which = 1, ask = FALSE) Check: Variability above and below 0 is similar without a distinct pattern 8.4.3.7 Step 7. Test for Normality # To test for normality, review plot plot(model.lm, which = 2, ask = FALSE) Check: Most points (with exception of observation 1) fall on the line, suggesting a normal distribution of residuals 8.4.4 Predicting values: Applications of linear models The following workflow provides predicted values and confidence intervals of these estimates for new values based on a linear regression model. The final steps are to create a plot with uncertainty bounds and the ability to predict a value and associated uncertainty in that predicted value. 8.4.4.1 Workflow: Confidence intervals are computed using the predict command: predict(lmheight, newdata = data.frame(age=22.5), interval = &quot;confidence&quot;, level = 0.95) Prediction intervals are computed as follows: predict(lmheight, newdata = data.frame(age=22.5), interval = &quot;prediction&quot;, level = 0.95) Prediction intervals are always greater than confidence intervals. While they include the uncertainty in the regression coefficients, the slope and intercept, they also includes the unexplained variability in y within the original data. 8.4.4.2 Example # Use model to create prediction intervals model.predict &lt;- predict(model.lm, interval = &quot;predict&quot;) ## Warning in predict.lm(model.lm, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses # Use model to create confidence intervals model.confidence &lt;- predict(model.lm, interval = &quot;confidence&quot;) colnames(model.confidence) &lt;- c(&quot;cfit&quot;, &quot;clwr&quot;, &quot;cupr&quot;) #rename columns # Create dataset that merges dataset data.all &lt;- cbind(ageandheight,model.predict, model.confidence) # Create ggplot p &lt;- ggplot(data.all, aes(x = age, y = height)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;Height [cm]&quot;) + xlab(&quot;Age [months]&quot;) + scale_y_continuous(breaks=seq(76,84,2)) + scale_x_continuous(breaks=seq(16,31,2)) p The resulting plot contains the confidence and prediction intervals over the range of x-values. 8.5 Now, let’s say you want to predict a y-value for a given age. When making predictions, you’ll want to use predict and not confidence. The rationale is that this approach provides a better sense of incorporating not just the confidence in the intercept and slope, but also the unexplained variation in the y-values. a &lt;- data.frame(&quot;age&quot; = 18.1) # key here is to label column name the same as what is used in the model.lm! value.predict &lt;- predict(model.lm, newdata=a, interval = &quot;predict&quot;, level = 0.95) value.predict ## fit lwr upr ## 1 76.42119 75.77412 77.06826 Thus, for an age of 18.1 months, the predicted height is 76.4 (75.77 - 77.1, alpha = 95%).’ 8.6 Example 2 from reading # Read in data press &lt;- read_excel(&quot;data/pressure.xlsx&quot;) # Plot data p &lt;- ggplot(press,aes(Temperature,Pressure)) + geom_point() + geom_smooth(method = &quot;lm&quot;, level = 0.95) p ## `geom_smooth()` using formula = &#39;y ~ x&#39; What do you notice??? Are the residuals going to be random? Does negative pressure make sense? lmTemp = lm(Pressure~Temperature, data = press) #Create the linear regression plot(lmTemp$residuals, pch = 16, col = &quot;red&quot;) So what to do? Transformation!We will learn more about these on Wednesday. press$x2 &lt;- press$Temperature^2 lmTemp2 = lm(Pressure~Temperature + I(Temperature^2), data = press) #Create the linear regression plot(lmTemp2$residuals, pch = 16, col = &quot;red&quot;) summary(lmTemp2) ## ## Call: ## lm(formula = Pressure ~ Temperature + I(Temperature^2), data = press) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.6045 -1.6330 0.5545 1.1795 4.8273 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 33.750000 3.615591 9.335 3.36e-05 *** ## Temperature -1.731591 0.151002 -11.467 8.62e-06 *** ## I(Temperature^2) 0.052386 0.001338 39.158 1.84e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.074 on 7 degrees of freedom ## Multiple R-squared: 0.9996, Adjusted R-squared: 0.9994 ## F-statistic: 7859 on 2 and 7 DF, p-value: 1.861e-12 # plot fitted smooth line lmTemp2plot&lt;- data.frame(lmTemp2$fitted.values,press$Temperature) p &lt;- p + geom_line(data = lmTemp2plot, aes(x = press.Temperature,y=lmTemp2.fitted.values),color = &#39;#E51837&#39;) p ## `geom_smooth()` using formula = &#39;y ~ x&#39; "],["linear-regression-examples.html", "Linear Regression Examples 8.7 Active Libraries 8.8 Linear Regression 8.9 Assumptions 8.10 Example 8.11 Take 2, try second order!", " Linear Regression Examples This tutorial is derived from https://www.datacamp.com/community/tutorials/linear-regression-R 8.7 Active Libraries library(ggplot2) library(cowplot) library(readxl) 8.8 Linear Regression R-studio provides the ability to create linear regressions easily, sometimes too easily. While this is not meant to be a substitute for a statistics course, the objective in this short tutorial is to develop a workflow approach that allows you to test the validity of regressions. 8.9 Assumptions Below is a figure from https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf. alt text here 8.9.0.1 Workflow: Read in data Plot data &amp; visualize linearity Transform data as appropriate Create linear model using lm function. * Assess assumption 1 review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior Test for homoscedasaticity * Assess assumption 3, the variability in the residuals does not vary over the range of predicted values if fails, transform data or choose an alternate model/independent variable Test for bias * Assess assumption 4, e values generally plot equally above and below zero Test for normality 8.10 Example 8.10.1 Step 1. Read in data S &lt;- c(1.3, 1.8, 3, 4.5, 6, 8, 9) v &lt;- c(.07, .13, .22, .275, .335, .35, .36) data.ex &lt;- data.frame(S,v) 8.10.2 Step 2. Plot data &amp; visualize linearity ## here you can either start a ggplot or just use the simple plot command. Since we&#39;re practicing ggplot, let&#39;s stick with this. p &lt;- ggplot(data.ex, aes(S,v)) + geom_point() + cowplot::theme_cowplot() + # adds theme scale_y_continuous(breaks=seq(0,0.8,.2)) + # changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(0,9,1)) + ylab(&quot;v&quot;) + # adds y-label with units xlab(&quot;[S]&quot;) p Check: The resulting plot does not look linear. We will compare 2 transformations: 8.10.3 Step 3. Transform data data.ex$vt &lt;- 1/data.ex$v data.ex$St &lt;- 1/data.ex$S # TODO remove 2nd order here and add actual Michaelis-menten equation and actual linearization examples data.ex$S2t &lt;- 1/data.ex$S^2 8.10.4 Step 4. Linear model Create linear model using lm function. * Assess assumption 1 - review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. - review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior # Create linear model, 1st order Michaelis Menton model.lm &lt;- lm(vt~St, data=data.ex) summary(model.lm) ## ## Call: ## lm(formula = vt ~ St, data = data.ex) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 1.47839 -1.61027 -1.11218 -0.19880 0.06114 0.61664 0.76508 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.1902 0.7762 0.245 0.816147 ## St 16.4022 1.9435 8.440 0.000383 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.185 on 5 degrees of freedom ## Multiple R-squared: 0.9344, Adjusted R-squared: 0.9213 ## F-statistic: 71.23 on 1 and 5 DF, p-value: 0.0003833 vm &lt;- 1/model.lm$coefficients[1] ks &lt;- vm*model.lm$coefficients[2] vm ## (Intercept) ## 5.256963 ks ## (Intercept) ## 86.22596 Check: |t-values| &lt; 2 for intercept, questionable. When vm is calculated, it’s a lot higher than reported velocities, and the half-saturation constant ks is higher than the maximum substrate concentration. # review leverage/influence of data points on regression. Use plot of Cook&#39;s D, evaluate subset of CooksD above threshold, and evaluate DFFITS (another influence diagnostic) plot(cooks.distance(model.lm), pch = 16, col = &quot;blue&quot;) #Plot the Cooks Distances. n &lt;- length(model.lm$residuals) p &lt;- length(model.lm$coefficients) subset(cooks.distance(model.lm), cooks.distance(model.lm) &gt; qf(0.1, p, n - p, lower.tail = FALSE)) # determines if there are any flagged observations from CooksD ## 1 ## 4.809895 subset(dffits(model.lm), dffits(model.lm) &gt; 2 * sqrt(p / n)) # determines if there are any flagged observations from DFFITS ## 1 ## 11.80479 For SLR with more than about 30 observations, the critical value for Di would be about 2.4. In the example above, observation 1 was identified as having higher influence for both Cooks and DDFITS. Consider options 1-3 described in workflow, but also intercept was questionable. 8.10.5 Step 5. Test for homoscedasaticity # Here, the which variable provides the ability to create 4 plots of interest: &quot;Residuals vs Fitted&quot;, &quot;Normal Q-Q&quot;, &quot;Scale-Location&quot;, &quot;Cook&#39;s distance&quot;, &quot;Residuals vs Leverage&quot; # To test for homoscedasaticity, review plot of standardized residuals plot(model.lm, which = 3, ask = FALSE) Check: Variability is a bit more variable. 8.10.6 Step 6. Test for bias # To test for bias, review plot of residuals plot(model.lm, which = 1, ask = FALSE) Check: Variability above and below 0 does not appear random. 8.10.7 Step 7. Test for Normality # To test for normality, review plot plot(model.lm, which = 2, ask = FALSE) Check: Residuals are not normally distributed. Questionable!! 8.10.8 Application The workflow provides confidence of a reasonable linear regression model. The final steps are to create a plot with uncertainity bounds and the ability to predict a value and associated uncertainity. ###Workflow: Confidence intervals are computed using the predict command: predict(lmheight, newdata = data.frame(age=22.5), interval = “confidence”, level = 0.95) Prediction intervals are computed as follows: predict(lmheight, newdata = data.frame(age=22.5), interval = “prediction”, level = 0.95) Prediction intervals are always greater. While it includes the uncertainity in the regression uncertainties in the slope and intercept, it also includes the unexplained variability in y. # Use model to create prediction intervals model.predict &lt;- predict(model.lm, interval = &quot;predict&quot;) ## Warning in predict.lm(model.lm, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses # Use model to create confidence intervals model.confidence &lt;- predict(model.lm, interval = &quot;confidence&quot;) colnames(model.confidence) &lt;- c(&quot;cfit&quot;, &quot;clwr&quot;, &quot;cupr&quot;) #rename columns # Create dataset that merges dataset data.all &lt;- cbind(data.ex,model.predict, model.confidence) # Create ggplot p1 &lt;- ggplot(data.all, aes(x = St, y = vt)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;1/v&quot;) + xlab(&quot;1/S&quot;) p1 data.all$fit &lt;- 1/data.all$fit data.all$lwr &lt;- 1/data.all$lwr data.all$upr &lt;- 1/data.all$upr data.all$clwr &lt;- 1/data.all$clwr data.all$cupr &lt;- 1/data.all$cupr p &lt;- ggplot(data.all, aes(x = S, y = v)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;v&quot;) + xlab(&quot;S&quot;) + scale_y_continuous(breaks=seq(0,0.8,.2)) + # changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(0,9,1)) + ylim(0,0.8) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p ## Warning: Removed 4 rows containing missing values or values outside the scale range ## (`geom_line()`). 8.11 Take 2, try second order! 8.11.1 Step 4. Linear model Create linear model using lm function. * Assess assumption 1 - review t-values from linear model summary. If the slope and intercept values have resulting |t| &gt; 2, then they are significant. - review leverage/influence of data points on regression. When data points have high leverage, one of 3 options come into play: (1) Someone made a recording error, (2) Someone made a fundamental mistake collecting the observation; or (3) The data point is perfectly valid, in which case the model cannot account for the behavior # Create linear model, 1st order Michaelis Menton plot(data.ex$S2t,data.ex$vt) model.lm &lt;- lm(vt~S2t, data=data.ex) summary(model.lm) ## ## Call: ## lm(formula = vt ~ S2t, data = data.ex) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 0.371400 -0.737166 -0.056662 0.230297 -0.002375 0.105165 0.089340 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4492 0.1877 13.05 4.71e-05 *** ## S2t 19.3760 0.7310 26.51 1.43e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.389 on 5 degrees of freedom ## Multiple R-squared: 0.9929, Adjusted R-squared: 0.9915 ## F-statistic: 702.7 on 1 and 5 DF, p-value: 1.428e-06 vm &lt;- 1/model.lm$coefficients[1] ks &lt;- sqrt(vm*model.lm$coefficients[2]) vm ## (Intercept) ## 0.408292 ks ## (Intercept) ## 2.812661 Check: |t-values| &gt;&gt; 2, looks better The half-saturation constant and vm are also consistent with the data. # review leverage/influence of data points on regression. Use plot of Cook&#39;s D, evaluate subset of CooksD above threshold, and evaluate DFFITS (another influence diagnostic) plot(cooks.distance(model.lm), pch = 16, col = &quot;blue&quot;) #Plot the Cooks Distances. n &lt;- length(model.lm$residuals) p &lt;- length(model.lm$coefficients) subset(cooks.distance(model.lm), cooks.distance(model.lm) &gt; qf(0.1, p, n - p, lower.tail = FALSE)) # determines if there are any flagged observations from CooksD ## 1 ## 9.365224 subset(dffits(model.lm), dffits(model.lm) &gt; 2 * sqrt(p / n)) # determines if there are any flagged observations from DFFITS ## 1 ## 13.90953 For SLR with more than about 30 observations, the critical value for Di would be about 2.4. In the example above, observation 1 was identified as having higher influence for both Cooks and DDFITS. Consider options 1-3 described in workflow, but also intercept was questionable. 8.11.2 Step 5. Test for homoscedasaticity # Here, the which variable provides the ability to create 4 plots of interest: &quot;Residuals vs Fitted&quot;, &quot;Normal Q-Q&quot;, &quot;Scale-Location&quot;, &quot;Cook&#39;s distance&quot;, &quot;Residuals vs Leverage&quot; # To test for homoscedasaticity, review plot of standardized residuals plot(model.lm, which = 3, ask = FALSE) Check: Variability is variable. 8.11.3 Step 6. Test for bias # To test for bias, review plot of residuals plot(model.lm, which = 1, ask = FALSE) Check: Variability above and below 0 somewhat random. 8.11.4 Step 7. Test for Normality # To test for normality, review plot plot(model.lm, which = 2, ask = FALSE) Check: Residuals are not normally distributed. Questionable. 8.11.5 Application The workflow provides confidence of a reasonable linear regression model. The final steps are to create a plot with uncertainity bounds and the ability to predict a value and associated uncertainity. ###Workflow: Confidence intervals are computed using the predict command: predict(lmheight, newdata = data.frame(age=22.5), interval = “confidence”, level = 0.95) Prediction intervals are computed as follows: predict(lmheight, newdata = data.frame(age=22.5), interval = “prediction”, level = 0.95) Prediction intervals are always greater. While it includes the uncertainity in the regression uncertainties in the slope and intercept, it also includes the unexplained variability in y. # Use model to create prediction intervals model.predict &lt;- predict(model.lm, interval = &quot;predict&quot;) ## Warning in predict.lm(model.lm, interval = &quot;predict&quot;): predictions on current data refer to _future_ responses # Use model to create confidence intervals model.confidence &lt;- predict(model.lm, interval = &quot;confidence&quot;) colnames(model.confidence) &lt;- c(&quot;cfit&quot;, &quot;clwr&quot;, &quot;cupr&quot;) #rename columns # Create dataset that merges dataset data.all &lt;- cbind(data.ex,model.predict, model.confidence) # Create ggplot p1 &lt;- ggplot(data.all, aes(x = S2t, y = vt)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;1/v&quot;) + xlab(&quot;1/S^2&quot;) p1 data.all$fit &lt;- (1/(data.all$fit)) data.all$lwr &lt;- (1/(data.all$lwr)) data.all$upr &lt;- (1/(data.all$upr)) data.all$clwr &lt;- (1/(data.all$clwr)) data.all$cupr &lt;- (1/(data.all$cupr)) p &lt;- ggplot(data.all, aes(x = S, y = v)) + geom_point() + # adds points geom_line(aes(y=lwr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + #lower prediction interval geom_line(aes(y=upr), col = &quot;coral2&quot;, linetype = &quot;dashed&quot;) + geom_ribbon(aes(ymin=clwr,ymax=cupr),alpha=0.3) + # confidence band geom_line(aes(y=fit), col = &quot;blue&quot;) + # confidence band theme_cowplot() + ylab(&quot;v&quot;) + xlab(&quot;S&quot;) + scale_y_continuous(breaks=seq(0,0.4,.1)) + # changes scale to min and max with prescribed spacing scale_x_continuous(breaks=seq(0,9,1)) + ylim(0,0.8) ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. p The resulting plot contains the confidence and prediction intervals over the range of x-values. Is transforming the data the best approach? The resulting transformation resulted in a linear model. Were the remainder of the tests valid? Not necessarily for the normality. The other approach would be to fit the data with a non-linear best-fit curve. Regardless, you can see how you’d apply linear regression to data. "],["intro-to-anova-and-non-linear-least-squares-regression.html", "9 Intro to ANOVA and non-linear least squares regression 9.1 Reading 9.2 ANOVA tests interactions between variables 9.3 Post hoc testing 9.4 Assumptions of ANOVA 9.5 Non-linear least squares", " 9 Intro to ANOVA and non-linear least squares regression This week our goals are to be able to: Conduct analysis of variance (ANOVA) for comparing means across multiple groups. Interpret the results of statistical tests and ANOVA, including understanding p-values, confidence intervals, and model summaries. Check assumptions such as normality of residuals in statistical models. Explore interaction effects in ANOVA models. Compare different models using the anova function. 9.1 Reading Read through section 3 of the Data Carpentries Statistics with R Read through Chapter 24 Non-linear regression in R here 9.2 ANOVA tests interactions between variables This lesson in basic analysis of variance focuses and extends upon an example originally published in Brady et al. 2015. Read through the introduction of this paper and come back when you’re at the “ANOVA PROPERLY TESTS GENETIC INTERACTIONS” section. ANOVA (Analysis of Variance) facilitates comparisons of quantitative outcome variables between and among different categorical, grouping variables as well as interactions between these variables. It is an extension of linear regression that allows us to compare group means in a more rigorous way than using pairwise T-tests. Let’s simulate an experiment in which we want to determine if two genes interact to contribute to a particular phenotype. These genes could be redundant or interact in some epistatic (non-linear) manner. To figure this out we will need to measure the phenotype of interest for the two single mutants, the wild type, and the double mutant. For this example, let’s use root length as the phenotype. Suppose we have two mutants in two genes that we suspect have an effect on root length, as well as a double mutant containing mutations in both genes. First, we will simulate an experiment by drawing root length values from a random normal distribution for each genotype, and then compile these values we drew into a data frame. We will pick means and standard deviations for the normal distributions from thin-air for now. Feel free to come back later and change these values to see how differences in means and standard deviations will affect the outcomes. 9.2.1 Experiment 1 # Draw 3 measurements from a random normal distribution for each plant line WT &lt;- rnorm(n = 3, mean=100, sd=10) # WT genotype M1 &lt;- rnorm(n = 3, mean=85, sd=10) # Single mutant in Gene 1 M2 &lt;- rnorm(n = 3, mean=85, sd=10) # Single mutant in Gene 2 DM &lt;- rnorm(n = 3, mean=70, sd=10) # Double mutant in Gene 1 and Gene 2 # Create a column of the simulated phenotype measurements that we randomly drew above Phen &lt;- c(WT, M1, M2, DM) Now we will create a dataframe with columns for the status of each gene, that this is the first experiment, and the measurement of phenotype. Then we will combine these columns with our root length measurements in a data frame. # Create a column for the genotype at gene 1 # 3 wild-types, 3 single mutants in Gene 1, 3 single mutants in Gene 2, 3 double mutants M1G &lt;- c(&quot;WT&quot;, &quot;WT&quot;, &quot;WT&quot;, &quot;M1&quot;, &quot;M1&quot;, &quot;M1&quot;, &quot;WT&quot;, &quot;WT&quot;, &quot;WT&quot;, &quot;M1&quot;, &quot;M1&quot;, &quot;M1&quot;) # Create a column for the genotype at gene 2 M2G &lt;- c(&quot;WT&quot;, &quot;WT&quot;, &quot;WT&quot;, &quot;WT&quot;, &quot;WT&quot;, &quot;WT&quot;, &quot;M2&quot;, &quot;M2&quot;, &quot;M2&quot;, &quot;M2&quot;, &quot;M2&quot;, &quot;M2&quot;) # Create a column for the repeat of the experiment, as we will be repeating this process several times Exp &lt;- c(&quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;, &quot;1&quot;) Rep1 &lt;- data.frame(Exp,M1G,M2G,Phen) Rep1 ## Exp M1G M2G Phen ## 1 1 WT WT 119.09990 ## 2 1 WT WT 103.43211 ## 3 1 WT WT 99.50851 ## 4 1 M1 WT 91.45467 ## 5 1 M1 WT 71.89873 ## 6 1 M1 WT 73.04681 ## 7 1 WT M2 84.24066 ## 8 1 WT M2 84.97193 ## 9 1 WT M2 83.15004 ## 10 1 M1 M2 65.84087 ## 11 1 M1 M2 84.31263 ## 12 1 M1 M2 53.13747 Note that the values you drew will be different from those above and different from everyone else. Now we also need to convert each of these independent variables from strings to factors, as required by the ANOVA function. class(Rep1$M1G) ## [1] &quot;character&quot; Rep1$M1G &lt;- factor(Rep1$M1G) Rep1$M2G &lt;- factor(Rep1$M2G) Rep1$Exp &lt;- factor(Rep1$Exp) Now that we have our data frame set up we can visualize our data and test our hypothesis. First, let’s plot these data to get an idea of what they look like #install a package from CRAN install.packages(&#39;ggplot2&#39;) #we will also need the &#39;magrittr&#39;, ‘dplyr’, &#39;forcats&#39;, &#39;car&#39;, &#39;agricolae&#39;, and &#39;plyr&#39; packages later on install.packages(&#39;magrittr&#39;) install.packages(&#39;dplyr&#39;) install.packages(&quot;forcats&quot;) install.packages(&quot;car&quot;) install.packages(&quot;agricolae&quot;) install.packages(&quot;plyr&quot;) Let’s explore this data a bit using the qplot function. #before using any package we must load it into our workspace library(&#39;ggplot2&#39;) ggplot(data = Rep1, mapping = aes(x = interaction(M1G, M2G), y = Phen, color = interaction(M1G, M2G))) + geom_point() #####???Question??? What can we say about this data? Write a quick summary of this graph and formulate a hypothesis. answer here Is one gene contributing more to the root length phenotype than another? answer here Does the double mutant have a more or less severe phenotype than the individual mutants? answer here Do you think the genes interact to contribute to the phenotype? answer here OK! Back to our analysis of variance. We want to ask whether or not the root length values for wildtype plants, the single mutants in each gene, and the double mutant all came from the same distribution. To ask this question analysis of variance, as the name implies, compares different variances in our experiment. ANOVA compares the variance between the groups we are comparing to the variance within each group. So in this case we want to compare the phenotypic variance between the means of the WT and each mutant to the phenotypic variance within each of these groups. Another way of thinking about this is that we are comparing the effect of the variable that separates the groups, to the precision of our measurment of this effect. The variance between groups is called the systematic variance and the variance within groups is called the error variance. \\[\\frac{variance\\ between}{variance\\ within} = \\frac{systematic\\ variance}{error\\ variance}\\] Let’s visually explore this ANOVA idea a bit. For each genotype we can calculate the mean phenotype and the variance around the mean. These is the “within” group variance. We’ll add means and standard error to this plot using the stat_summary function to represent “within” variance. We can use the alpha aesthetic to change the saturation of the colors to make the mean stand out a bit more and hide the raw data somewhat. We can also use the geom argument jitter in the qplot function to prevent the mean and standard error from being plotted overtop of the raw data. ggplot(data = Rep1, mapping = aes(x = interaction(M1G, M2G), y = Phen, color = interaction(M1G, M2G))) + geom_jitter( alpha = 0.7) + stat_summary(fun.data = &quot;mean_se&quot;, alpha = 1) The “between” group mean and variance is calculated using the mean for each group. Here, we’ll split up our data frame Rep1 into groups by the experiment, genotype at gene1, and genotype at gene2. Then we will calculate the mean for each of these groups. To do this we will use the dplyr package. We will also use the pipe function %&gt;% from the magrittr package. The pipe function passes the result of the code on the left side to the first argument of the code on the right side. # Load the necessary packages library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # library(magrittr) no longer necessary # Take the Rep1 dataset and split it into groups Rep1 |&gt; group_by(Exp, M1G, M2G) |&gt; # Note that we don&#39;t need to use quotes with functions from the dplyr package # Technically each of Exp, M1G, and M2G are passed to the &quot;...&quot; argument of the group_by function # We added a pipe function to the end of this line so we can pass these groups to the `summarise` function to calculation the mean of each group # We can then use the right arrow to assign this new data frame of the means to an object summarise(Phen = mean(Phen)) -&gt; means ## `summarise()` has grouped output by &#39;Exp&#39;, &#39;M1G&#39;. You can override using the ## `.groups` argument. means$M1G &lt;- &quot;mean&quot; means$M2G &lt;- &quot;mean&quot; # Create a new data frame with the mean rows combined with our data Rep1means &lt;- bind_rows(Rep1, means) ggplot(data = Rep1means, mapping = aes(x = interaction(M1G, M2G), y = Phen, color = interaction(M1G, M2G))) + geom_jitter(alpha = 0.7) + stat_summary(fun.data = &quot;mean_se&quot;, alpha = 1) So now the mean.mean column contains four points representing the means of the four groups. The mean of these means, called the grand mean, is represented by the large point and the variance of the group means about the grand mean represents the between groups variance, but here we’ve used the mean_se function to calculate standard error. The true value of the variance is the difference between each data point and the mean. We can write our own function to calculate the true variance. Don’t get bogged down in the code here (unless you want to :), just pay attention to the graphs. We can also use the forcats package to move the mean.mean column to the end of the graph and relabel each of the columns. In general the forcats package has functions that allow us to arrange factors (forcats is a rearrangement of factor, haha!). Let’s also clean up the axis labels by adding labs to the plot. library(forcats) ggplot(data = Rep1means, mapping = aes(x = fct_recode(fct_relevel(fct_rev(interaction(M1G, M2G)), &quot;mean.mean&quot;, after = Inf), double = &quot;M1.M2&quot;, mut2 = &quot;WT.M2&quot;, mut1 = &quot;M1.WT&quot;, WT = &quot;WT.WT&quot;, grand_means = &quot;mean.mean&quot;), y = Phen, color = interaction(M1G, M2G))) + geom_jitter(alpha = 0.7) + stat_summary(fun.data = function(y){ data.frame( ymin = mean(y) - sum(mean(y) - y[which(y&lt;mean(y))]), y = mean(y), ymax = mean(y) + sum(y[which(y&gt;mean(y))] - mean(y))) }, alpha = 1) + labs(x = &quot;genotype&quot;, y = &quot;root length (mm)&quot;) Now we have a pretty graph showing the mean and variance for each genotype (the first four columns) as well as the grand mean and variance (the last column). #####???Question??? How does the variance in each group compare to the variance in the grand mean? answer here Now, let’s formulate that hypothesis as a mathematical model so that we can perform the ANOVA analysis in R. Generally, we know that we have a response variable that we are measuring, that is a function of some set of predictors. \\[response \\sim f(predictors)\\] The ~ here means that this is a hypothetical or approximate relationship between the response and predictors as there are certainly other factors that we cannot predict or error that we cannot account for in our model. #####???Question??? What is the dependent variable in our simulated experiment? This is also known as the response variable and is typically continuous. answer here What are the independent or predictor variables of that response? These are typically discrete or categorical for ANOVA analysis. answer here \\[phenotype \\sim f(Gene 1, Gene2)\\] Expanding this function a bit, we can assess each individual genes functional contribution to the phenotype as well as the extent to which they interact (or interfere) with one another. Therefore, our final model that we would like to test is \\[Phen \\sim M1G + M2G + M1G:M2G\\] This is called a general linear model. This should remind you of a linear regression analysis from way back in Algebra II. We have written out an equation that we think should fit our data. ANOVA assigns the deviation or error between our model and our data into groups based on the predictors. It does this by comparing the variance between groups of predictors to the variance within groups of predictors. Now that we have our model formulated, let’s set this up in the ANOVA framework in R to finally get an answer to our question: “Are these genes interacting to regulate to root length?” ?aov Exp1 &lt;- aov(Phen ~ M1G + M2G + M1G:M2G , data=Rep1) summary(Exp1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## M1G 1 1512.3 1512.3 12.749 0.00729 ** ## M2G 1 880.4 880.4 7.422 0.02607 * ## M1G:M2G 1 111.4 111.4 0.939 0.36080 ## Residuals 8 948.9 118.6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So what does this summary tell us? Sum Sq stands for sum of squares. This is the sum of the squared deviation between the groups of the variable. Mean Sq is Sum Sq divided by the degrees of freedom or Df in that variable and is essentially the variance of that variable. The degrees of freedom are the number of groups for that variable minus one. Residuals is the total within groups variance. Now for the comparison of the variances that we’ve been talking about all along. The F value is the ratio of the Mean Sq of the variable over the Residuals. So the larger the F value the more the variance between the predictor levels dominates the variance within predictor levels (i.e. the between treatment variance is greater than the sampling variance). So for our experiment a gene with an F value much greater than one indicates that the variance among genotypes is much greater than the variance within genotypes. The Pr(&gt;F), is the probability that the data are consistent with the null hypothesis, which in this case is that the variance between genotypes of gene is not different from the variance within the genotypes. This is referred to as the p-value. So if this p-value is less than the confidence threshold you have set for your experiment (frequently 0.05), you can reject the null hypothesis and conclude that the gene has a significant effect on the phenotype. However, we cannot accept the null hypothesis for those independent variables with p-values greater than our confidence threshold!!! We can only fail to reject the null, meaning we can only say that we don’t have enough data to detect an effect on the phenotype. Chances are with this first small experiment you don’t have enough data to say anything significant about the interaction between gene1 and gene2 (i.e. we don’t have enough evidence to reject the null hypothesis). Let’s repeat the experiment! ##Experiment 2 WT &lt;- rnorm(3, mean=100, sd=10) M1 &lt;- rnorm(3, mean=85, sd=10) M2 &lt;- rnorm(3, mean=85, sd=10) DM &lt;- rnorm(3, mean=70, sd=10) Exp &lt;- c(&quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;, &quot;2&quot;) Phen &lt;- c(WT, M1, M2, DM) Rep2 &lt;- data.frame(Exp,M1G,M2G,Phen) Rep2$M1G &lt;- factor(Rep2$M1G) Rep2$M2G &lt;- factor(Rep2$M2G) Rep2$Exp &lt;- factor(Rep2$Exp) ANOVA analysis for trial 2 Exp2 &lt;- aov(Phen ~ M1G + M2G + M1G:M2G , data=Rep2) summary(Exp2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## M1G 1 349.4 349.4 10.455 0.01199 * ## M2G 1 1454.1 1454.1 43.508 0.00017 *** ## M1G:M2G 1 15.2 15.2 0.456 0.51845 ## Residuals 8 267.4 33.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #####???Question??? What can you conclude from this experiment? answer here Because all good things come in threes, and the conclusions likely differ between experiments, let’s repeat the experiment a third time. ##Experiment 3 WT &lt;- rnorm(3, mean=100, sd=10) M1 &lt;- rnorm(3, mean=85, sd=10) M2 &lt;- rnorm(3, mean=85, sd=10) DM &lt;- rnorm(3, mean=70, sd=10) Exp &lt;- c(&quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;3&quot;) Phen &lt;- c(WT, M1, M2, DM) Rep3 &lt;- data.frame(Exp,M1G,M2G,Phen) Rep3$M1G &lt;- factor(Rep3$M1G) Rep3$M2G &lt;- factor(Rep3$M2G) Rep3$Exp &lt;- factor(Rep3$Exp) ANOVA analysis for trial 3 Exp3 &lt;- aov(Phen ~ M1G + M2G + M1G:M2G , data=Rep3) summary(Exp3) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## M1G 1 1017.9 1017.9 12.222 0.00813 ** ## M2G 1 924.8 924.8 11.104 0.01035 * ## M1G:M2G 1 139.9 139.9 1.679 0.23113 ## Residuals 8 666.3 83.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ##Combined ANOVA Now we can combine the experiments and control control for trial number effects. AllExp &lt;- rbind(Rep1, Rep2, Rep3) # Notice how we add an Exp variable to the model Full &lt;- aov(Phen ~ M1G + M2G + M1G:M2G + Exp , data=AllExp) summary(Full) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## M1G 1 2669 2669 33.350 2.62e-06 *** ## M2G 1 3215 3215 40.174 5.44e-07 *** ## Exp 2 58 29 0.360 0.701 ## M1G:M2G 1 2 2 0.029 0.866 ## Residuals 30 2401 80 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.2.2 Ploting the full dataset Let’s plot the full data set and see if the ANOVA results match our visual expectations. ggplot(data = AllExp, mapping = aes(x = interaction(M1G, M2G), y = Phen, color = interaction(M1G, M2G))) + geom_point(alpha = 0.7, position = &quot;jitter&quot;) + stat_summary(fun.data = &quot;mean_se&quot;, alpha = 1) Let’s try plotting this as a boxplot as well. All we have to do is specify a a different geom. ggplot(data = AllExp, aes(x = interaction(M1G, M2G), y = Phen, color = interaction(M1G, M2G))) + geom_boxplot() In the end you will likely be able to conclude that both gene1 and gene2 have significant effects on the phenotype, but that there is not evidence to suggest that there is a significant interaction between the two genes. Also, the experimental trial most likely does not have an effect. This is all dependent on the mean and variance of the distributions we set in the beginning. # Draw 3 measurements from a random normal distribution for each plant line WT &lt;- rnorm(n = 3, mean=100, sd=10) # WT genotype M1 &lt;- rnorm(n = 3, mean=85, sd=10) # Single mutant in Gene 1 M2 &lt;- rnorm(n = 3, mean=85, sd=10) # Single mutant in Gene 2 DM &lt;- rnorm(n = 3, mean=70, sd=10) # Double mutant in Gene 1 and Gene 2 #####???Question??? What value of the mean of the double mutant distribution would be likely to result in a significant interaction between the two genes? answer here 9.3 Post hoc testing While the ANOVA summary tells us which independent variables explain a significant amount of variance in the experiment, it doesn’t allow us to compare the levels of the independent variables. How can we tell if the mutant in gene 1 has a different phenotype from the mutant in gene 2? What if the experimental trial did have an effect? How could we figure out which trial is different from the others? To compare group or level means and variances and answer these questions we will need to do what is called a post hoc test, following our ANOVA analysis. One of the most commonly used post hoc tests is Tukey’s Honest Significant Difference test. This compares all of the group means in a pairwise manner and corrects the confidence threshold since we are making multiple comparisons. We do this test on our ANOVA object Full from above. tuk &lt;- TukeyHSD(Full) tuk ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Phen ~ M1G + M2G + M1G:M2G + Exp, data = AllExp) ## ## $M1G ## diff lwr upr p adj ## WT-M1 17.22146 11.13116 23.31175 2.6e-06 ## ## $M2G ## diff lwr upr p adj ## WT-M2 18.90149 12.8112 24.99179 5e-07 ## ## $Exp ## diff lwr upr p adj ## 2-1 0.564423 -8.439567 9.568413 0.9869259 ## 3-1 2.920947 -6.083043 11.924937 0.7060519 ## 3-2 2.356524 -6.647466 11.360514 0.7964662 ## ## $`M1G:M2G` ## diff lwr upr p adj ## WT:M2-M1:M2 16.714477 5.247041 28.18191 0.0022665 ## M1:WT-M1:M2 18.394514 6.927078 29.86195 0.0007697 ## WT:WT-M1:M2 36.122949 24.655513 47.59038 0.0000000 ## M1:WT-WT:M2 1.680037 -9.787399 13.14747 0.9781927 ## WT:WT-WT:M2 19.408472 7.941036 30.87591 0.0003969 ## WT:WT-M1:WT 17.728435 6.260999 29.19587 0.0011848 So more than likely, you will see for your full experiment with 3 replications that WT is significantly different from M1 and M2. This is shown in the first two sections of the TukeyHSD summary which compare the levels (wildtype vs mutant) of the factors M1G and M2G. The p adj is the p-value (adjusted for the multiple comparisons) resulting from the equivalent of a t-test comparing the two groups. So if the p adj value is less than the accepted confidence level (typically 0.05), then the two compared groups are significantly different from one another. Hopefully, each replication of the experiment will not be significantly different from the others, i.e. the adjusted p-values will be greater than 0.05. We can visualize the differences in the means by plotting the TukeyHSD results. This uses base R plotting which is much less user-friendly than ggplot. Don’t get bogged down in the code here, just pay attention to the graphs. par(mfrow = c(2,2), cex = 0.65) plot(tuk, las = 1, cex.axis = 0.75) This plots the confidence intervals around the difference between the means for each pair of groups. If these confidence intervals intersect zero we cannot conclude that the two groups being compared are significantly different. We could also use the HSD.test function from the agricolae package, which puts the groups into groups that are not significantly different from each other. library(&quot;agricolae&quot;) HSD.test(y = Full, trt = c(&quot;M1G&quot;,&quot;M2G&quot;), console = TRUE) ## ## Study: Full ~ c(&quot;M1G&quot;, &quot;M2G&quot;) ## ## HSD Test for Phen ## ## Mean Square Error: 80.03718 ## ## M1G:M2G, means ## ## Phen std r se Min Max Q25 Q50 ## M1:M2 67.86166 9.088762 9 2.982117 53.13747 84.31263 63.26955 68.98669 ## M1:WT 86.25618 9.371402 9 2.982117 71.89873 98.45158 82.41986 88.98817 ## WT:M2 84.57614 8.449593 9 2.982117 73.13639 98.28265 79.46525 84.24066 ## WT:WT 103.98461 8.094250 9 2.982117 92.60075 119.09990 99.50851 101.01887 ## Q75 ## M1:M2 70.39661 ## M1:WT 91.45467 ## WT:M2 86.64331 ## WT:WT 108.93233 ## ## Alpha: 0.05 ; DF Error: 30 ## Critical Value of Studentized Range: 3.845401 ## ## Minimun Significant Difference: 11.46744 ## ## Treatments with the same letter are not significantly different. ## ## Phen groups ## WT:WT 103.98461 a ## M1:WT 86.25618 b ## WT:M2 84.57614 b ## M1:M2 67.86166 c Finally we can add these groups to our graph and make the titles a bit more understandable. We’ll add a text geometry to add the post hoc comparisons. Another tricky point is that we have to make sure that the x-axis labels match between our data and our HSD test. To do this we use gsub to swap out the colons for periods. HSD.groups &lt;- HSD.test(y = Full, trt = c(&quot;M1G&quot;,&quot;M2G&quot;))$groups HSD.groups$trt &lt;- row.names(HSD.groups) ggplot(data = AllExp, aes(x = interaction(M1G, M2G), y = Phen, color = interaction(M1G, M2G))) + geom_boxplot() + geom_text(data = HSD.groups, mapping = aes(x = gsub(&quot;:&quot;, &quot;.&quot;, trt), y = max(Phen) + 20, label = groups), color = &quot;black&quot;) Now let’s clean this up a bit by changing the axis titles and labels. We’ll use forcats functions like before but this time with pipes to make the process a little more readable. We’ll also use a new theme to clean up the plot a bit and get rid of the legend. library(magrittr) # First we will use mutate from `dplyr` to add a column for the full genotype AllExp %&lt;&gt;% mutate(genotype = interaction(M1G, M2G)) # %&lt;&gt;% is a two way pipe that passes AllExp as the first argument to mutate and then assigns the result back to AllExp # Now to recode genotype we will unfurl the nested forcats function using pipes AllExp$genotype %&lt;&gt;% fct_rev() %&gt;% fct_recode(double_mutant = &quot;M1.M2&quot;, mut2 = &quot;WT.M2&quot;, mut1 = &quot;M1.WT&quot;, WT = &quot;WT.WT&quot;) HSD.groups$trt %&lt;&gt;% fct_rev() %&gt;% fct_recode(double_mutant = &quot;M1:M2&quot;, mut2 = &quot;WT:M2&quot;, mut1 = &quot;M1:WT&quot;, WT = &quot;WT:WT&quot;) ggplot(data = AllExp, aes(x = genotype, y = Phen, color = genotype)) + geom_boxplot() + geom_text(data = HSD.groups, mapping = aes(x = trt, y = max(Phen) + 20, label = groups), color = &quot;black&quot;) + xlab(&quot;genotype&quot;) + ylab(&quot;root length (mm)&quot;) + theme_classic() + theme(legend.position=&quot;none&quot;) 9.4 Assumptions of ANOVA Before making any final conclusions we should check and make sure that our data satisfies the assumptions of the F-statistic, and really the assumptions of nearly any statistical test. These are normality (that the groups are normally distributed), homogeneity of variance (that the variances are similar for each group) and independence (that each observation is independent of the others, e.g. each measurement was of a different plant). It’s not a deal-breaker if our data defies one of these assumptions. There are variations of ANOVA procedures that are robust to certain violations, but these will have to wait for another class. How do you test these assumptions? We can use our diagnostic plots. plot(Full, ask = FALSE) 9.4.1 Normality of residuals Based on the qq plot above which shows very little deviation from a straight line 9.4.2 Homogeneity of variance The test for homo/heterogeneity of variance is called Levene’s Test. The null hypothesis of this test is that there is no difference in the variance of each group. So a significant Levene’s test with \\(p &lt; 0.5\\) would mean that the assumption of homogeneity is violated. The function leveneTest is in the car package. library(&#39;car&#39;) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode leveneTest(AllExp$Phen, interaction(AllExp$M1G, AllExp$M2G), center = median) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 0.0904 0.9648 ## 32 9.4.3 Independence To test for independence we have to think “is there any reason that any measurements might be dependent on another measurement?” Typically this is due to a repeated-measures design. This might be that you have measured the same phenotype of different leaves from the same plant, or measured the same leaf at two different times. Hopefully with this introduction you can now you can proceed to analyze the data you have collected in lab. Follow the same steps of formulating your hypothesis as a mathematical model, graphing the data, and performing an ANOVA and post hoc test to test your hypothesis. 9.5 Non-linear least squares We will walk through an example of using non-linear least squares regression in class Monday. Be sure to look through the reading Chapter 24 Non-linear regression in R here "],["anova-and-non-linear-least-squares-regression-examples.html", "ANOVA and non-linear least squares regression examples 9.6 Nonlinear least squares regression", " ANOVA and non-linear least squares regression examples The substrate concentration dependence on the rate or “velocity” of an enzymatic reaction in most cases can be described by the following equation: \\[\\nu = \\frac{V_mS}{K_m + S}\\] where the maximum reaction velocity is \\(V_m = k_{cat}E_0 = k_2E_0\\), with \\(E_0\\) the initial enzyme concentration and \\(k_{cat}\\) the catalytic rate constant. \\(K_m\\) is the Michaelis constant which represents the binding equilibrium of substrate-enzyme. We often need to estimate the two parameters, \\(V_m\\) and \\(K_m\\) based on experimental data. Although with today’s computational power these parameters could be estimated in many possible ways with non-linear regression, historically and still often today linearizations of this enzyme kinetic model are used to fit these parameters to experimental data. Last week we used the Lineweaver-Burke linearization. This week we will use non-linear least squares regression. 9.5.1 Lineweaver-Burk plot One method of linearization would be to invert the entire model equation. \\[\\frac{1}{\\nu} = \\frac{1}{V_m} + \\frac{K_m}{V_m} \\frac{1}{S}\\] The resulting plot is called a Lineweaver-Burk plot. 9.5.1.1 ???Questions??? What would you plot as x and y, and what would be the slope and y-intercept of the plot be. A Lineweaver-Burke plot is \\(1/\\nu\\) (y) vs \\(1/S\\) (x). The slope is \\(K_m/V_m\\) and the y-intercept is \\(1/V_m\\) This plot is also sometimes called a double-reciprocal plot, because we plot the reciprocal of our independent variable \\(\\nu\\) versus our dependent variable \\(S\\). To use this plot to find \\(V_m\\) and \\(K_m\\) we need to collect reaction velocity data (\\(\\nu = dP/dt = -dS/dt\\)) at different initial substrate concentrations. The reaction velocity will of course decrease throughout the experiment, but to match the initial substrate concentrations, we want to find the initial velocity, i.e. the initial slope of the curve of product or substrate versus time. You can perhaps see how determining the instantaneous initial velocity of this curve can be difficult and error prone. We need very rapid and accurate measurements of product or substrate in order to determine this initial slope. Due to the properties of the reciprocal, the error at low substrate concentrations is amplified and has a strong influence on the slope and intercept of the graph. For these reasons the Lineweaver-Burke plot provides better estimates of \\(V_m\\) than \\(K_m\\). Let’s look at an some example data and calculate \\(V_m\\) and \\(K_m\\) in several ways. Here we have performed experiments with two different concentrations of the same purified enzyme across a range of substrate concentrations, and calculated the initial velocity of each reaction. S &lt;- c(20, 10, 6.7, 5, 4, 3.3, 2.9, 2.5) # substrate concentration (g/L) v_0.015 &lt;- c(1.14, 0.87, 0.7, 0.59, 0.50, 0.44, 0.39, 0.35) # reaction velocity (g/L-min) at E_0 = 0.015 g/L data &lt;- data.frame(S, v_0.015) require(knitr) ## Loading required package: knitr knitr::kable(data) S v_0.015 20.0 1.14 10.0 0.87 6.7 0.70 5.0 0.59 4.0 0.50 3.3 0.44 2.9 0.39 2.5 0.35 Sometimes we won’t know the concentration of our enzyme, if say we are working with just a cell lysate or if we are unable to completly purify our enzyme. In this case we will just use a specific activity of the mixture. Specific activity is just the activity of the enzyme per mass of total protein in the complex mixture. The enzyme activity is measure in units of product formed per time at particular conditions (e.g. temperature, pH, substrate concentration, etc.). Let’s initially look at this data by plotting \\(\\nu\\) vs \\(S\\) as dictated by the Michaelis-Menten equation. require(tidyr) ## Loading required package: tidyr data &lt;- tidyr::gather(data = data, key = E_0, value = v, v_0.015) require(stringr) ## Loading required package: stringr data$E_0 &lt;- as.numeric(stringr::str_remove(string = data$E_0, pattern = &quot;v_&quot;)) require(ggplot2) ## Loading required package: ggplot2 ggplot2::ggplot(data = data, mapping = aes(x = S, y = v, color = as.factor(E_0))) + geom_point() + labs(y = &quot;v (g/L-min)&quot;, x = &quot;S (g/L)&quot;) + expand_limits(x = 0, y = 0) 9.5.1.2 ???Questions??? What can you say about this graph? The higher concentration of enzyme has a higher velocity across all substrate concentrations. The shape of the two curves is similar. As substrate concentration increases the increase in reaction velocity decreases. Now let’s make a Lineweaver-Burk plot. \\[\\frac{1}{\\nu} = \\frac{1}{V_m} + \\frac{K_m}{V_m} \\frac{1}{S}\\] data$LB.y &lt;- 1/data$v data$LB.x &lt;- 1/data$S ggplot2::ggplot(data = data, mapping = aes(x = LB.x, y = LB.y, color = as.factor(E_0))) + geom_point() + labs(y = &quot;1/v (L-min/g)&quot;, x = &quot;1/S (L/g)&quot;) + expand_limits(x = 0, y = 0) Now we can use linear regression to find the slope and intercept of the line of best fit. LB &lt;- lm(formula = LB.y~LB.x, data = data) summary(LB) ## ## Call: ## lm(formula = LB.y ~ LB.x, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.027826 -0.004796 0.001278 0.007841 0.026257 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.58019 0.01462 39.69 1.71e-08 *** ## LB.x 5.67720 0.05801 97.87 7.67e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.01874 on 6 degrees of freedom ## Multiple R-squared: 0.9994, Adjusted R-squared: 0.9993 ## F-statistic: 9578 on 1 and 6 DF, p-value: 7.67e-11 9.6 Nonlinear least squares regression With the computational power available today these linearizations are really historical relicts, as we can fit these parameters to our nonlinear model. Nonlinear regression methods essentially sweep through the parameter space, by iteratively changing each parameter and calculating the distance between the model with the current parameter set and the data points. The sum of the squared distance between our model and data is called the residual. The parameter values that minimize the residual are chosen our best fit. # Perform nonlinear least squares regression nls1 &lt;- nls(formula = v ~ S/(S + Km) * Vm, data = data, start = list(Km=1, Vm=1)) summary(nls1) ## ## Formula: v ~ S/(S + Km) * Vm ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## Km 9.35733 0.13814 67.74 6.96e-10 *** ## Vm 1.67708 0.01314 127.66 1.56e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.004591 on 6 degrees of freedom ## ## Number of iterations to convergence: 6 ## Achieved convergence tolerance: 3.9e-07 Note that we don’t get an R2 with non-linear least squares. To compare nonlinear vs linear models we must calculate the residuals given the parameters for each model, so plugging back in the Vm and Km parameters from our lm call and nls call into the Michaelis-Menten equation and calculating the predictions, then subtract the original v values from data from those that our models predict to calculate residuals for each model on the same scale of the original v without transformation. You can see how this is done in my Bioprocess course packet here: https://wrightrc.github.io/Bioprocess-Book/05-enzyme-rate-parameters.html#comparing-linearization-vs-nonlinear-methods. Another advantage is that our \\(K_m\\) and \\(k_{cat}\\) as well as standard errors do not need any scaling; we get direct estimates of parameters of this enzyme and uncertainty in these estimates. While we can calculate these parameters and uncertainties it is cumbersome and inevitably will result in poorer estimates just based on the worse model-fit per the residuals. Looking back at our linearized model, \\[\\frac{E_0}{\\nu} = \\frac{K_m}{k_{cat}S} + \\frac{1}{k_{cat}}\\] we can calculate our estimate of \\(k_{cat}=\\) 1.7235773 \\(\\pm\\) , and our estimate of \\(K_m =\\) 9.7850987. To calculate the uncertainty in these estimates we must propagate the standard error in our slope and intercept, according to the division formula \\[Q = \\frac{a}{x}, \\delta Q = Q \\sqrt{(\\frac{\\delta a}{a})^2 + (\\frac{\\delta x}{x})^2}\\] There is a nice wikipedia article on Propagation of Uncertainty describing these formulas for standard error/deviation calculation and how they were derived: https://en.wikipedia.org/wiki/Propagation_of_uncertainty ggplot2::ggplot(data = data, mapping = aes(x = S, y = v)) + geom_point() + labs(y = &quot;v (g/L-min)&quot;, x = &quot;S (g/L)&quot;) + geom_smooth(method = &quot;nls&quot;, se = FALSE, formula = y ~ x/(x + Km) * Vm, method.args = list(start = list(Km=111, Vm=9.5))) Unfortunately geom smooth doesn’t work with nls to make confidence intervals, so we have to use geom_ribbon and predictNLS to make them separately. But I’m not sure in this case if the confidence intervals really make sense… conf.int &lt;- propagate::predictNLS(nls1, newdata = data.frame(S = seq(0, 20, 1)), interval = &quot;confidence&quot;, nsim = 100000) ## predictNLS: Propagating predictor value #1... ## predictNLS: Propagating predictor value #2... ## predictNLS: Propagating predictor value #3... ## predictNLS: Propagating predictor value #4... ## predictNLS: Propagating predictor value #5... ## predictNLS: Propagating predictor value #6... ## predictNLS: Propagating predictor value #7... ## predictNLS: Propagating predictor value #8... ## predictNLS: Propagating predictor value #9... ## predictNLS: Propagating predictor value #10... ## predictNLS: Propagating predictor value #11... ## predictNLS: Propagating predictor value #12... ## predictNLS: Propagating predictor value #13... ## predictNLS: Propagating predictor value #14... ## predictNLS: Propagating predictor value #15... ## predictNLS: Propagating predictor value #16... ## predictNLS: Propagating predictor value #17... ## predictNLS: Propagating predictor value #18... ## predictNLS: Propagating predictor value #19... ## predictNLS: Propagating predictor value #20... ## predictNLS: Propagating predictor value #21... conf.int &lt;- data.frame(x = seq(0, 20, 1), ymin = conf.int$summary[,5], ymax = conf.int$summary[,6]) ggplot2::ggplot(data = data, mapping = aes(x = S, y = v)) + geom_point() + labs(y = &quot;v (g/L-min)&quot;, x = &quot;S (g/L)&quot;) + # geom_smooth(method = &quot;nls&quot;, se = FALSE, # formula = y ~ x/(x + Km) * Vm, # method.args = list(start = list(Km=111, Vm=9.5))) + geom_ribbon(data = conf.int, mapping = aes(x = x, ymin = ymin, ymax = ymax), inherit.aes = FALSE, alpha = 0.2) "],["roots-and-optima-introduction.html", "10 Roots and Optima Introduction 10.1 Roots 10.2 Optima/Optimization 10.3 Optimization, 1-Dimensional 10.4 Optimization, 2-Dimensional", " 10 Roots and Optima Introduction This week our goals are to be able to: - Identify problem statements that are asking to identify roots or optima of equations. - Outline the algorithm for solving root and optima problems. - Correctly set up equations and solve for roots and optima. - Visualize functions to assist in identifying search ranges for roots and optima. Read and/or watch: - Read https://en.wikipedia.org/wiki/Newton%27s_method (through Description section) and https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization (through Geometric interpretation section) - Watch https://www.youtube.com/watch?v=lBfqvBJaFmc first 10 min. From: http://mathhasproblems.weebly.com/polynomial-functions.html Then review the following examples in R. 10.1 Roots library(ggplot2) library(rootSolve) 10.1.1 Example 1 Suppose you have a model that represents the velocity of a bungee jumper (v) that depends on the mass of the person (m), the drag force (cd), the time in free fall (t), and the gravitational constant (g). Medical studies have shown that if the velocity increases above 36 m/s after 4 seconds of free fall, the health risk to the jumper is elevated. What is the critical mass of a person given the following: \\[v(t) = \\sqrt{\\frac{gm}{c_d}} \\tanh \\left( t\\sqrt{\\frac{gc_d}{m}} \\right)\\] (This equation is written with in-line LaTeX code in Rmarkdown). \\(c_d = 0.25\\) kg/m; \\(g = 9.81\\) m/s2 How would you solve? graph root(s) 10.1.1.1 Approach Rearrange equation Plot equation Run uniroot 10.1.1.1.1 Rearrange equation All we need to do here is subtract over \\(v\\) to the right side. \\[0 = \\sqrt{\\frac{gm}{c_d}} \\tanh \\left( t\\sqrt{\\frac{gc_d}{m}} \\right) - v(t)\\] Now we can set up this equation along with all of the variable and constant values in our R session. 10.1.1.1.2 Plot Equation cd &lt;- 0.25 # drag coefficient #TODO: change to c_d g &lt;- 9.81 # gravitational constant v &lt;- 36 # velocity (m/s) t &lt;- 4 # time (s) mp &lt;- seq(0,200) # mass predictions, # a range of values where the root might be. # You might need to play with this range. f &lt;- function(mp = 1,cd = 1,g = 9.8,v = 36,t = 4){ # all variables in the equation are arguments sqrt(g*mp/cd)*tanh(sqrt(g*cd/mp)*t)-v # what the function does # by default our output is the result of the above line } f(mp = 147, cd = cd, g = g, v = v, t = t) ## [1] 0.08490453 plot(mp,f(mp,cd,g,v,t)) # plot our function f # these variables have already been defined 10.1.1.1.3 Solve equation using uniroot Now we can solve for the root of this equation, i.e. where the plot crosses the x-axis. We will use the uniroot function. “The function uniroot searches the interval from lower to upper for a root (i.e., zero) of the function f with respect to its first argument.” # Don&#39;t run this, this is from the help documentation of uniroot uniroot(f, interval, ..., lower = min(interval), upper = max(interval), f.lower = f(lower, ...), f.upper = f(upper, ...), extendInt = c(&quot;no&quot;, &quot;yes&quot;, &quot;downX&quot;, &quot;upX&quot;), check.conv = FALSE, tol = .Machine$double.eps^0.25, maxiter = 1000, trace = 0) You can set extendInt = \"yes\" to extend the interval if you didn’t see a root in your plot. Now we can use uniroot to solve. (mass &lt;- uniroot(f = f, interval = c(50,200), cd = cd, g = g, v = v, t = t)) ## $root ## [1] 142.7376 ## ## $f.root ## [1] 3.146922e-07 ## ## $iter ## [1] 7 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 # cd, g, v, t are `... =` additional named or unnamed arguments to be passed # to f. I&#39;ve named cd and g, v and t are not named, so they need to be in the # same order as our function definition in lines 45-47. So the root of this equation, the mass at which this maximum velocity without spinal damage is 142.7376485. It took 7 iterations to reach this root, and the function has a value of 3.1469221^{-7} as this value. If we change the tol argument to be lower we could get a more accurate number, but to be safe let’s just round down our root a bit to 142; nobody wants spinal damage. 10.1.2 Example 2 You buy a $25,000 piece of equipment for nothing down at $5,500 per year for 6 years. What intereest rate are you paying? The formula relating present worth \\(P\\), annual payments \\(A\\), number of years \\(n\\), and interest rate \\(i\\) is \\[ A = P \\frac{i(1 + i)^n}{(1 + i)^n - 1}\\] Add your own comments to the code chunk below, describing what each line is doing. A &lt;- 25000 p &lt;- 6500 n &lt;- 5 i &lt;- seq(0,10) f &lt;- function(i,A,p,n) (p*i*(1+i)^n)/((1+i)^n-1)-A plot(i,f(i,A,p,n)) uniroot(f,c(0.1,10),A,p,n) ## $root ## [1] 3.844713 ## ## $f.root ## [1] -0.001068101 ## ## $iter ## [1] 4 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 10.1.3 Example 3 more than 1 root In this case we will need to use uniroot.all to find the multiple roots. x &lt;- seq(3,6,.01) f &lt;- function(x) sin(10*x)+cos(3*x) fx &lt;- f(x) d &lt;- data.frame(x,fx) ggplot(d,aes(x,fx)) + geom_line() uniroot(f, c(3,6)) ## $root ## [1] 5.679042 ## ## $f.root ## [1] 0.0001150379 ## ## $iter ## [1] 7 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 # uniroot.all is in the rootSolve package library(rootSolve) rootSolve::uniroot.all(f, c(3,6)) -&gt; roots 10.2 Optima/Optimization library(ggplot2) library(pracma) # has meshgrid ## ## Attaching package: &#39;pracma&#39; ## The following objects are masked from &#39;package:rootSolve&#39;: ## ## gradient, hessian library(plotly) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout 10.3 Optimization, 1-Dimensional Definition: The optimize R function performs one dimensional optimization. You can find the basic R programming syntax of the optimize function below. #from ?optimize #(You can also spell it `optimise`) # Usage optimize(f, interval, ..., lower = min(interval), upper = max(interval), maximum = FALSE, tol = .Machine$double.eps^0.25) # f # the function to be optimized. The function is either minimized or maximized over its first argument depending on the value of maximum. # interval # a vector containing the end-points of the interval to be searched for the minimum. # ... # additional named or unnamed arguments to be passed to f. This looks very similar to uniroot, right? Our approach to solving these will also be similar. 10.3.1 Approach: Write function. Plot function. Apply optimize command. Set maximum=TRUE if you want to find maximum. 10.3.2 Example: Optimizing User-Defined Function Using optimize() in R 10.3.2.1 Write function The following R programming syntax illustrates how to use the optimize function in R. First, we have to create our own function that we want to optimize: my_function &lt;- function(x) x^3 + 2 * x^2 - 10 * x # Write Function f &lt;- function(x) { # Create function x^2/10-2*sin(x)} 10.3.2.2 Plot You can use geom_function(fun = f) now to plot functions ggplot() + geom_function(fun = f) + xlim(0,4) # which is much better than the old way x &lt;- seq(0,4,0.1) # create x vector, you can change the limits fx &lt;- f(x) # evaluate using x vector d &lt;- data.frame(x,fx) # create data frame ggplot(d,aes(x,fx))+geom_line() # plot 10.3.2.3 Find optimum Finally we can find the local optima of this function. From our plot it is between 1 and 2. # Apply optimize (f_min &lt;- optimize(f = f, interval = c(1, 2))) ## $minimum ## [1] 1.427552 ## ## $objective ## [1] -1.775726 The interval range for plotting provides the opportunity to hone in on the minimum. Here, I started larger and then chose to plot between 0 and 4. This allowed me to select a reasonable interval for the optimize function. The minimum value occurs at an x-value of 1.43 and the minimum is -1.78. 10.3.3 Example: Finding a maximum 10.3.3.1 Write function In the example above, we found the minimum. Suppose we want to find the maximum value of a function. Let’s imagine we have a soccer ball that we kick into the air at a starting velocity, and we want to compute how high the ball gets. The model equation is below: \\[ z = z_0 + \\frac{m}{c} \\left( v_0 + \\frac{m g}{c}\\right)\\left( 1 - e^{(-c/mt)}\\right) - \\frac{mg}{c}t \\] where \\(z =\\) height above surface [m] \\(z_0 =\\) initial height [m] \\(m =\\) mass [kg] \\(c =\\) drag coefficient [kg/s] \\(v_0 =\\) initial velocity [m/s] \\(t =\\) time [s] Given the following values: g = 9.81 #m/s2 z_0 = 0 #m c = 0.25 #kg/s, https://www.grc.nasa.gov/WWW/K-12/airplane/socdrag.html m = 0.45 #kg v_0 = 25 #m/s f &lt;- function(t,g,z_0,c,m,v_0) z_0 + ((m/c)*(v_0+(m*g/c))*(1-exp(-1*c/m*t)))-(m*g/c*t) 10.3.3.2 Plot t &lt;- seq(0,5,.01) fx &lt;- f(t = t, g = g, z_0 = z_0, c, m, v_0) d &lt;- data.frame(t,fx) ggplot(d,aes(t,fx)) + geom_line() + xlab(&quot;t [s]&quot;) + ylab(&quot;Height [m]&quot;) # OR ggplot() + geom_function(fun = f, args = list(g = g, z_0 = z_0, c = c,m = m, v_0 = v_0)) + xlim(0,5) + xlab(&quot;t [s]&quot;) + ylab(&quot;Height [m]&quot;) # kind of convenient how the args list to our function f is also necessary for # optimize... 10.3.3.3 Find optimum optimize(f = f, interval = c(0,5), g = g, z_0 = z_0,c = c,m = m, v_0 = v_0, maximum = TRUE) ## $maximum ## [1] 1.58766 ## ## $objective ## [1] 16.96534 10.4 Optimization, 2-Dimensional Now let’s look at a 2-D example. Here we will do our plotting and function writing a bit backwards, as plotting in 3D, can be cumbersome. You can create contour or heatmap plots where you define a grid of x and y values using meshgrid to the calculate the z value, z(x,y). Once you plot, you can play around with the start and end value under contours to zoom in on the optimum you are interested in. The plot allows you to identify the location in the x and y vector, and from there you can identify the x and y value. We will use plotly package to make these plots. Plotly conveniently allows you to see the values of a plot as you hover over it. Plotly makes interactive HTML plots, which are really cool but don’t play well with PDFs, unfortunately. So you will not see your plot_ly plots if you knit to PDF. You can knit to HTML and then print the page to a PDF if you want to see these plots. 10.4.1 Example: 2-D Here is a purely mathematical example where we want to solve for the optima of the following function \\[ f(x, y) = 2 + x-y+2x^2+2xy+y^2\\] 10.4.1.1 Plot equation library(plotly) x &lt;- seq(-3,3,0.01) # define range in x values y &lt;- seq(-3,3,0.1) # define range in y values FF &lt;- pracma::meshgrid(x,y) # create grid of x,y pairs to evaluate # NOTE: meshgrid takes two vectors and makes a matrix of all pairs of these values and renames these values X and Y Z &lt;- 2 + FF$X-FF$Y+2*FF$X^2+2*FF$X*FF$Y+FF$Y^2 # use grid to create Z values (3rd dimension) fig &lt;- plot_ly(z = Z, x = x, y = y, type = &quot;contour&quot;, contours = list( start = 0, end = 10, size = 0.5 )) fig # another option is a heatmap fig &lt;- plot_ly(z = Z, x = x, y = y, type = &quot;heatmap&quot;) fig # we can also add labels to the contour fig &lt;- plot_ly(z = Z, x = x, y = y, type = &quot;contour&quot;, contours = list(showlabels = TRUE)) fig Try hovering over the plot to find a good guess for the minimum. We will need this starting value for the fminsearch function. You’ll also have to rewrite the function as below where x[1] is x, and x[2] is y. This helps the code run more efficiently by passing a single x object with 2 values, instead of 2 objects x and y. Then use fminsearch, where x0 is the starting value for the search. If we wanted to find the maximum, set minimize=FALSE. 10.4.1.2 Write function r &lt;- function(xy) 2+xy[1]-xy[2]+2*xy[1]^2+2*xy[1]*xy[2]+xy[2]^2 10.4.1.3 Find optimum fminsearch(fn = r, x0 = c(-1, 1.5), method=&quot;Nelder-Mead&quot;) ## $xmin ## [1] -1.0 1.5 ## ## $fmin ## [1] 0.75 ## ## $count ## [1] 107 ## ## $convergence ## [1] 0 ## ## $info ## $info$solver ## [1] &quot;Nelder-Mead&quot; ## ## $info$restarts ## [1] 0 # An alternate method: fminsearch(fn = r, x0 = c(-1.2, 1), method=&quot;Hooke-Jeeves&quot;) ## $xmin ## [1] -1.0 1.5 ## ## $fmin ## [1] 0.75 ## ## $count ## [1] 307 ## ## $convergence ## [1] 0 ## ## $info ## $info$solver ## [1] &quot;Hooke-Jeeves&quot; ## ## $info$iterations ## [1] 26 10.4.2 Example: Another simple example my_function &lt;- function(c) 2*c/(4+0.8*c+c^2+0.2*c^3) # Write Function f &lt;- function(c) -2*c/(4+0.8*c+c^2+0.2*c^3) # Plot c &lt;- seq(0,10,0.1) # create x vector, you can change the limits fc &lt;- f(c) # evaluate using x vector d &lt;- data.frame(c,fc) # create data frame ggplot(d,aes(c,fc))+geom_line() # plot # Apply optimize optimize(f,interval = c(0, 10)) ## $minimum ## [1] 1.567899 ## ## $objective ## [1] -0.3696349 "],["roots-and-optima-examples.html", "Roots and Optima Examples 10.5 Specific heat 10.6 Channel perimeter", " Roots and Optima Examples 10.5 Specific heat Engineers use thermodynamics in their work to precisely heat and cool solids, liquids and gases. The following equation relates the zero-pressure specific heat of dry air \\(c_p\\) [kJ/(kg K)] to temperature [K]: \\[c_p = 0.99302 + 1.672 \\times 10^{-4}T + 9.7216 \\times 10^{-8}T^2 - 9.5837 \\times 10^{-11}T^3 +1.9320 \\times 10^{-14}T^4\\] Make a plot of \\(c_p\\) versus temperature from \\(T = 0\\) to 1200 K. Then, find the temperature that corresponds to a \\(c_p\\) of 1.1 kJ/(kg K). # define our variables A_0 &lt;- 0.99302 A_1 &lt;- 1.672E-4 A_2 &lt;- 9.7216E-8 A_3 &lt;- -9.5837E-11 A_4 &lt;- 1.9320E-14 T_range &lt;- seq(0,1200, by = 1) c_p0 &lt;- 1.1 # define our function c_p &lt;- function(T_K, A_0, A_1, A_2, A_3, A_4, c_p0){ A_0 + A_1 * T_K + A_2 * T_K^2 + A_3 * T_K^3 + A_4 * T_K^4 - c_p0 } plot(T_range, c_p(T_K = T_range, A_0 = A_0, A_1 = A_1, A_2 = A_2, A_3 = A_3, A_4 = A_4, c_p0 = 1.1)) uniroot(f = c_p, interval = c(0, 1200), A_0 = A_0, A_1 = A_1, A_2 = A_2, A_3 = A_3, A_4 = A_4, c_p0 = c_p0) ## $root ## [1] 548.946 ## ## $f.root ## [1] 1.309735e-10 ## ## $iter ## [1] 5 ## ## $init.it ## [1] NA ## ## $estim.prec ## [1] 6.103516e-05 10.6 Channel perimeter As a Biological Systems Engineer, imagine you are asked to design a trapezoidal channel to carry irrigation water. Determine the optimal dimensions to minimize the wetted perimeter for a cross-sectional area of 50 m2. Diagram of a trapezoidal channel and descriptive equations \\[P = a + 2 \\sqrt{\\left(\\frac{b-a}{2}\\right)^2 +h^2}\\] \\[A = \\frac{1}{2} \\left(a + b \\right) h\\] \\[50 = \\frac{1}{2} \\left(a + b \\right) h\\] Solve for h. \\[h = \\frac{100}{\\left(a + b \\right)}\\] Plug into perimeter function. \\[P = a + 2 \\sqrt{\\left(\\frac{b-a}{2}\\right)^2 +\\left( \\frac{100}{\\left(a + b \\right)} \\right)^2}\\] #### Plot equation library(plotly) ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(pracma) a &lt;- seq(1,50,0.1) # define range in a values b &lt;- seq(1,50,0.1) # define range in y values ab &lt;- pracma::meshgrid(a,b) # create grid of x,y pairs to evaluate # NOTE: meshgrid takes two vectors and makes a matrix of all pairs of these values and renames these values X and Y P &lt;- ab$X + 2 * sqrt(((ab$Y-ab$X)/2)^2 +(100/(ab$X + ab$Y))^2) # use grid to create Z values (3rd dimension) fig &lt;- plot_ly(z = P, x = a, y = b, type = &quot;contour&quot;, contours = list( start = 0, end = 50, size = 0.1 )) fig # another option is a heatmap fig &lt;- plot_ly(z = P, x = a, y = b, type = &quot;heatmap&quot;) fig # we can also add labels to the contour fig &lt;- plot_ly(z = P, x = a, y = b, type = &quot;contour&quot;, contours = list(showlabels = TRUE)) fig 10.6.0.1 Write function P_func &lt;- function(ab){ ab[1] + 2 * sqrt(((ab[2]-ab[1])/2)^2 +(100/(ab[1] + ab[2]))^2) } P_func(ab = c(5,10)) ## [1] 19.24001 10.6.0.2 Find optimum range_X &lt;- c(5, 10) range_Y &lt;- c(10, 15) fminsearch(fn = P_func, x0 = c(7, 13)) ## $xmin ## [1] 6.204032 12.408065 ## ## $fmin ## [1] 18.6121 ## ## $count ## [1] 104 ## ## $convergence ## [1] 0 ## ## $info ## $info$solver ## [1] &quot;Nelder-Mead&quot; ## ## $info$restarts ## [1] 0 "],["interpolation-and-extrapolation.html", "11 Interpolation and Extrapolation 11.1 Reading 11.2 Overview of interpolation", " 11 Interpolation and Extrapolation knitr::opts_chunk$set(echo = TRUE) library(ggplot2) library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ lubridate 1.9.4 ✔ tibble 3.2.1 ## ✔ purrr 1.0.4 ✔ tidyr 1.3.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(PolynomF) ## ## Attaching package: &#39;PolynomF&#39; ## ## The following object is masked from &#39;package:purrr&#39;: ## ## zap library(pracma) ## ## Attaching package: &#39;pracma&#39; ## ## The following objects are masked from &#39;package:PolynomF&#39;: ## ## integral, neville ## ## The following object is masked from &#39;package:purrr&#39;: ## ## cross This week our goals are to learn the following: - Interpolation Basics: Understand the concept and importance of interpolation in engineering. Explore real-world applications such as estimating values between known data points. - Linear Interpolation: Learn the basic linear interpolation method, including the formula and its application to estimate values between two known points. - Polynomial and Spline Interpolation: Explore polynomial and spline interpolation methods. Understand how they introduce curvature and minimize errors compared to higher-order polynomials. - Extrapolation Awareness: Understand the risks associated with extrapolating data beyond the observed range and the importance of caution in such scenarios. 11.1 Reading Read through this Intro material below and also skim through this example: http://www.sthda.com/english/articles/40-regression-analysis/162-nonlinear-regression-essentials-in-r-polynomial-and-spline-regression-models/#spline-regression 11.2 Overview of interpolation Within the Engineering discipline, we often use interpolation techniques to estimate a value. For example, imagine you are using a thermodynamic table to look up the specific enthalpy of water at 33.4°C. The table has values for 30°C and 35°C; thus, you need to interpolate the value at 33.4°C. This week we’ll look at different interpolation approaches for such problems. More advance interpolation for 2-D surfaces includes interpolating properties across a surface—for example, estimating average annual rainfall across a state or country uses interpolation and modeling approaches (e.g. see https://prism.oregonstate.edu/normals/) since rainfall is not measured everywhere. We’ll focus on 1-D examples in this unit; there’s an excellent appendix with examples at the end of Manuel Gimond’s book on spatial analysis. One might ask what the difference is between interpolation and regression: in contrast to regression, interpolation schemes result in the line going through every observational point exactly. The goal is to make the most accurate predictions of values between experimental observations, not to define the most simple, or mechanistic model that fits the data. 11.2.1 Linear interpololation The most basic interpolation approach is to assume a straight line between 2 points. Imagine you want to estimate a value \\(x_3\\) based on the following diagram. The simplest approach is to assume a linear line connects \\(x_1\\) and \\(x_2\\). Using your knowledge of \\(x_1\\), \\(f(x_1)\\), \\(x_2\\), and \\(f(x_2)\\), the equation to calculate the slope of the line to estimate \\(f(x_3)\\) is \\[ f(x_3) = f(x_1) + \\frac{f(x_2) - f(x_1)}{x_2 - x_1} (x_3 - x_1) \\] Figure 11.1: Linear interpolation diagram with equation. Let’s look at an example. Suppose you have a table for the specific volume of a gas. You’ve been asked for the value at 377°C, and you know the following: T @ 360°C, specific volume = 4.789 cubic liters/kg T @ 380°C, specific volume = 5.987 cubic liters/kg fx1 &lt;- 4.789 fx2 &lt;- 5.987 x1 &lt;- 360 x2 &lt;-380 x3 &lt;- 377 lin_interp &lt;- function(fx1,fx2,x1,x2,x3) fx1+((fx2-fx1)/(x2-x1))*(x3-x1) fx3 &lt;- lin_interp(fx1,fx2,x1,x2,x3) fx3 ## [1] 5.8073 So the specific volume at \\(x_3 =\\) 377°C is 5.8073 cubic liters/kg. 11.2.2 Curvature and Interpolation. There are different approaches to introduce curvature into an interpolation scheme to reduce the error. Here, we’ll explore polynomials and spline functions. 11.2.2.1 Polynomial Interpolation Polynomials are frequently used in interpolation, and they provide one approach to inducing curvature into the interpolation. We generally use 2 basic approaches: Newtons and Lagrange polynomials. Under most conditions, Newtons method is more computationally efficient and we’ll use this approach here. In essence, a term (or terms) is added to the linear interpolation that includes curvature. Thus, if you have 3 data-points, a simple quadratic equation would suffice. If you have 4 points, a third order polynomial (4-1) would go through all the points. The general form of the polynomial is using Newton’s method. If you remember, we talked briefly during regression week about over-fitting models. In interpolation we are purposely over fitting to use all available information to predict intermediate values. Suppose we have the following dataset: x = 0, 2, 3, 4 y = 7, 11, 28, 63 Plot the data and use the poly_calc() function within the PolynomF package. Here, the polycalc function creates a polynomial (that I assign to a) that can then be used for interpolation (I evaluated the polynomial a at all points in the xx vector). The resulting polynomial function can be accessed as shown below and is an R function as well: a ## 7 - 2.25*x + 2.708333*x^2 - 0.2916667*x^3 a(x = 5) ## [1] 27 Now what happens when we extrapolate beyond the observed data we used to generate the function? Imagine we had another observation that was just determined. You can see in the plot below that the original polynomial will result in a poor extrapolation! xx &lt;- seq(0,10,.1) yy &lt;- a(xx) dat2&lt;- data.frame(cbind(xx, yy)) x[5] &lt;- 10 y[5] &lt;- 25 dat &lt;- data.frame(cbind(x, y)) pp &lt;- ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat2,aes(x=xx,y=yy)) pp Thus, caution is needed when using and interpreting the results of interpolation and especially extrapolation. 11.2.2.2 Splines Interpolation Let’s redo our example using “the method of splines”, instead of polynomial interpolation. Splines are piecewise polynomial interpolations, so there can be a different polynomial function between each pair of points, but each polynomial takes into account the ones next to it in order to minimize the roughness of the curve. You can imagine this as bending a thin piece of wood to fit all of the points, which is actually where the method comes from in the early days of the airline industry. We can implement interpolation by splines (as well as many other methods we will demonstrate) via interp1 within the pracma package: #The general usage of interp1 interp1(x, y, xi = x, method = c(&quot;linear&quot;, &quot;constant&quot;, &quot;nearest&quot;, &quot;spline&quot;, &quot;cubic&quot;)) 11.2.2.3 Spline interpolation: introduces some simple curvature xx &lt;- seq(0,8,.01) yy &lt;- interp1(x, y, xi = xx, method = c(&quot;spline&quot;)) dat2&lt;- data.frame(cbind(xx, yy)) pp &lt;- ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat2,aes(x=xx,y=yy)) pp 11.2.2.3.1 Additional methods in the interp1 function In addition to splines interp1 has several other methods. 11.2.2.3.1.1 Linear interpolation yy &lt;- interp1(x, y, xi = xx, method = c(&quot;linear&quot;)) dat2&lt;- data.frame(cbind(xx, yy)) pp &lt;- ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat2,aes(x=xx,y=yy)) pp 11.2.2.3.1.2 Constant interpolation: assume same value as previous (no knowledge of future value) xx &lt;- seq(0,8,.01) yy &lt;- interp1(x, y, xi = xx, method = c(&quot;constant&quot;)) dat2&lt;- data.frame(cbind(xx, yy)) pp &lt;- ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat2,aes(x=xx,y=yy)) pp 11.2.2.3.1.3 Nearest interpolation: assume same value as closest observation xx &lt;- seq(0,8,.01) yy &lt;- interp1(x, y, xi = xx, method = c(&quot;nearest&quot;)) dat2&lt;- data.frame(cbind(xx, yy)) pp &lt;- ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat2,aes(x=xx,y=yy)) pp 11.2.3 Oscillations Back to our splines and polynomials… Oscillations are another issue that come into play in any polynomial-based interpolation. Splines are an alternative to polynomials that result in a simplified interpolation. Splines apply lower-order polynomials in a piecewise fashion to subsets of data points, and minimize oscillations and reduce round-off error due to their lower-order nature. But they are not always perfect! x &lt;- seq(-1,1,.5) y &lt;- 1/(1+25*x^2) # Runge&#39;s function a &lt;- poly_calc(x,y) xx &lt;- seq(-1,1,.01) yy &lt;- a(xx) dat3&lt;- data.frame(cbind(xx, yy)) # When we plot Runge&#39;s function: yyy &lt;- 1/(1+25*xx^2) xxx &lt;- seq(-1,1,.01) dat4&lt;- data.frame(cbind(xxx, yyy)) dat &lt;- data.frame(cbind(x, y)) pp &lt;- ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat3,aes(x=xx,y=yy),col=&quot;red&quot;) + geom_line(data=dat4,aes(x=xxx,y=yyy),col=&quot;black&quot;) pp The black line is the true value from the function, the red line is from the interpolation using a polynomial. This illustrates the importance of being careful with higher order polynomials! If we use splines here the fit is a little better, but still has some oscillations. dat5 &lt;- data.frame(x = xx, y = interp1(x, y, xi = xx, method = &quot;spline&quot;)) ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat3,aes(x=xx,y=yy),col=&quot;red&quot;) + geom_line(data=dat4,aes(x=xxx,y=yyy),col=&quot;black&quot;) + geom_line(data = dat5, aes(x = x, y = y), col = &quot;green&quot;) The best way to improve interpolation is to increase the amount of data you feed into the interpolation algorithm. If instead of using 5 points we use 9, the splines have a much better fit to the function the blue data points were pulled from. x &lt;- seq(-1,1,.25) y &lt;- 1/(1+25*x^2) dat &lt;- data.frame(x, y) dat6 &lt;- data.frame(x = xx, y = interp1(x, y, xi = xx, method = &quot;spline&quot;)) ggplot() + geom_point(data=dat,aes(x=x, y=y),size=3, col=&#39;blue&#39;) + geom_line(data=dat4,aes(x=xxx,y=yyy),col=&quot;black&quot;) + geom_line(data = dat6, aes(x = x, y = y), col = &quot;green&quot;) "],["interpolation-and-extrapolation-examples.html", "Interpolation and Extrapolation Examples 11.3 Example - CO2 at Mana Loa Observatory", " Interpolation and Extrapolation Examples knitr::opts_chunk$set(echo = TRUE) library(ggplot2) library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ lubridate 1.9.4 ✔ tibble 3.2.1 ## ✔ purrr 1.0.4 ✔ tidyr 1.3.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(PolynomF) ## ## Attaching package: &#39;PolynomF&#39; ## ## The following object is masked from &#39;package:purrr&#39;: ## ## zap library(pracma) ## ## Attaching package: &#39;pracma&#39; ## ## The following objects are masked from &#39;package:PolynomF&#39;: ## ## integral, neville ## ## The following object is masked from &#39;package:purrr&#39;: ## ## cross 11.3 Example - CO2 at Mana Loa Observatory The Mana Loa observatory contains the longest in situ record of CO2 in the atmosphere. We’ve downloaded this data from ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_annmean_mlo.txt. Let’s read in this data and do some extrapolation. Instead of using the lin_interp function we wrote, we can just make linear regression model of the data. coln &lt;- c(&quot;year&quot;,&quot;co2&quot;,&quot;y&quot;) dfc &lt;- read_delim(&quot;data/co2_annmean_mlo.txt&quot;,delim = &quot; &quot;, comment = &quot;#&quot;,col_names = coln) ## Rows: 65 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot; &quot; ## chr (1): year ## dbl (2): co2, X5 ## lgl (2): y, X4 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. dfc$year &lt;- as.numeric(dfc$year) dfc$co2 &lt;- as.numeric((dfc$co2)) plot(dfc$year,dfc$co2) Using this data let’s extrapolate the CO2 level at 2100. We’ll start with linear interpolation and in the assignment you will try polynomial and splines. m &lt;- lm(co2 ~ year, data = dfc) xx &lt;- seq(1970, 2100, 1) xxd &lt;- data.frame(xx) colnames(xxd) &lt;- &quot;year&quot; yy &lt;- predict(m, xxd) lm_co2 &lt;- data.frame(cbind(xx, yy)) pp &lt;- ggplot() + geom_point( data = dfc, aes(x = year, y = co2), size = 1, col = &#39;blue&#39; ) pp + geom_line(data = lm_co2, aes(x = xx, y = yy)) Well over 550 ppm CO2 by 2100. But this doesn’t really fit the data very well. Perhaps we could improve a bit by using our lin_interp function on the last 2 datapoints. lin_interp &lt;- function(fx1,fx2,x1,x2,x3) fx1+((fx2-fx1)/(x2-x1))*(x3-x1) lin_extrap_data &lt;- tail(dfc, n = 2) lin_interp(fx1 = lin_extrap_data[[1,&quot;co2&quot;]], fx2 = lin_extrap_data[[2,&quot;co2&quot;]], x1 = lin_extrap_data[[1,&quot;year&quot;]], x2 = lin_extrap_data[[2,&quot;year&quot;]], x3 = 2100) ## [1] 617.43 m2 &lt;- lm(co2 ~ year, data = lin_extrap_data) 620 ppm! That’s more than 10% higher, but does it visually fit better? We can use the geom_abline function from ggplot2 to plot a line with the right slope and intercept, although it would probably be easier to go back and make a linear model with the lin_extrap_data and use predict to make a line as we did above. pp + geom_abline(slope = (lin_extrap_data[[2,&quot;co2&quot;]] - lin_extrap_data[[1,&quot;co2&quot;]]) / (lin_extrap_data[[2,&quot;year&quot;]] - lin_extrap_data[[1,&quot;year&quot;]]), intercept = lin_extrap_data[[1,&quot;co2&quot;]] - (lin_extrap_data[[2,&quot;co2&quot;]] - lin_extrap_data[[1,&quot;co2&quot;]]) / (lin_extrap_data[[2,&quot;year&quot;]] - lin_extrap_data[[1,&quot;year&quot;]])* lin_extrap_data[[1,&quot;year&quot;]], color = &quot;grey&quot;) + ylim(300, 800) + xlim(1970, 2100) ## Warning: Removed 11 rows containing missing values or values outside the scale range ## (`geom_point()`). What do you think the predictions from polynomial and splines will be? co2_poly &lt;- poly_calc(dfc$year, dfc$co2) dfc_pred &lt;- data.frame(year = xx, co2_poly = co2_poly(xx)) pp + geom_line(data = dfc_pred, mapping = aes(x = year, y = co2_poly)) m3 &lt;- lm(co2 ~ poly(x = year, degree = 6, raw = TRUE), data = dfc) summary(m3) ## ## Call: ## lm(formula = co2 ~ poly(x = year, degree = 6, raw = TRUE), data = dfc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.18271 -0.56983 -0.03815 0.51934 1.78748 ## ## Coefficients: (3 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.663e+05 1.329e+05 -2.004 0.0496 ## poly(x = year, degree = 6, raw = TRUE)1 4.248e+02 2.003e+02 2.121 0.0380 ## poly(x = year, degree = 6, raw = TRUE)2 -2.257e-01 1.006e-01 -2.243 0.0285 ## poly(x = year, degree = 6, raw = TRUE)3 3.998e-05 1.684e-05 2.374 0.0208 ## poly(x = year, degree = 6, raw = TRUE)4 NA NA NA NA ## poly(x = year, degree = 6, raw = TRUE)5 NA NA NA NA ## poly(x = year, degree = 6, raw = TRUE)6 NA NA NA NA ## ## (Intercept) * ## poly(x = year, degree = 6, raw = TRUE)1 * ## poly(x = year, degree = 6, raw = TRUE)2 * ## poly(x = year, degree = 6, raw = TRUE)3 * ## poly(x = year, degree = 6, raw = TRUE)4 ## poly(x = year, degree = 6, raw = TRUE)5 ## poly(x = year, degree = 6, raw = TRUE)6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7036 on 61 degrees of freedom ## Multiple R-squared: 0.9995, Adjusted R-squared: 0.9995 ## F-statistic: 4.226e+04 on 3 and 61 DF, p-value: &lt; 2.2e-16 dfc_pred$co2_m3 &lt;- m3 %&gt;% predict(dfc_pred) pp + geom_line(data = dfc_pred, mapping = aes(x = year, y = co2_m3)) To see the predicted CO2 concentration in the year 2100 we can use tail to look at the last line of dfc_pred dfc_pred %&gt;% tail(n = 1) ## year co2_poly co2_m3 ## 131 2100 5.394051e+154 738.7013 We could also use stat_smooth to plot the model object and include confidence intervals around our prediction. Within stat_smooth our formula needs to be in terms of x and y, which correspond to the variables mapped back in the ggplot call when we defined pp; so looking back aes(x = year, y = co2). To expand the range of the model predictions in stat_smooth we need to add fullrange = TRUE and then use xlim to expand the x-axis. ## Note that previously (and still above for pp) the data and mapping arguments ## were in the `geom_point`, ## which was why stat_smooth was not working in class. # TODO fix pp above, and also rename... ggplot(data = dfc, aes(x = year, y = co2)) + geom_point(size = 1) + stat_smooth(method = lm, formula = y ~ poly(x = x, degree = 3, raw = TRUE), fullrange = TRUE) + xlim(1959, 2100) Finally, we can use the method of splines by similarly defining a splines model using the bs (stands for b-splines, which are a type of spline) from the splines package. library(splines) m_splines &lt;- lm(co2 ~ splines::bs(year, knots = dfc$co2), data = dfc) dfc_pred$co2_m_splines &lt;- m_splines %&gt;% predict(dfc_pred) ## Warning in splines::bs(year, degree = 3L, knots = c(315.98, 316.91, 317.64, : ## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases pp + geom_line(data = dfc_pred, mapping = aes(x = year, y = co2_m_splines)) dfc_pred %&gt;% tail(n=1) ## year co2_poly co2_m3 co2_m_splines ## 131 2100 5.394051e+154 738.7013 738.7013 ggplot(data = dfc, aes(x = year, y = co2)) + geom_point(size = 1) + stat_smooth(method = lm, formula = y ~ bs(x = x, knots = dfc$co2), fullrange = TRUE) + xlim(1959, 2100) ## Warning in bs(x = x, degree = 3L, knots = c(315.98, 316.91, 317.64, 318.45, : ## some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases "],["differentiation-integration.html", "12 Differentiation &amp; Integration 12.1 Biggest takeaway - workflow 12.2 Reading", " 12 Differentiation &amp; Integration This week our goals are to be able to: Differentiation: Understand the concept of numerical differentiation and its application in solving differential equations. Apply numerical methods, such as finite differences, to calculate derivatives from experimental data. Explain the process of numerical integration and its significance in science and engineering. Apply numerical integration techniques, such as the trapezoidal rule, to calculate approximate areas under functions or data. Evaluate the limitations and associated error of basic numerical differentiation and integration methods and the advantages of using smoothing techniques, such as splines. Now that we’ve got some tools from earlier in this semester, we’re going to focus on differentiation &amp; integration. Specifically, we will look at examples that apply these calculus methods to real life scenarios. This topic will be particularly useful in future job positions. Being able to automate workflows that can quickly load data and analyze rates &amp; variable relationships has normalized educated &amp; efficient decision-making in the workplace. There will first be a differentiation problem &amp; then an integration problem - that integrates in 2 ways. We’ll use mathematical integration to quantify mass/volume under a curve. If you have mass/time on the y-axis, and time on the x-axis, then you can integrate under the observational points &amp; obtain the total mass that has passed through or accumulated in a system over a specific time period. We’ll start the HW assignment in class - it’s more like a case-study this week. You’ll draw on previous content you’ve learned, and use a new function for integration - trapz again from the pracma package. 12.1 Biggest takeaway - workflow The biggest thing I want you to get out of this week: workflow. That is, think about how you are going to accomplish this problem, break it down into steps, and then implement those steps. 12.2 Reading Skim through the examples. Watch at least the first 24 minutes of this Numerical Differentiation video: https://www.youtube.com/watch?v=9fGaTU1-f-0 Watch at least the first 23 minutes of this Numerical Integration video: https://www.youtube.com/watch?v=Xw6W3Ph3Wzc "],["week-12---differentiation-integration.html", "13 Week 12 - Differentiation &amp; Integration 13.1 Differentiation 13.2 Calculating derivative values from data 13.3 Integration 13.4 Objective: calculate total annual sediment [mass/year] and water [volume/year] for a given year for a specific station 13.5 Approach: 13.6 Workflow 13.7 Apply case scenario: 13.8 Compare 2011 and 2012 in case study", " 13 Week 12 - Differentiation &amp; Integration 13.1 Differentiation library(readr) library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) knitr::opts_chunk$set(echo = TRUE) One place were we often use numerical differentiation to solve problems is in predicting and comparing cell growth rates. Cell growth can be defined by the differential equation: \\[\\frac{dX}{dt} = \\mu X\\] where X is the concentration of cells and \\(\\mu\\) is the specific growth rate, which in the simplest case is just a rate constant. So the rate of change of the concentration of cells is proportional to the current concentration of cells. In the simplest case, where we can just solve this by integration by parts (IBP) to yield: \\[\\ln\\frac{X}{X_0} = \\mu t\\] or \\[X = X_0 e^{\\mu t}\\] However, this simple case often deviates from reality, especially as whatever food (substrate) the cells are growing on becomes limited as the cells grow and consume it. To describe this, we expand the specific growth rate to include a dependence on the substrate concentration, \\(S\\). This is called the Monod equation: \\[\\mu = \\frac{\\mu_{max}S}{K_S+S}\\] Consider the following variables from the above formula, \\(\\mu_{max}\\) and \\(K_S\\). Here \\(\\mu_{max}\\) and \\(K_S\\) are constants known as the maximum specific growth rate at saturating concentrations of substrate and the equilibrium concentration of substrate, respectively. Now, we rearrange the equations to obtain a form that is able to be differentiated. Adding this into the equation above: \\[\\frac{dX}{dt} = \\frac{\\mu_{max}S}{K_S+S} X\\] This model fits real data much better; as substrate gets consumed within the culture, the growth rate slows. Unfortunately, we now have 2 parameters to fit and a differential equation to deal with at the same time! 13.2 Calculating derivative values from data To fit these parameters, we often use numerical differentiation to find \\(dX/dt\\) values from \\(X\\) vs time data. We will also typically measure S vs time as well. This data might look like the below: cell_growth &lt;- read_csv(&quot;Data/Cell_Growth_Example_Data.csv&quot;) ## Rows: 8 Columns: 3 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (3): Time (h), Cell Concentration (g/L), Glucose Concentration (g/L) ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. knitr::kable(cell_growth) Time (h) Cell Concentration (g/L) Glucose Concentration (g/L) 0 1.25 100.00 9 2.45 97.00 16 5.10 90.40 23 10.50 76.90 30 22.00 48.10 34 33.00 20.60 36 37.50 9.38 40 41.00 0.63 In this table, Cell Concentration is \\(X\\) and Glucose Concentration is the substrate concentration \\(S\\). Now we can use this data to calculate \\(dX/dt\\) and regress this calculated derivative against \\(X\\) to find \\(\\mu\\) and then use a linearization of the Monod equation and an additional regression model to find the parameters \\(\\mu_{max}\\) and \\(K_S\\). 13.2.1 Difference methods An error-prone way to do this, which basically propagates any error in the data throughout the calculation, would be to just find \\(\\Delta X / \\Delta t\\) for each pair of points in the dataset. To do this, we can use the diff function from base R. diff calculates the difference between each value in a vector and the next value. Here are a couple of examples of diff # diff(1:5) ## [1] 1 1 1 1 diff(c(1,2,4,8,16)) ## [1] 1 2 4 8 And now to calculate the derivative at each point with diff dX_dt &lt;- diff(cell_growth$`Cell Concentration (g/L)`) / diff(cell_growth$`Time (h)`) Note that the length of this is vector is 1 shorter than the full dataset length(dX_dt) ## [1] 7 length(cell_growth$`Time (h)`) ## [1] 8 So we need to add in an NA for the first row where there can be no diff dX_dt &lt;- c(NA, dX_dt) cell_growth ## # A tibble: 8 × 3 ## `Time (h)` `Cell Concentration (g/L)` `Glucose Concentration (g/L)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1.25 100 ## 2 9 2.45 97 ## 3 16 5.1 90.4 ## 4 23 10.5 76.9 ## 5 30 22 48.1 ## 6 34 33 20.6 ## 7 36 37.5 9.38 ## 8 40 41 0.63 A tidyverse way to do this in one line without having to deal with the length difference is to use the lag function in a mutate call, instead of diff. lag and lead just shift the values in a vector left or right respectively and add an NA to the respective side. lag(1:5) ## [1] NA 1 2 3 4 lead(1:5) ## [1] 2 3 4 5 NA cell_growth %&gt;% mutate(dX_dt = (`Cell Concentration (g/L)` - lag(`Cell Concentration (g/L)`)) / (`Time (h)` - lag(`Time (h)`))) ## # A tibble: 8 × 4 ## `Time (h)` `Cell Concentration (g/L)` `Glucose Concentration (g/L)` dX_dt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1.25 100 NA ## 2 9 2.45 97 0.133 ## 3 16 5.1 90.4 0.379 ## 4 23 10.5 76.9 0.771 ## 5 30 22 48.1 1.64 ## 6 34 33 20.6 2.75 ## 7 36 37.5 9.38 2.25 ## 8 40 41 0.63 0.875 The above gives us values, but depending on how noisy the data is the derivative values could be really noisy. This is because in calculating the derivative the noise in two data points is combined, amplifying it. 13.2.2 Interpolation methods The best way to find the derivative of a dataset is to first fit a function to the dataset to smooth the noise of the data. Then find the derivative of that curve. We can use splines for this, and this will work in many general cases, when we don’t know the function. Since our data is pretty smooth and sparse (we don’t have any overlapping data points) we can adjust the degrees of freedom or df parameter of the model to best fit our data. This might require some knowledge of how the data should look to do effectively. For example if we just use the defaults in splines::bs: ggplot(data = cell_growth, mapping = aes(x = `Time (h)`, y = `Cell Concentration (g/L)`)) + geom_point() + stat_smooth(method = lm, formula = y ~ splines::bs(x = x)) It looks like we miss the slow down in cell growth at to the top of the curve which is what we expect because the substrate is nearly completely consumed. We can use the df argument in bs to include more splines and better fit the data. The default for df is 3, 8 data points minus the two outer point, minus 3 for the cubic degree of the splines. Let’s increase df to 4. ggplot(data = cell_growth, mapping = aes(x = `Time (h)`, y = `Cell Concentration (g/L)`)) + geom_point() + stat_smooth(method = lm, formula = y ~ splines::bs(x = x, df = 4)) #add df That looks better. Now we can recreate this model in lm defining an object that we can use to predict values and define derivatives. Since we have a smooth function now we can use a very small interval to calculate the derivatives, for example 0.5 hours. We also don’t have to worry about not having derivative values at the first or last datapoints. X_t &lt;- lm(X ~ splines::bs(x = t, df = 4), data = cell_growth %&gt;% rename(X = `Cell Concentration (g/L)` , t = `Time (h)`)) # NOTE: I had to change the names of the columns here because the spaces and parentheses were F-ing things up with predict delta_t &lt;- 0.1 # remember predict uses data frames to define variable names t_new &lt;- data.frame(t = cell_growth$`Time (h)` - delta_t) #without the renaming step the following was returning all TRUE because predict defaults to using the data passed to lm, so when it didn&#39;t recognize the variable names it was using the default data predict(object = X_t) == predict(object = X_t, newdata = t_new) ## Warning in splines::bs(x = t, degree = 3L, knots = 26.5, Boundary.knots = c(0, ## : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases ## 1 2 3 4 5 6 7 8 ## FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE cell_growth %&gt;% mutate(dX_dt = (predict(X_t) - predict(X_t, t_new))/delta_t) -&gt; cell_growth ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `dX_dt = (predict(X_t) - predict(X_t, t_new))/delta_t`. ## Caused by warning in `splines::bs()`: ## ! some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases OK! So now we have \\(dX/dt\\) values. Now we can calculate \\(\\mu\\) values, by dividing \\(dX/dt\\) by \\(X\\). I’m also going to fix the column names so we don’t run into any more issues. cell_growth &lt;- cell_growth %&gt;% rename(t = `Time (h)`, X = `Cell Concentration (g/L)`, S = `Glucose Concentration (g/L)`) %&gt;% mutate(mu = dX_dt / X) Now that we have our \\(\\mu\\) values we can figure out how to model the Monod equation so that we can find values for the \\(\\mu_{max}\\) and \\(K_S\\) for these cells and media with glucose as a limiting substrate. Because the Monod equation is very nonlinear one of the easiest ways to find parameters for this equation is to linearize it by algebraic manipulation. \\[\\mu = \\frac{\\mu_{max}S}{K_S + S}\\] To linearize here we will multiply by \\(K_S + S\\) \\[(\\mu*K_S) + (\\mu*S) = \\mu_{max}S\\] and then to get this into a convenient form of \\(y = mx + b\\) (so we can calculate y and x values using our data table) we can subtract \\(\\muK_S\\) and divide by \\(S\\) to yield: \\[\\mu = - K_S\\frac{\\mu}{S} + \\mu_{max}\\] This is called the Eadie-Hofstee linearization. So if we plot \\(\\mu\\) vs \\(\\mu/S\\) and find the slope and intercept of a regression model the slope will be \\(-K_S\\) and the intercept will be \\(\\mu_{max}\\). cell_growth %&gt;% mutate(EH_X = mu / S) -&gt; cell_growth EH &lt;- lm(mu ~ EH_X, data = cell_growth) summary(EH) ## ## Call: ## lm(formula = mu ~ EH_X, data = cell_growth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.092115 -0.051180 -0.041225 -0.008532 0.293031 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.12952 0.06073 2.133 0.0769 . ## EH_X -4.70939 8.97458 -0.525 0.6186 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1315 on 6 degrees of freedom ## Multiple R-squared: 0.04388, Adjusted R-squared: -0.1155 ## F-statistic: 0.2754 on 1 and 6 DF, p-value: 0.6186 So the \\(\\mu_{max}\\) is 0.1295245 1/h and \\(K_S\\) is -4.709391 g/L. Now we can define a function to predict the \\(\\DeltaX\\) for each timestep, calculate the predicted \\(X\\) values and add this to our plot. cell_growth_monod_dX &lt;- function(S, dt, mu_max, K_S) { dX &lt;- mu_max * S / (K_S + S ) * dt return(dX) } cell_growth &lt;- cell_growth %&gt;% mutate(monodX = X + cell_growth_monod_dX(S = .$S, dt = .$t - lag(.$t), mu_max = EH$coefficients[1], K_S = -EH$coefficients[2])) ggplot(data = cell_growth, mapping = aes(x = t, y = X)) + geom_point() + geom_point(aes(y = monodX), color = &quot;green&quot;) ## Warning: Removed 1 row containing missing values or values outside the scale range ## (`geom_point()`). 13.3 Integration library(dataRetrieval) library(ggplot2) library(pracma) library(readxl) library(dplyr) library(cowplot) library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:cowplot&#39;: ## ## stamp ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union Paper source: Rosburg et al. (2016). The effect of flow data resolution on sediment yield estimation and channel design. https://doi.org/10.1016/j.jhydrol.2016.04.040 13.4 Objective: calculate total annual sediment [mass/year] and water [volume/year] for a given year for a specific station 13.5 Approach: Here, you’ll apply numerical integration using the trapz function in pracma. You’ll want to set up a data frame of x and y, with x = day of year ranging between 1-365; y is the sediment load based on the equation and given streamflow). 13.6 Workflow Read in sediment-rating curve (data here in the file 1-s2.0-S0022169416302311-mmc2.xlsx) Read in streamflow data using readNWISdv from dataRetrieval package Calculate average daily sediment yield [kg/s] For a given year, integrate under the daily sediment yield and daily streamflow using trapz function within the pracma package. 13.7 Apply case scenario: Compare the 2011 and 2012 year for water and sediment in the Lower Mississippi River (station id 07374000). Create 1 plot (with day of year on x-axis) of streamflow (2 lines, one for 2011 and one for 2012); Create 1 plot (with day of year on x-axis) of sediment flux (2 lines, one for 2011 and one for 2012). Calculate total annual sediment mass and water volume and compare the values between 2011 and 2012. Is the ratio of sediment mass for each year the same as the ratio of water volume (e.g. mass sed 2012/mass sed 2011)? In other words, was the streamflow carrying the same amount of sediment in 2011 and 2012? Create clear graphics and write up your comparison in a paragraph. 13.7.0.1 Background on sediment rating curve table and the coefficients \\(a\\) and \\(b\\): From Rosburg et al.: Relationships between water discharge and sediment load often can be expressed as a simple power function: \\[Q_s = aQ^b\\] where \\(Q_s\\) is the sediment discharge rate (kg/s), \\(Q\\) is the concurrently-measured water discharge rate (m3/s), and \\(a\\) and \\(b\\) are best fit regression parameters (Asselman, 2000, Syvitski et al., 2000). This function is referred to as a sediment rating curve. In this relationship, it has been suggested that the exponent \\(b\\) is related to the transport capacity in excess of sediment supply within a river channel, while the coefficient \\(a\\) is related to absolute sediment supply (Barry et al., 2004). The exponent, \\(b\\), tends to increase with the size of bed material in coarse bed rivers (Emmett and Wolman, 2001). 13.7.1 Read in sediment rating curve table ds &lt;- readxl::read_excel(&quot;Data/dataset-1-s2.0-S0022169416302311-mmc2.xlsx&quot;, col_types = c(&quot;text&quot;, &quot;text&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;text&quot;, &quot;text&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;text&quot;), skip = 1) 13.7.2 Station Identification and Data Retreival site_id &lt;- &#39;07374000&#39; startDate &lt;- &#39;2000-01-01&#39; endDate &lt;- &#39;2020-12-30&#39; pCode &lt;- &#39;00060&#39; rawDailyQ &lt;- readNWISdv(siteNumbers = site_id, parameterCd = pCode, startDate = startDate, endDate = endDate) ## GET:https://waterservices.usgs.gov/nwis/dv/?site=07374000&amp;format=waterml%2C1.1&amp;ParameterCd=00060&amp;StatCd=00003&amp;startDT=2000-01-01&amp;endDT=2020-12-30 13.7.3 Filter sediment table for station of interest dsm &lt;- ds[which(ds$`Site Number`==site_id),] 13.7.4 Calculate Daily sediment mass flux #cfs Q_cms &lt;- rawDailyQ$X_00060_00003/3.28084^3 # conversion to cms a &lt;- dsm$a # set a coefficient from filtered rating curve table b &lt;- dsm$b # set b coefficient from filtered rating curve table Qs_kgs &lt;- a*Q_cms^b Qs_kgd &lt;- Qs_kgs*60*60*24 # sediment load per day in kg/day d &lt;- data.frame(Q_cms,Qs_kgd,rawDailyQ$Date) colnames(d)[3] &lt;- &quot;Date&quot; p &lt;- ggplot(d, aes(Q_cms, Qs_kgd)) + geom_line() + theme_cowplot() + xlab(&#39;Streamflow [cms]&#39;) + ylab(&#39;Sediment load [kg per day]&#39;) p p2 &lt;- ggplot(rawDailyQ, aes(Date,X_00060_00003)) + geom_point() + theme_cowplot() + xlab(&#39;Date&#39;) + ylab(&#39;Streamflow [cfs]&#39;) p2 p3 &lt;- ggplot(d, aes(Date,Qs_kgd)) + geom_point() + theme_cowplot() + xlab(&#39;Date&#39;) + ylab(&#39;Sediment Flux [kg/d]&#39;) p3 13.8 Compare 2011 and 2012 in case study d1 &lt;- filter(d, Date &gt; &quot;2010-12-31&quot; &amp; Date &lt; &quot;2012-01-01&quot;) d2011 &lt;- trapz(d1$Qs_kgd)*.001 # include conversion seconds to days, kg to metric tons q2011 &lt;- trapz(d1$Q_cms*86400)/1e9 # include conversion seconds to days, scale by 1e9, so units are billion cubic meters q2011 ## [1] 543.9495 d2011 ## [1] 1054709 d2 &lt;- filter(d, Date &gt; &quot;2011-12-31&quot; &amp; Date &lt; &quot;2013-01-01&quot;) d2012 &lt;- trapz(d2$Qs_kgd)*.001 # include conversion seconds to days, kg to metric tons q2012 &lt;- trapz(d2$Q_cms*86400)/1e9 # include conversion seconds to days, scale by 1e9, so units are billion cubic meters q2012 ## [1] 319.2169 d2012 ## [1] 194539.8 q2012/q2011 # ratio of total water volume per year ## [1] 0.5868502 d2012/d2011 # ratio of total sediment mass per year ## [1] 0.1844487 p2 &lt;- ggplot() + geom_line(data=d1, aes(x=yday(Date),y=Q_cms), color=&quot;blue&quot;) + geom_line(data=d2, aes(x=yday(Date),y=Q_cms), color=&quot;red&quot;) + theme_cowplot() + xlab(&#39;Day of year&#39;) + ylab(&#39;Streamflow [cfs]&#39;) p2 p3 &lt;- ggplot() + geom_line(data=d1, aes(x=yday(Date),y=Qs_kgd), color=&quot;blue&quot;) + geom_line(data=d2, aes(x=yday(Date),y=Qs_kgd), color=&quot;red&quot;) + #theme_cowplot() + xlab(&#39;Day of year&#39;) + ylab(&#39;Sediment load [kgd]&#39;) + theme(legend.position = &quot;right&quot;) p3 d_f &lt;- d %&gt;% #create a year column mutate(yr=year(Date)) %&gt;% # group by year and then integrate to find total volume or mass for each year group_by(yr) %&gt;% mutate(Q_int = trapz(Q_cms)*8640/1e9) %&gt;% mutate(Qs_int = trapz(Qs_kgd)*.001) %&gt;% summarize_all(max) d_f ## # A tibble: 17 × 6 ## yr Q_cms Qs_kgd Date Q_int Qs_int ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004 26221. 4553560. 2004-12-31 38.2 322037. ## 2 2005 32848. 9862029. 2005-12-31 38.2 367596. ## 3 2006 21266. 2219833. 2006-12-31 31.6 110515. ## 4 2007 25315. 4036111. 2007-12-31 39.1 235636. ## 5 2008 37378. 15361866. 2008-12-31 55.4 950416. ## 6 2009 33414. 10457567. 2009-12-31 56.3 699609. ## 7 2010 29450. 6781094. 2010-12-31 51.0 546120. ## 8 2011 40493. 20215198. 2011-12-31 54.4 1054709. ## 9 2012 23390. 3076925. 2012-12-31 31.9 194540. ## 10 2013 28317. 5927552. 2013-12-31 47.7 468387. ## 11 2014 24239. 3477446. 2014-12-31 43.6 263207. ## 12 2015 30016. 7238932. 2015-12-31 55.2 753439. ## 13 2016 37661. 15764729. 2016-12-31 55.5 851360. ## 14 2017 34547. 11724372. 2017-12-31 48.4 579603. ## 15 2018 38228. 16592820. 2018-12-31 59.7 974006. ## 16 2019 38794. 17451266. 2019-12-31 81.3 2325045. ## 17 2020 38511. 17018208. 2020-12-30 65.3 1584083. d_f$rq &lt;- d_f$Qs_int/d_f$Q_int/1000 d_f$m &lt;- d_f$Qs_int/d_f$Q_cms d_f ## # A tibble: 17 × 8 ## yr Q_cms Qs_kgd Date Q_int Qs_int rq m ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2004 26221. 4553560. 2004-12-31 38.2 322037. 8.42 12.3 ## 2 2005 32848. 9862029. 2005-12-31 38.2 367596. 9.63 11.2 ## 3 2006 21266. 2219833. 2006-12-31 31.6 110515. 3.50 5.20 ## 4 2007 25315. 4036111. 2007-12-31 39.1 235636. 6.02 9.31 ## 5 2008 37378. 15361866. 2008-12-31 55.4 950416. 17.2 25.4 ## 6 2009 33414. 10457567. 2009-12-31 56.3 699609. 12.4 20.9 ## 7 2010 29450. 6781094. 2010-12-31 51.0 546120. 10.7 18.5 ## 8 2011 40493. 20215198. 2011-12-31 54.4 1054709. 19.4 26.0 ## 9 2012 23390. 3076925. 2012-12-31 31.9 194540. 6.09 8.32 ## 10 2013 28317. 5927552. 2013-12-31 47.7 468387. 9.81 16.5 ## 11 2014 24239. 3477446. 2014-12-31 43.6 263207. 6.04 10.9 ## 12 2015 30016. 7238932. 2015-12-31 55.2 753439. 13.6 25.1 ## 13 2016 37661. 15764729. 2016-12-31 55.5 851360. 15.3 22.6 ## 14 2017 34547. 11724372. 2017-12-31 48.4 579603. 12.0 16.8 ## 15 2018 38228. 16592820. 2018-12-31 59.7 974006. 16.3 25.5 ## 16 2019 38794. 17451266. 2019-12-31 81.3 2325045. 28.6 59.9 ## 17 2020 38511. 17018208. 2020-12-30 65.3 1584083. 24.2 41.1 Let’s build one single dataset from the 2011 and 2012 data above so that we can make a nice legend on our plot. data_2011_2012 &lt;- bind_rows(d1, d2) %&gt;% mutate(Year = year(Date) %&gt;% as.factor(), DOTY = yday(Date)) ggplot(data = data_2011_2012, mapping = aes(x = DOTY, y = Qs_kgd, color = Year)) + geom_line() + theme_cowplot() + xlab(&#39;Day of year&#39;) + ylab(&#39;Sediment load [kgd]&#39;) "],["systems-of-equations.html", "14 Systems of Equations 14.1 This week’s prep:", " 14 Systems of Equations This week, we will be focusing on Systems of Equations in R. Please review the following readings to refresh your linear algebra skills. 14.1 This week’s prep: (Also found on Week 13 Canvas page) 14.1.1 Required reading: Primer - Linear Algebra can be performed easily by computers, but we need to remind ourselves of how to set up problems as a set of matrices before we can so https://drive.google.com/file/d/132NvRrMVbBYNex5iNhoeBwC1ck5MZMAB/view?usp=sharing Matrix Algebra Overview - basics, notation, refresher on matrix multiplication, using inverse (e.g. eq 8.7), and converting equations to matrix form (section 8.1.3) https://drive.google.com/file/d/132NvRrMVbBYNex5iNhoeBwC1ck5MZMAB/view?usp=sharing Matrix operations in R - https://www.tutorialkart.com/r-tutorial/r-matrix/ 14.1.2 Optional resources: (these materials will be reviewed in class, but are here if you want to get a head start) Matrix overview slides - https://docs.google.com/presentation/d/1Dp0XE1iyCFbnKjpW4kCnsMJ9ldn0cEAL "],["systems-of-equations-examples.html", "Systems of Equations Examples 14.2 Example 1 14.3 Example 2 14.4 Example 3 (Problem 11.3 in Chapra)", " Systems of Equations Examples 14.2 Example 1 Imagine you have a simple set of equations, and you plot them out. The solution is at the intersection if you plot them. Let’s solve using matrix algebra: \\[ 3x_1 + 2x_2 = 18 \\\\ -x_1 + 2x_2 = 2 \\] # Define our matrix of coefficients coef_matrix &lt;- matrix(data = c(3, 2, -1, 2), nrow = 2, ncol = 2, byrow = TRUE) # Define right hand side matrix rhs_matrix &lt;- matrix( data = c(18, 2), nrow = 2, ncol = 1, byrow = TRUE ) # Solving the system of equations # using inverse # calculate the inverse of coef_matrix inv_coef_matrix &lt;- solve(coef_matrix) # multiply inverse coefficient matrix by rhs to get the solution soln &lt;- inv_coef_matrix %*% rhs_matrix soln ## [,1] ## [1,] 4 ## [2,] 3 # Solve using the &#39;solve&#39; function solve(coef_matrix, rhs_matrix) ## [,1] ## [1,] 4 ## [2,] 3 We’ll employ the solve function. When you enter just the A matrix, you get the inverse as the output. When you enter the A and B matrix/vectors, you’ll get the x vector. matrix(vector, ncol = col, byrow = TRUE) = define a matrix from vector filling in values across col columns starting with the first row and proceeding left to right top to bottom. %*% = matrix multiplication operator. solve = function for solving systems, can input A or A &amp; B. 14.2.1 Simple Matrix and solving for X A &lt;- matrix(c(3, 2, -1, 2), ncol = 2, byrow=TRUE) Ainv &lt;- solve(A) B &lt;- matrix(c(18,2), ncol = 1, byrow=TRUE) X &lt;- Ainv %*% B #or X &lt;- solve(A,B) X ## [,1] ## [1,] 4 ## [2,] 3 14.3 Example 2 \\[ \\begin{aligned} 10 x_1 + 2x_2 - x_3 &amp; = 27 \\\\ -3 x_1 - 6x_2 + 2x_3 &amp; = -61.5 \\\\ x_1 + x_2 + 5x_3 &amp; = -21.5 \\end{aligned} \\] A &lt;- matrix(c(10, 2, -1, -3, -6, 2, 1, 1, 5), nrow = 3, byrow = TRUE) B &lt;- matrix(data = c(27, -61.5, -21.5), ncol = 1, byrow = TRUE) x &lt;- inv(A) %*% B x ## [,1] ## [1,] 0.5 ## [2,] 8.0 ## [3,] -6.0 14.4 Example 3 (Problem 11.3 in Chapra) \\[ \\begin{aligned} In &amp;= Out \\\\ \\text{Tank 1: } 3800 + 3c_2 + 1c_3 &amp;= (8 + 3 + 4)c_1 \\\\ \\text{Tank 2: } 1200 + 3c_1 + 6c_3 &amp;= (3 + 1 + 14)c_2 \\\\ \\text{Tank 3: } 2350 + 4c_1 + 1c_2 &amp;= (5 + 6 + 1)c_3 \\end{aligned} \\] and rearranging and simplifying this \\[ \\begin{aligned} \\text{Tank 1: } 3800 &amp;= 15c_1 &amp;- 3c_2 &amp;- 1c_3\\\\ \\text{Tank 2: } 1200 &amp;= -3c_1 &amp;+ 18c_2 &amp;- 6c_3 \\\\ \\text{Tank 3: } 2350 &amp;= - 4c_1 &amp;- 1c_2 &amp;+ 12c_3 \\end{aligned} \\] Here we’ll use the PRACMA package that uses mldivide to solve for x in A*x=b. We can also first solve for the inverse of A using solve(A). library(pracma) A &lt;- matrix(c(15, -3, -1, -3, 18, -6, -4, -1, 12), ncol = 3, byrow=TRUE) A ## [,1] [,2] [,3] ## [1,] 15 -3 -1 ## [2,] -3 18 -6 ## [3,] -4 -1 12 invA &lt;- solve(A) invA[1,1] ## [1] 0.07253886 b &lt;- matrix(c(3800,1200,2350)) b ## [,1] ## [1,] 3800 ## [2,] 1200 ## [3,] 2350 x &lt;- invA %*% b x &lt;- mldivide(A, b) # matrix multiplication vs element by element x ## [,1] ## [1,] 320.2073 ## [2,] 227.2021 ## [3,] 321.5026 14.4.1 To find effect of reactor 3 on reactor 1…. This is where the matrix inverse is really cool, and provides an opportunity to quantify change in one part of a system on another component. Here, imagine we make a change in reactor 3, and want to know how this in turn changes the concentration in reactor 1. We’ll walk through this in class in some more detail, but the basic premise is the following: when we multiple the inverse of A by b, the first equation for reactor 1 is the following: Equation Thus we can use the circled part to determine the effect of reactor 3 on 1! x1effect &lt;- invA[1,3] * b[3] x1effect ## [1] 29.2228 "],["assessment-2.html", "15 Assessment 2 15.1 Directions 15.2 Finding a data set 15.3 Questions/Tasks 15.4 Guideline on the Use of AI Tools", " 15 Assessment 2 In today’s data-driven world, the ability to work with real-world datasets and draw meaningful insights is an essential skill for engineers and scientists. This assessment is designed to give you hands-on experience in exploring, cleaning, transforming, and visualizing data in R. Unlike traditional problem sets, this project empowers you to choose a dataset that aligns with your interests—whether related to health, environment, agriculture, or another topic—and apply the data wrangling and visualization skills you’ve developed in class. The goal of this project is to help you move beyond following code examples to thinking critically and creatively about data: formulating your own questions, designing a strategy to answer them, and communicating your results effectively. Along the way, you’ll also reflect on the role of emerging tools like AI in the data analysis process, learning to use them thoughtfully and responsibly. By the end of this assessment, you should be able to: Identify and obtain a relevant and well-structured dataset Tidy and transform data using tools from the tidyverse Pose and answer meaningful questions through summary statistics and visualizations Document and reflect on your data analysis process, including any use of AI tools This is an opportunity to demonstrate not only your technical skills, but your curiosity, creativity, and ability to engage with data in a real-world context. You can download the project template here: https://github.com/VT-BSE3144/07_Assessment1/blob/main/Assessment1.Rmd 15.1 Directions In this project-based assessment you will identify a data set you are interested in and demonstrate your skills of data wrangling and visualization through asking questions about the dataset. 15.2 Finding a data set Data is everywhere these days. Below we have some recommendations about where to find datasets to use for this assessment, but there are many more options out there, from different governmental agencies, to research papers, to data repositories. Google a topic you are interested in plus “data” and see what you can find, or check out the resources below. The critical things a dataset must have for this assessment are: ability to be downloaded (as a csv/tsv/excel/sas/stata/spss file ideally) at least 1 numerical variable at least 1 categorical variable something that you care about, so you are willing to put some time into this assessment 15.2.1 General https://data.world/datasets - data.world has thousands of data sets on all kinds of different topics that are all open and freely available, but you have to make an account. https://datadryad.org/stash - This site is a repository for all kinds of research data, use the search tool to find something you are interested in. https://udc.vt.edu/ - Virginia Tech’s University data commons has all kinds of data about our campus community over time that can be visualized in different ways and downloaded as CSVs. https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html - The R datasets package has lots of different data sets on a wide variety of topics https://www.vdh.virginia.gov/data/ - Virginia Department of Health has lots of datasets, mostly health related but also some environmental datasets. 15.2.2 Health Safe Drinking Water Information System (SDWIS) - data on centralized water service design and violations throughout the United States. (https://www.epa.gov/ground-water-and-drinking-water/safe-drinking-water-information-system-sdwis-federal-reporting) https://www.who.int/data - The World Health organization has lots of very interesting datasets that are pretty accessible. We would recommend in particular, Child mortality and causes of death and Life expectancy data by country. https://healthdata.gov/ - This site is dedicated to making high value health data more accessible to entrepreneurs, researchers, and policy makers in the hopes of better health outcomes. https://www.nyam.org/library/collections-and-resources/data-sets/ - The New York Academy of Medicine has links to many publicly available, medical data sets. https://www.cdc.gov/datastatistics/index.html - CDC has many datasets, some may be difficult to access or read into R, as they often have their own data visualization tools, but with some digging you can find the raw datasets. 15.2.3 Environment https://echo.epa.gov/ - USEPA Enforcement and Compliance System (ECHO) - all permitted releases to surface waters in the United States. https://vtstreamlab.weebly.com/live-data.html - StREAM Lab - real-time monitoring of water quantity and quality at Stroubles Creek. https://www.deq.virginia.gov/topics-of-interest/per-and-polyfluoroalkyl-substances-pfas - VADEQ PFAS monitoring data - Forever chemicals PFAS, perfluoroalkyl substances, measured in water sources throughout Virginia. https://waterdata.usgs.gov/nwis - These pages provide access to water-resources data collected at approximately 1.9 million sites across the US and its territories. https://data.noaa.gov/datasetsearch/ - NOAA has many datasets related to the environment from weather and water, to ecology and environmental health. These are generally pretty accessible too. 15.2.4 Agriculture https://datl-chandel.github.io/Agroclimate/ - Agroclimate Viewer &amp; Planner App - This tools enables monitoring crop health (from satellite imagery), weather history and 16 day forecast, and soil properties. https://data.nal.usda.gov/ - The USDA has a large collection of agriculturally relevant data sets. 15.3 Questions/Tasks Any text that is flanked by two asterisk’s ** (which causes text to appear in bold face in the “Visual” version of the document) is a prompt for you to answer or fill in details below. Outside of code chunks, this text will appear as bold. We have still added the ** inside of comments in code chunks so you will know where you need to fill in. 15.4 Guideline on the Use of AI Tools In this assignment, you are welcome to use AI tools (such as ChatGPT, Claude, Copilot, automated data cleaning packages, etc) to support your brainstorming, code generation, data wrangling, or visualization tasks. However, it is essential that you use these tools critically and responsibly. Specifically consider: Verification and Validation: Always verify and validate any output generated by AI tools. Ensure that the code and analysis you submit is accurate, reproducible, and truly reflects your own understanding. Documentation: Clearly document any instance where AI tools were used. Describe the process, the suggestions provided by the AI, and any modifications you made to tailor the output to your project’s needs. Ethical Considerations: Reflect on and acknowledge the benefits and limitations of using AI. Consider issues such as potential biases in AI-generated outputs, the transparency of the process, and the ethical implications of relying on automated tools in data analysis. By adhering to these guidelines, you will demonstrate both technical proficiency and a critical, ethical approach to the integration of AI in your data analysis process. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
