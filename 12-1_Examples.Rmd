---
output: html_document
params:
  version: 1.0
---

# Week 12 - Differentiation & Integration

## Differentiation

```{r setup}
library(readr)
library(dplyr)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

One place were we often use numerical differentiation to solve problems is in predicting and comparing cell growth rates. Cell growth can be defined by the differential equation:
$$\frac{dX}{dt} = \mu X$$
where X is the concentration of cells and $\mu$ is the specific growth rate, which in the simplest case is just a rate constant. So the rate of change of the concentration of cells is proportional to the current concentration of cells. In the simplest case, where  we can just solve this by integration by parts (IBP) to yield:
$$\ln\frac{X}{X_0} = \mu t$$
or 
$$X = X_0 e^{\mu t}$$

However, this simple case often deviates from reality, especially as whatever food (substrate) the cells are growing on becomes limited as the cells grow and consume it. To describe this, we expand the specific growth rate to include a dependence on the substrate concentration, $S$. This is called the Monod equation:
$$\mu = \frac{\mu_{max}S}{K_S+S}$$
Consider the following variables from the above formula, 
$\mu_{max}$ and $K_S$. Here $\mu_{max}$ and $K_S$ are constants known as the maximum specific growth rate at saturating concentrations of substrate and the equilibrium concentration of substrate, respectively. 

Now, we rearrange the equations to obtain a form that is able to be differentiated.

Adding this into the equation above:
$$\frac{dX}{dt} = \frac{\mu_{max}S}{K_S+S} X$$
This model fits real data much better; as substrate gets consumed within the culture, the growth rate slows. Unfortunately, we now have 2 parameters to fit and a differential equation to deal with at the same time!

## Calculating derivative values from data

To fit these parameters, we often use numerical differentiation to find $dX/dt$ values from $X$ vs time data. We will also typically measure S vs time as well. This data might look like the below:

```{r}
cell_growth <- read_csv("Data/Cell_Growth_Example_Data.csv")
knitr::kable(cell_growth)
```

In this table, Cell Concentration is $X$ and Glucose Concentration is the substrate concentration $S$. 

Now we can use this data to calculate $dX/dt$ and regress this calculated derivative against $X$ to find $\mu$ and then use a linearization of the Monod equation and an additional regression model to find the parameters $\mu_{max}$ and $K_S$.

### Difference methods

An error-prone way to do this, which basically propagates any error in the data throughout the calculation, would be to just find $\Delta X / \Delta t$ for each pair of points in the dataset. To do this, we can use the `diff` function from base R. `diff` calculates the difference between each value in a vector and the next value.

Here are  a couple of examples of `diff`
```{r}
# 
diff(1:5)
diff(c(1,2,4,8,16))
```

And now to calculate the derivative at each point with `diff`
```{r}
dX_dt <- diff(cell_growth$`Cell Concentration (g/L)`) / 
  diff(cell_growth$`Time (h)`) 
```

Note that the length of this is vector is 1 shorter than the full dataset
```{r}
length(dX_dt)
length(cell_growth$`Time (h)`)
```

So we need to add in an NA for the first row where there can be no diff 
```{r}
dX_dt <-  c(NA, dX_dt)
cell_growth
```

A tidyverse way to do this in one line without having to deal with the length difference is to use the `lag` function in a mutate call, instead of `diff`. `lag` and `lead` just shift the values in a vector left or right respectively and add an `NA` to the respective side.  
```{r}
lag(1:5)
lead(1:5)
cell_growth %>% 
  mutate(dX_dt  = (`Cell Concentration (g/L)` - 
                     lag(`Cell Concentration (g/L)`)) / 
         (`Time (h)` - lag(`Time (h)`)))  
```

The above gives us values, but depending on how noisy the data is the derivative values could be really noisy. This is because in calculating the derivative the noise in two data points is combined, amplifying it.  

### Interpolation methods

The best way to find the derivative of a dataset is to first fit a function to the dataset to smooth the noise of the data. Then find the derivative of that curve. We can use splines for this, and this will work in many general cases, when we don't know the function. Since our data is pretty smooth and sparse (we don't have any overlapping data points) we can adjust the degrees of freedom or `df` parameter of the model to best fit our data. This might require some knowledge of how the data should look to do effectively. For example if we just use the defaults in `splines::bs`:

```{r}
ggplot(data = cell_growth, 
       mapping = aes(x = `Time (h)`, y = `Cell Concentration (g/L)`)) + 
  geom_point() + 
  stat_smooth(method = lm, 
              formula = y ~ splines::bs(x = x))
```

It looks like we miss the slow down in cell growth at to the top of the curve which is what we expect because the substrate is nearly completely consumed. We can use the `df` argument in `bs` to include more splines and better fit the data. The default for `df` is 3, 8 data points minus the two outer point, minus 3 for the cubic degree of the splines. Let's increase `df` to 4.   

```{r}
ggplot(data = cell_growth, 
       mapping = aes(x = `Time (h)`, y = `Cell Concentration (g/L)`)) + 
  geom_point() + 
  stat_smooth(method = lm, 
              formula = y ~ splines::bs(x = x, df = 4)) #add df
```

That looks better. Now we can recreate this model in `lm` defining an object that we can use to predict values and define derivatives. Since we have a smooth function now we can use a very small interval to calculate the derivatives, for example 0.5 hours. We also don't have to worry about not having derivative values at the first or last datapoints. 
```{r}
X_t <- lm(X ~ splines::bs(x = t, df = 4), 
          data = cell_growth %>% 
            rename(X = `Cell Concentration (g/L)` , t = `Time (h)`))
# NOTE: I had to change the names of the columns here because the spaces and parentheses were F-ing things up with predict
delta_t <- 0.1
# remember predict uses data frames to define variable names
t_new <- data.frame(t = cell_growth$`Time (h)` - delta_t)
#without the renaming step the following was returning all TRUE because predict defaults to using the data passed to lm, so when it didn't recognize the variable names it was using the default data
predict(object = X_t) == predict(object = X_t, newdata = t_new)

cell_growth %>% 
  mutate(dX_dt  = (predict(X_t)  - 
                     predict(X_t, t_new))/delta_t) ->
  cell_growth

```

OK! So now we have $dX/dt$ values. Now we can calculate $\mu$ values, by dividing $dX/dt$ by $X$. I'm also going to fix the column names so we don't run into any more issues.
```{r}
cell_growth <-
cell_growth %>% 
  rename(t = `Time (h)`, 
         X = `Cell Concentration (g/L)`, 
         S = `Glucose Concentration (g/L)`) %>%
  mutate(mu = dX_dt / X)
```

Now that we have our $\mu$ values we can figure out how to model the Monod equation so that we can find values for the $\mu_{max}$ and $K_S$ for these cells and media with glucose as a limiting substrate. Because the Monod equation is very nonlinear one of the easiest ways to find parameters for this equation is to linearize it by algebraic manipulation. 
$$\mu = \frac{\mu_{max}S}{K_S + S}$$
To linearize here we will multiply by $K_S + S$ 
$$(\mu*K_S) + (\mu*S) = \mu_{max}S$$
and then to get this into a convenient form of $y = mx + b$ (so we can calculate y and x values using our data table) we can subtract $\muK_S$ and divide by $S$ to yield:
$$\mu = - K_S\frac{\mu}{S} + \mu_{max}$$
This is called the Eadie-Hofstee linearization. So if we plot $\mu$ vs $\mu/S$ and find the slope and intercept of a regression model the slope will be $-K_S$ and the intercept will be $\mu_{max}$.
```{r}
cell_growth %>% mutate(EH_X = mu / S) ->
  cell_growth
EH <- lm(mu ~ EH_X, data = cell_growth)
summary(EH)
```

So the $\mu_{max}$ is `r EH$coefficients[1]` 1/h and $K_S$ is `r EH$coefficients[2]` g/L. Now we can define a function to predict the $\DeltaX$ for each timestep, calculate the predicted $X$ values and add this to our plot. 
```{r}
cell_growth_monod_dX <- function(S, dt, mu_max, K_S) {
  dX <- mu_max * S / (K_S + S ) * dt
  return(dX)
}

cell_growth <- 
  cell_growth %>% 
  mutate(monodX = X + cell_growth_monod_dX(S = .$S, 
                                   dt = .$t - lag(.$t),
                                   mu_max = EH$coefficients[1],
                                   K_S = -EH$coefficients[2]))

ggplot(data = cell_growth, 
       mapping = aes(x = t, y = X)) + 
  geom_point() +
  geom_point(aes(y = monodX), color = "green")
```



## Integration

```{r}
library(dataRetrieval)
library(ggplot2)
library(pracma)
library(readxl)
library(dplyr)
library(cowplot)
library(lubridate)
```
Paper source:
Rosburg et al. (2016). The effect of flow data resolution on sediment yield estimation and channel design. https://doi.org/10.1016/j.jhydrol.2016.04.040

## Objective: calculate total annual sediment [mass/year] and water [volume/year] for a given year for a specific station

## Approach: 
Here, you'll apply numerical integration using the `trapz` function in `pracma`. You'll want to set up a data frame of x and y, with x = day of year ranging between 1-365; y is the sediment load based on the equation and given streamflow).

## Workflow

1. Read in sediment-rating curve (data here in the file 1-s2.0-S0022169416302311-mmc2.xlsx)
2. Read in streamflow data using `readNWISdv` from `dataRetrieval` package
2. Calculate average daily sediment yield [kg/s]
3. For a given year, integrate under the daily sediment yield and daily streamflow using `trapz` function within the `pracma` package. 

## Apply case scenario: 
Compare the 2011 and 2012 year for water and sediment in the Lower Mississippi River (station id 07374000). Create 1 plot (with day of year on x-axis) of streamflow (2 lines, one for 2011 and one for 2012); Create 1 plot (with day of year on x-axis) of sediment flux (2 lines, one for 2011 and one for 2012). Calculate total annual sediment mass and water volume and compare the values between 2011 and 2012. Is the ratio of sediment mass for each year the same as the ratio of water volume (e.g. mass sed 2012/mass sed 2011)? In other words, was the streamflow carrying the same amount of sediment in 2011 and 2012? Create clear graphics and write up your comparison in a paragraph. 


#### Background on sediment rating curve table and the coefficients $a$ and $b$:
From Rosburg et al.: Relationships between water discharge and sediment load often can be expressed as a simple power function: 
$$Q_s = aQ^b$$
where $Q_s$ is the sediment discharge rate (kg/s), $Q$ is the concurrently-measured water discharge rate (m^3^/s), and $a$ and $b$ are best fit regression parameters (Asselman, 2000, Syvitski et al., 2000). This function is referred to as a sediment rating curve. In this relationship, it has been suggested that the exponent $b$ is related to the transport capacity in excess of sediment supply within a river channel, while the coefficient $a$ is related to absolute sediment supply (Barry et al., 2004). The exponent, $b$, tends to increase with the size of bed material in coarse bed rivers (Emmett and Wolman, 2001).

### Read in sediment rating curve table
```{r}
ds <- readxl::read_excel("Data/dataset-1-s2.0-S0022169416302311-mmc2.xlsx", 
     col_types = c("text", "text", "numeric", 
         "numeric", "numeric", "numeric", 
         "numeric", "numeric", "text", 
         "text", "numeric", "numeric", 
         "text"), skip = 1)
```
### Station Identification and Data Retreival

```{r}
site_id <- '07374000'
startDate <- '2000-01-01'
endDate <- '2020-12-30'
pCode <- '00060'
rawDailyQ <- readNWISdv(siteNumbers = site_id, 
                        parameterCd = pCode, 
                        startDate = startDate, 
                        endDate = endDate)
```

### Filter sediment table for station of interest
```{r}
dsm <- ds[which(ds$`Site Number`==site_id),]
```

### Calculate Daily sediment mass flux
```{r}
#cfs
Q_cms <- rawDailyQ$X_00060_00003/3.28084^3 # conversion to cms
a <- dsm$a # set a coefficient from filtered rating curve table
b <- dsm$b # set b coefficient from filtered rating curve table
Qs_kgs <- a*Q_cms^b
Qs_kgd <- Qs_kgs*60*60*24 # sediment load per day in kg/day
d <- data.frame(Q_cms,Qs_kgd,rawDailyQ$Date)
colnames(d)[3] <- "Date"
p <- ggplot(d, aes(Q_cms, Qs_kgd)) + 
  geom_line() + 
  theme_cowplot() +
  xlab('Streamflow [cms]') +
  ylab('Sediment load [kg per day]')
p
```


```{r}
p2 <- ggplot(rawDailyQ, aes(Date,X_00060_00003)) + 
  geom_point() + 
  theme_cowplot() +
  xlab('Date') +
  ylab('Streamflow [cfs]')
p2
```

```{r}
p3 <- ggplot(d, aes(Date,Qs_kgd)) + 
  geom_point() + 
  theme_cowplot() +
  xlab('Date') +
  ylab('Sediment Flux [kg/d]')
p3
```

## Compare 2011 and 2012 in case study

```{r}

d1 <- filter(d, Date > "2010-12-31" & Date < "2012-01-01")
d2011 <- trapz(d1$Qs_kgd)*.001 # include conversion seconds to days, kg to metric tons
q2011 <- trapz(d1$Q_cms*86400)/1e9 # include conversion seconds to days, scale by 1e9, so units are billion cubic meters 
q2011
d2011
```


```{r}
d2 <- filter(d, Date > "2011-12-31" & Date < "2013-01-01")
d2012 <- trapz(d2$Qs_kgd)*.001 # include conversion seconds to days, kg to metric tons
q2012 <- trapz(d2$Q_cms*86400)/1e9 # include conversion seconds to days, scale by 1e9, so units are billion cubic meters 
q2012
d2012
```

```{r}

q2012/q2011 # ratio of total water volume per year
d2012/d2011 # ratio of total sediment mass per year
```
```{r}
p2 <- ggplot() +
  geom_line(data=d1, aes(x=yday(Date),y=Q_cms), color="blue") +
  geom_line(data=d2, aes(x=yday(Date),y=Q_cms), color="red") +
    theme_cowplot() +
  xlab('Day of year') +
  ylab('Streamflow [cfs]')
p2
```
```{r}
p3 <- ggplot() +
  geom_line(data=d1, aes(x=yday(Date),y=Qs_kgd), color="blue") +
  geom_line(data=d2, aes(x=yday(Date),y=Qs_kgd), color="red") +
    #theme_cowplot() +
  xlab('Day of year') +
  ylab('Sediment load [kgd]') + theme(legend.position = "right")
p3

```

```{r summarize_year}

d_f <- d %>% 
  #create a year column
  mutate(yr=year(Date)) %>% 
  # group by year and then integrate to find total volume or mass for each year
  group_by(yr) %>% mutate(Q_int = trapz(Q_cms)*8640/1e9) %>% 
  mutate(Qs_int = trapz(Qs_kgd)*.001)  %>% 
  summarize_all(max)
d_f
d_f$rq <- d_f$Qs_int/d_f$Q_int/1000
d_f$m <- d_f$Qs_int/d_f$Q_cms
d_f
```


Let's build one single dataset from the 2011 and 2012 data above so that we can make a nice legend on our plot.

```{r}
data_2011_2012 <- 
  bind_rows(d1, d2) %>%
  mutate(Year = year(Date) %>% as.factor(),
         DOTY = yday(Date))


ggplot(data = data_2011_2012, 
       mapping = aes(x = DOTY, y = Qs_kgd, color = Year)) +
  geom_line() +
  theme_cowplot() +
  xlab('Day of year') +
  ylab('Sediment load [kgd]') 

```
